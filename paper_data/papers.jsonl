{"paperId": "02f3f8ef1fd1b92d552e554069e282960d0e79cd", "title": "Software Languages", "abstract": null, "year": 2018, "citationCount": 15, "fieldsOfStudy": null}
{"paperId": "07c46bfae9829c72d30ca08e1954bf97cd0ce33c", "title": "Integrating the evaluation of out of the platform autoevaluated programming exercises with personalized answer in Open edX", "abstract": "This paper describes a procedure to integrate personalized self-evaluated programming exercises created in an external programming interactive environment using a standard problem type of a MOOC platform, making use of the anonymized identifier provided by the platform. We will explain how to integrate auto evaluated programming exercises with personalized answers created with Python notebooks, using the standard problem types that Open edX provides. We will review the alternatives we evaluated and why we discarded them and explain our final workflow with an example problem in the edx platform. In our workflow the autoevaluated programming exercises are created as if we were doing some test-driven development where the problem is our functionality and the unit tests are actually the verifications done to generate hints and evaluate the students. Once the problem is designed the unit tests create a code, based on the answer and the Anonymous userID, code that is obfuscated using an encryption technique. That code is used as the answer of a standard Open edX problem, creating a completely automated personalized environment and avoiding the use of Open Response Assessment tools that depend on the correction of other students.", "year": 2020, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "07d7305f09d843b8fba7552fa8104b8f188f3f38", "title": "DLInfer: Deep Learning with Static Slicing for Python Type Inference", "abstract": "Python programming language has gained enor-mous popularity in the past decades. While its flexibility signifi-cantly improves software development productivity, the dynamic typing feature challenges software maintenance and quality assurance. To facilitate programming and type error checking, the Python programming language has provided a type hint mechanism enabling developers to annotate type information for variables. However, this manual annotation process often requires plenty of resources and may introduce errors. In this paper, we propose a deep learning type inference technique, namely DLInfer, to automatically infer the type infor-mation for Python programs. DLInfer collects slice statements for variables through static analysis and then vectorizes them with the Unigram Language Model algorithm. Based on the vectorized slicing features, we designed a bi-directional gated recurrent unit model to learn the type propagation information for inference. To validate the effectiveness of DLInfer, we conduct an extensive empirical study on 700 open-source projects. We evaluate its accuracy in inferring three kinds of fundamental types, including built-in, library, and user-defined types. By training with a large-scale dataset, DLInfer achieves an average of 98.79% Top-1 accuracy for the variables that can get type information through static analysis and manual annotation. Further, DLInfer achieves 83.03% type inference accuracy on average for the variables that can only obtain the type information through dynamic analysis. The results indicate DLInfer is highly effective in inferring types. It is promising to apply it to assist in various software engineering tasks for Python programs.", "year": 2023, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "10b94e7522103ec4d2bce2bbe74d55ac0581b3d2", "title": "Starshade rendezvous: exoplanet sensitivity and observing strategy", "abstract": "Launching a starshade to rendezvous with the Nancy Grace Roman Space Telescope (Roman) would provide the first opportunity to directly image the habitable zones (HZs) of nearby sunlike stars in the coming decade. A report on the science and feasibility of such a mission was recently submitted to NASA as a probe study concept. The driving objective of the concept is to determine whether Earth-like exoplanets exist in the HZs of the nearest sunlike stars and have biosignature gases in their atmospheres. With the sensitivity provided by this telescope, it is possible to measure the brightness of zodiacal dust disks around the nearest sunlike stars and establish how their population compares with our own. In addition, known gas-giant exoplanets can be targeted to measure their atmospheric metallicity and thereby determine if the correlation with planet mass follows the trend observed in the Solar System and hinted at by exoplanet transit spectroscopy data. We provide the details of the calculations used to estimate the sensitivity of Roman with a starshade and describe the publicly available Python-based source code used to make these calculations. Given the fixed capability of Roman and the constrained observing windows inherent for the starshade, we calculate the sensitivity of the combined observatory to detect these three types of targets, and we present an overall observing strategy that enables us to achieve these objectives.", "year": 2021, "citationCount": 13, "fieldsOfStudy": ["Physics", "Computer Science"]}
{"paperId": "10ec942891cd9d4cdb22b670d96318bc956568cc", "title": "Simulation of research-grade physics, chemistry, and engineering experiments in LabVIEW as a flexible template for remote laboratories", "abstract": "We propose to repurpose sophisticated experimental simulations used for testing of control and analysis software as the foundation for flexible, realistic, and robust remote access-simulated-resource-type [1] project-based learning in higher education. It is impractical to implement undergraduate projects or laboratories directly on \u201cbig science\u201d experiments by virtue of their uniqueness, rarity, and running costs. By implementing simulations we can take advantage of the benefits of scalability and reduced running costs [2] in addition to the unique affordances of simulations such as zero acquisition time, enhanced opportunity for iteration of technique, and learning outcomes adaptability [3], all while maintaining a realistic learning experience. There is a danger with virtual laboratories that students \u201cact before thinking\u201d as opposed to \u201cthinking before acting\u201d, with potentially negative effects on their learning and the realism of the experience [4]. In order to minimise this effect and to ensure as realistic an experience as possible, we propose embedding the interaction with simulations within a facsimile of a research group environment which includes time budgeting and peer accountability. We demonstrate the practicality of this concept by implementing a LabVIEW-based simulation of the KATRIN TILO (Karlsruhe Tritium Neutrino Experiment Test of Inner Loop) tritium gas assaying system [5] which can be adapted for physics, chemistry, and engineering projects. Multiple simulations run at different physical scales and variable timescales, taking advantage of LabVIEW's inherent parallelism, including the quantum mechanics of Raman scattering, isotope exchange mechanics, transmission efficiency of the light collection system, and a realistic interface for controlling the laser, gas handling, and spectral acquisition. 1 Corresponding author: LewisR54@cardiff.ac.uk 1 SIMULATIONS IN RESEARCH AND EDUCATIONAL ENVIRONMENTS 1.1 Simulations in Research Environments In experimental physical sciences and engineering, simulations are routinely constructed by practising professionals at scales / levels of depth from fundamental physical interactions, through individual experimental components, to comprehensive simulations of entire experimental set-ups. It is beyond the scope of this paper to provide a comprehensive review of such simulations; the following examples from physics serve only to illustrate the depth, sophistication, and universality of simulations in the physical sciences. At the fundamental level of physical processes, one encounters highly contextand application-specific single-use simulations in fields as diverse as the particle physics of hadronic showers [6], simulation of muon backgrounds for detector commissioning [7], and gravitational waveform simulations for black hole binary merger searches [8]. Multiphysics suites do exist, however, and are also widely used. Notable examples include COMSOL [9] and ANSYS [10]. At the application level of entire detectors or elements of detectors, one encounters more frequent use of general-purpose multiphysics and simulation tool-kits such as GEANT package, applied for example to calibrate calorimeters [11] or muon flux through the KATRIN main detector [12]. Such simulations and iterative comparison with experiment are a vital part of the design and evaluation process for devices of all scales, from portable neutron flux detectors [13], to large satellites [14], to entire \u201cbig science\u201d experimental set-ups and facilities [15], [16]. This practice is nothing new in engineering and industry, however; aerospace have long used simulationand hardware-in-the-loop [17], [18], and it remains a central pillar of modern engineering practice [19]. In summary, simulations in research and industrial environments are vital and ubiquitous. Their implementation is usually thoroughly planned, often highly complex, cover every conceivable time, energy, and length scale and are iteratively linked to experiment / implementation by design. As a consequence of their application, such simulations are restricted to use by the host research group / institution / company, and are very rarely adapted for wider use. 1.2 Simulations in Educational Environments When considering the use of simulations in higher education in physical sciences and engineering, the concept of the \u201cvirtual laboratory\u201d is commonly encountered [1], [2]. This term hints at the inextricability of a simulation from its educational context. In the field of physics education, for instance, the importance of learning concepts by means of constructing models is emphasised [20]. Formal modelling frameworks have been developed which emphasise the students\u2019 engagement as practising researchers and the iterative nature of model construction [21], construction of models in a mixed-reality setting [22], and the effect of the blurred boundaries between physical and virtual laboratories [23]. Indeed, the dividing line between what a practising scientist might consider a simulation and a virtual environment is itself blurred. The blurring is further complicated by the module/ programme-level framework into which the laboratories are embedded; examples include realistic \u201cpractice-centred\u201d project-based learning frameworks with time budgets, peer-accountability, and the real possibility of failure [3], and more traditional discrete \u201clearning units\u201d closer in form to typical undergraduate laboratories. The latter appears to be the dominant form; educational simulations / virtual laboratories are rarely \u201clarge scale\u201d, in both the sense of operational complexity and breadth of the underlying concepts. Rather, simulated experiments are often virtual versions of simple undergraduate experiments [24]\u2013[29]. This is in stark contrast to the research and industrial simulations already discussed. Within the educational context, simulations / virtual laboratories are instances of active learning, which are well-known to have an overall positive effect on student learning [30]. Virtual laboratories have a number of potential enhancements over physical laboratories, what Nolen and Koretsky refer to as the \u201caffordances\u201d of the virtual environment. Affordances in turn influence the instructional design of a virtual laboratory, such as the overlaying of visual representations of invisible phenomena on a user interface (UI) [3]. Potkonjak et al. neatly summarise the advantages of virtual laboratories such as lower operational and maintenance costs due to the lack of physical equipment, ease of reconfiguration, and multiple, simultaneous (perhaps remote) access, balanced against disadvantages such as the necessity of (possibly large) computer and software resources, the typical lack of (often instructive) \u201cbad\u201d outcomes in a \u201csafe\u201d virtual environment [2]. Interestingly, Potkonjak et al. state that \u201cthe final stage in training ... requires real equipment\u201d [2]. The use of the word \u201ctraining\u201d suggests a predefined idea of the purpose of virtual laboratories. In the context of physics virtual laboratories this is debatable, since understanding of underlying fundamental concepts is likely the intended learning outcome, rather than mastery of a particular experimental set-up. In summary, the operating context of simulations as part or the entirety of a virtual laboratory is very different to that of research simulations, and are typically restricted (although not always) to small-scale defined-concept implementations, although they are evidently very effective in this role. 2 PROPOSAL: BRIDGE THE GAP BETWEEN THE RESEARCH AND EDUCATIONAL REGIMES We observe that there is a gap in the educational application of simulations in virtual laboratories that lies between the simple, small-scale virtualisation of traditional laboratories, and the simulation / computer operation of large-scale experimental set-ups in research environments. More specifically, there is little opportunity for undergraduate students to experience operation of cutting-edge \u201clarge\u201d experimental set-ups, and hence limited access to realistic experience of grappling with the complexities researchgrade experimental work. We propose the use of simulations of practical experimental set-ups as flexible, robust, and, above all, realistic virtual laboratories. The general principle is either to adapt existing simulation and experimental control software for educational use and / or to develop applications which simulate the operation and physical processes underlying a particular experimental set-up. It is likely that a research group will have several such applications that could be adapted, or existing simulation elements that could be combined within a framework such as a LabVIEW project or a GUI-driven Python application. An advantage of this approach is that institutions will already have the authors / maintainers of the source software and a team of experts in the field on-site. We note, however, that development time may be significant, so it is likely that a virtual laboratory lead would actually develop the application in consultation with the research group. Specific implementations will naturally vary between disciplines and in light of the intended learning outcomes of the planned activities. In this paper we suggest one possible specific implementation in order to illustrate the general principle; the development of a simulation of a subsystem (TILO) of a large experiment (KATRIN), based in large part on repurposing existing code. We suggest, without prescription, the use of the LabVIEW development environment due the relative ease of UI development and inherent parallelism. We further suggest how this proof-of-principle implementation can be adapted for multiple disciplines (physics, chemistry, engineering), and adapted and scaled for use in a remotelyand multiply-accessed virtual environment. 2.1 General Simulation Requirements A simulation of an educational experimental set-up will ", "year": 2019, "citationCount": 1, "fieldsOfStudy": ["Chemistry", "Physics"]}
{"paperId": "1875e84b27310dad3eb24b7b865ad68f41f68b7b", "title": "Real world instrumentation with Python", "abstract": "With this convenient, single-source reference, you'll learn how to develop instrumentation software with Python that lets you monitor or control devices in the physical world. Whether you need to acquire data from a hardware device or automate its functions, this book shows you how to build interfaces that include everything from software to wiring. You get step-by-step instructions, clear examples, and hands-on tips and hints for interfacing a PC to the real world, along with advice to help you avoid common pitfalls. Use the book's hardware survey to discover the interface type for your particular device, and then follow detailed examples to develop the interface with Python and C. Organized by interface type, data processing activities, and user interface implementations, this book is for anyone who works with instrumentation, robotics, data acquisition, or process control. * Understand how to define the scope of an application and determine the algorithms necessary -- and why it's important * Learn how to use industry-standard interfaces such as RS-232, RS-485, and GPIB * Create low-level extension modules in C to interface Python with a variety of hardware and test instruments * Explore the console, curses, TkInter, and wxPython for graphical and text-based user interfaces * Use open source software tools and libraries to reduce cost and avoid implementing functionality from scratch", "year": 2011, "citationCount": 8, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1e655fa69c62b430b051224153f701f1b607fd9c", "title": "Typilus: neural type hints", "abstract": "Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program\u2019s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace \u2014 a continuous relaxation of the discrete space of types \u2014 and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.", "year": 2020, "citationCount": 111, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "223e23754dd2e8e2938664e0042d2c6d34fc3d93", "title": "Python Type Hints Are Turing Complete (Pearl/Brave New Idea)", "abstract": null, "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "23ff29e7a88af7c58fdf4ca18d56d8d57b3a3345", "title": "PyTy: Repairing Static Type Errors in Python", "abstract": "Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based re-pair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.", "year": 2024, "citationCount": 6, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "273e1452eacd1caca5245636a00d330de4b91543", "title": "GrayWulf : Scalable Clustered Architecture for Data Intensive Computing Team Information", "abstract": "Data intensive computing presents novel challenges for traditional computing architectures that have focused on FLOPS. CPU speeds have surpassed IO capabilities of both commodity clusters and traditional supercomputers. We present the architecture of a database cluster targeted at dataintensive computations with petascale data sets. The goal of our design is to build a balanced system in terms of IO capability, following Amdahl\u2019s Laws. The system is built from commodity servers, similar to the well-known BeoWulf architecture. The GrayWulf name pays tribute to Jim Gray who was actively involved in its early design. The hardware at JHU exceeds one petabyte of disk space, and has 70GB/sec aggregate IO bandwidth. Our benchmarks are based on data from the petascale Pan-STARRS project, building the largest sky survey to date. The benchmarks involve sequential searches over hundreds of terabytes. 1. Problem Statement The nature of high performance computing is changing. While a few years ago much of high-end computing involved maximizing CPU cycles per second allocated for a given problem; today it revolves around performing computations over large data sets. This means that efficient data access from disks and data movement across servers is an essential part of the computation.  Data sets are doubling every year, growing slightly faster than Moore\u2019s Law[1]. As a result, a new challenge is emerging, as many groups in science (but also beyond) are facing analyses of data sets in tens of terabytes, eventually extending to a petabyte since disk access and data-rates have not grown with their size. There is no magic way to manage and analyze such data sets today. The problem exists both on the hardware and the software levels. The requirements for the data analysis environment are (i) scalability, including the ability to evolve over a long period, (ii) performance, (iii) ease of use, (iv) some fault tolerance and (v) most important\u2014low entry cost. Database-Centric Computing Many of the typical data access patterns in science require a first, rapid pass through the data, with relatively few CPU cycles carried out on each byte. These involve filtering by a simple search pattern, or computing a statistical aggregate. Such operations are quite naturally performed within a relational database, and expressed in SQL. So a traditional relational database fits these patterns extremely well. The picture gets more complicated when one needs to run more complex algorithms on the data, not necessarily easily expressed in a declarative language. Examples of such applications can include complex geospatial queries, processing time series data, or running the BLAST algorithm for gene sequence matching. The traditional approach of bringing the data to where there is an analysis facility is inherently not scalable, once the data sizes exceed a terabyte, due to network bandwidth, latency, and cost. It has been suggested [2] that the best approach is to bring the analysis to the data. If the data are stored in a relational database, nothing is closer to the data than the CPU of the database server. It is quite easy today with most relational database systems to import procedural (or object oriented) code and expose their methods as user defined functions within the query. This approach has proved to be very successful in many of our reference applications, and while writing class libraries linked against SQL was not always the easiest coding paradigm, its excellent performance made the coding effort worthwhile. Typical data-intensive scientific workloads Over the last few years we have implemented several eScience applications, in experimental dataintesive physical sciences applications such as astronomy, oceanography and water resources. We have been monitoring the usage and the typical workloads corresponding to different types of users. When analyzing the workload on the publicly available multi-terabyte Sloan Digital Survey SkyServer database[6], it was found that most user metrics have a 1/f power law distribution[7]. Of the several hundred million data accesses most queries were very simple, single row lookups in the data set, which heavily used indices such as on position over the celestial sphere (nearest object queries). These made up the high frequency, low volume part of the power law distribution. On the other end there were analyses which did not map very well on any of the precomputed indices, thus the system had to perform a sequential scan, often combined with a merge join. These often took over an hour to scan through the multi-terabyte database. In order to submit a long query, users had to register with an email address, while the short accesses were anonymous. We have noticed a pattern in-between these two types of accesses. Long, sequential accesses to the data were broken up into small, templated queries, typically implemented by a simple client-side Python script, submitted once in every 10 seconds. These \u201ccrawlers\u201d had the advantage of returning data quickly, and in small buckets. If the inspection of the first few buckets hinted at an incorrect request (in the science sense), the users could terminate the queries without having to wait too long. The \u201cpower users\u201d have adopted a different pattern. Their analyses involved complex, multi-step workflows, where the end result was approached in a multi-step, hit-and-miss fashion. Once the workflow was finalized, they executed it over the whole data set, by submitting a large job into a batch queue. In summary, most scientific analyses are done in a exploratory fashion, where \u201ceverything goes\u201d, and few predefined patterns apply. Users typically want to experiment, try innovative things that often do not fit preconceived notions, and would like to get rapid feedback on the momentary approach. Amdahl\u2019s laws Amdahl has established several laws for building a balanced computer system [8]. These were reviewed recently[9] in the context of the explosion of data. The paper pointed out that contemporary computer systems IO subsystems are lagging CPU cycles. In the discussion below we will be concerned with two of Amdahl\u2019s Laws:", "year": 2008, "citationCount": 8, "fieldsOfStudy": null}
{"paperId": "2a6a1d001eb6ad511d1ff4273d2c12b69f4a35ef", "title": "Analyzing Suicide Attempts by Adolescents", "abstract": "The report explores the multifaceted issue of adolescent suicide attempts, drawing from both psychological and sociological perspectives. It utilizes Emile Durkheim\u2019s theories, categorizing the motives for suicide into four types influenced by social factors: anomic, altruistic, egoistic, and fatalistic. The research aims to understand the complex interplay between individual identity, societal integration, and external social events in influencing suicide tendencies among adolescents. The methodology involves sophisticated data analysis, using Python for data extraction and visualization to identify correlations between suicide rates and various sociological factors. This analytical approach helps to discern patterns and trends that could inform interventions. Preliminary findings indicate a fluctuation in suicide rates, with significant peaks and troughs over the years, hinting at underlying, unexplored factors. The study also considers a range of variables from personal behavior to social experiences, highlighting the impact of bullying, serious injuries, and the lack of close friendships on the likelihood of attempted suicide. This research is crucial in providing insights into the reasons behind adolescent suicide attempts, which could be instrumental in developing targeted prevention strategies. The interrelationship of the mentioned factors underscores the complexity of addressing adolescent suicide and the need for a nuanced approach that considers both individual and societal dimensions.", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "3bf6a97663ae221e56d7b92bfd3353f04e5a676d", "title": "Smtlink 2.0", "abstract": "Smtlink is an extension of ACL2 with Satisfiability Modulo Theories (SMT) solvers. We presented an earlier version at ACL2'2015. Smtlink 2.0 makes major improvements over the initial version with respect to soundness, extensibility, ease-of-use, and the range of types and associated theory-solvers supported. Most theorems that one would want to prove using an SMT solver must first be translated to use only the primitive operations supported by the SMT solver -- this translation includes function expansion and type inference. Smtlink 2.0 performs this translation using a sequence of steps performed by verified clause processors and computed hints. These steps are ensured to be sound. The final transliteration from ACL2 to Z3's Python interface requires a trusted clause processor. This is a great improvement in soundness and extensibility over the original Smtlink which was implemented as a single, monolithic, trusted clause processor. Smtlink 2.0 provides support for FTY defprod, deflist, defalist, and defoption types by using Z3's arrays and user-defined data types. We have identified common usage patterns and simplified the configuration and hint information needed to use Smtlink.", "year": 2018, "citationCount": 7, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "449ab79be2d6086b88832cfc9c5d502524c8524f", "title": "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors", "abstract": "Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies. State-of-the-art models like OpenAI\u2019s ChatGPT [8] and GPT-4 [9] could enhance programming education in various roles, e.g., by acting as a personalized digital tutor for a student, a digital assistant for an educator, and a digital peer for collaborative learning [1, 2, 7]. In our work, we seek to comprehensively evaluate and benchmark state-of-the-art large language models for various scenarios in programming education. Recent works have evaluated several large language models in the context of programming education [4, 6, 10, 11, 12]. However, these works are limited for several reasons: they have typically focused on evaluating a specific model for a specific education scenario (e.g., generating explanations), or have considered models that are already outdated (e.g., OpenAI\u2019s Codex [3] is no longer publicly available since March 2023). Consequently, there is a lack of systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios in programming education. These scenarios are designed to capture distinct roles these models could play, namely digital tutors, assistants, and peers, as discussed above. More concretely, we consider the following six scenarios: (1) program repair, i.e., fixing a student\u2019s buggy program; (2) hint generation, i.e., providing a natural language hint to the student to help resolve current issues; (3) grading feedback, i.e., grading a student\u2019s program w.r.t. a given rubric; (4) peer programming, i.e., completing a partially written program or generating a sketch for the solution program; (5) task creation, i.e., generating new tasks that exercise specific types of concepts or bugs; (6) contextualized explanation, i.e., explaining specific concepts or functions in the context of a given program. Our study uses a mix of quantitative and qualitative evaluation to compare the performance of these models with the performance of human tutors. We conduct our evaluation based on 5 introductory Python programming problems with a diverse set of input/output specifications. For each of these problems, we consider 5 buggy programs based on publicly accessible submissions from geeksforgeeks.org [5] (see Figure 1); these buggy programs are picked to capture different types of bugs for each problem. We will provide a detailed analysis of the data and results in a longer version of this poster. Our preliminary results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors\u2019 performance for several scenarios.", "year": 2023, "citationCount": 47, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "571178fc2b37ea642d54dbf970ea6e7f58622f6f", "title": "A wizard for e-learning computer programming", "abstract": "We present an e-learning and e-assessment aid for studying and teaching programming languages (Java, C, Perl, Ruby and Python). The student uses the wizard to go through a series of pages which present information, link to web resources, ask questions, and mark answers. The questions can be of short-answer type, multiple choice or programming. In the short-answer questions, hints can be automatically generated in the form of scrambled spelling or first letters and lengths of the words in the answer. In programming questions, hints may be in the form of skeletons of programs with blank \u201cto do\u201d regions which are to be filled in. Programs submitted by the student are compiled, run and checked against expected output. The wizard can present a randomly selected and shuffled subset of the questions taken from a question repository. The repository contains flags determining the hints and format of each question. A record of the session, containing the questions and possibly multiple attempts at solutions from the student, is saved in an encrypted format. The answer key within the wizard is also encrypted so that answers cannot be obtained by reverse engineering.", "year": 2012, "citationCount": 4, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "676a0d434a3131d361ed8974b5e2398981364c98", "title": "A functional scripting interface to an object oriented C++ library", "abstract": "The object oriented programming paradigm is widely used in science and engineering. Many open and commercial libraries are written in C++ and increasingly provide bindings to Python, which is much easier to learn, but still partly encourages the use of object oriented programming. However, scientific ideas are much more directly and meaningfully expressed in the purely functional programming paradigm. Here, we take a best practice example, CERNs Python binding for its ROOT library, designed to handle the enormous amounts of data generated by the worlds largest particle accelerator, and translate a simple segment of its tutorial into Clojure, a functional language from the Lisp family. The code examples demonstrate how a purely functional language straightforwardly expresses scientific ideas. Subsequently, we develop a compiled Lisp-C++ interoperation layer to access the ROOT library exclusively via functional code. To preserve the expressivity of the Lisp code, the type hints necessary for C++ code generation are stored in a separate file. The interop system presented here is a generic framework that, when provided with a suitable file of type hints, facilitates access to methods of arbitrary C++ libraries and platforms like real-time microcontrollers.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6b6b798c0a19f612578b781f5912e8a87ad1b6f7", "title": "PYTHON FOR EDUCATION Computational Methods for Nonlinear Systems", "abstract": "T he field of computational science and engineering (CSE) integrates mastery of specific domain sciences with expertise in data structures, algorithms, numerical analysis, programming methodologies, simulation, visualization, data analysis, and performance optimization. The CSE community has embraced Python as a platform for attacking a wide variety of research problems, in part because of Python\u2019s support for easily gluing together tools from different domains to solve complex problems. Many of the same advantages that Python brings to CSE research also make it useful for teaching: Python and its many batteries can help students learn a wide swath of techniques necessary to perform effective CSE research. \u201cComputational Methods for Nonlinear Systems\u201d is a graduate-level computational science laboratory course that we jointly teach at Cornell. We began developing the course in summer 2004 to support the curricular needs of the Cornell IGERT program in nonlinear systems, a broad and interdisciplinary graduate fellowship program aimed at introducing theoretical and computational techniques developed in the study of nonlinear and complex systems to a range of fields. The course\u2019s format is somewhat unusual. As a computational laboratory course, it provides relatively little in the way of lectures: we prefer to have students learn by doing rather than listening. The course is autonomous, modular, and self-paced: students choose computational modules to work on from a large (and hopefully growing) suite of those available, and then proceed to implement relevant simulations and analyses as laid out in the exercises. We provide \u201cHints\u201d files to help the students along: these consist of documented skeletal code that the students are meant to flesh out. We\u2019ve written several different visualization tools to provide visual feedback. We find these help engage the students in new problems and are useful in code debugging. Python is a useful teaching language for several reasons. Its clean syntax lets students learn the language quickly, and lets us provide concise programming hints in our documented code fragments. Python\u2019s dynamic typing and high-level, built-in datatypes enable students to get programs working quickly, without struggling with type declarations and compile-link-run loops. Because Python is interpreted, students can learn the language by executing and analyzing individual commands, and we can help them debug their programs by working with them in the interpreter. Another key advantage that Python brings to scientific computing is the availability of many packages supporting numerical algorithms and visualization. While some of our exercises require developing algorithms from scratch, others rely on established numerical routines implemented in thirdparty libraries. Although it\u2019s important to understand the fundamentals of algorithms, error analysis, and algorithmic complexity, it\u2019s also useful to know when and how to use existing solutions. We make heavy use of the NumPy (www.scipy.org/numpy) and SciPy (www. scipy.org) packages for efficiently manipulating arrays and for accessing routines to generate random numbers, integrate ordinary differential equations, find roots, compute eigenvalues, and so on. We use matplotlib (http:// matplotlib.sourceforge.net) for x-y plotting and histograms. We\u2019ve written several visualization modules that we provide to students, based on the Python Imaging Library (PIL; www. pythonware.com/products/pil), using PIL\u2019s ImageDraw module to place graphics primitives within an image, and the ImageTk module to paste an image into a Tk window for real-time animation. We recommend the use of the IPython interpreter, which facilitates exploration by students (www. ipython.scipy.org). We\u2019ve also used VPython (www.vpython.org) to generate 3D animations to accompany some of our modules. PYTHON FOR EDUCATION", "year": 2007, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "70bd304ce4bebc087a363fc668c8750184166cbe", "title": "Scientometric review of transition metal oxides for hydrogen energy production", "abstract": "ABSTRACT Hydrogen production from electrocatalytic and photo(electro)catalytic water splitting is regarded as a potential technology for clean energy production and sustainable energy storage, but the implementation is limited by the development of cost-effective and high-activity hydrogen evolution reaction (HER) catalysts. Transition metal oxides, a class of important functional materials with changeable elementary composition and crystal structures, have aroused scientists\u2019 attention for the investigation on HER. Under the circumstance, it is necessary to prepare a systematic and comprehensive evaluation of the development in this scientific area and identify the most potential materials, technologies, as well as the existing limitations and opportunities for the future commercialization. In this article, a scientometric study in the scientific area of transition metal oxides for electrocatalytic and photo(electro)catalytic HER was performed. In total, 1447 papers from Web of Science (WoS) database were extracted via the combination of relevant keywords and analyzed using various scientometric indices through Anaconda Prompt, ScientoPy, Citespace combined with Python. The investigation results indicate that the research in this field dated from 1992, and the development in this scientific area greatly accelerated from 2015. The top two types of publications in article and review share 83.8% and 13.4% of all the documents published in this area. Specifically, the most contributed countries in this field are China and United States. Journal of Materials Chemistry A is the most active journal in this scientific area. The analysis of important keywords in this research field hints that nanotechnology should be paid great attention to further promote the development of this area.", "year": 2022, "citationCount": 6, "fieldsOfStudy": null}
{"paperId": "732ff8a6314317b1eee53d2d263f5e356ca2acf3", "title": "Effective and Innovative Interactives for icseBooks", "abstract": null, "year": 2019, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7ac8bf2c9d54859f8f3849604a7431ab7015d952", "title": "MINO : Data-driven approximate type inference for Python", "abstract": "Dynamic programming languages, such as Python and Ruby, are becoming the languages of choice for many developers. The lack of strict compile-time type checking is often cited as making development cycles faster. However, a big drawback to dynamically typed languages is that it is impossible to know the exact type of a given variable at compile time; it is a well known result that this problem is undecidable for such languages. Considerable effort has been spent by the programming languages community [2, 3, 5, 7] to develop systems which attempt to mitigate this problem for Python. These solutions rely on formal logic to prove type assertions for variables throughout the program. In many cases, such techniques can effectively reason about types statically. However, such approaches are necessarily conservative, and usually make certain restrictive assumptions about the program. MINO takes a radically different approach to the problem of static type inference for dynamic languages, specifically Python. Instead of relying on formal semantics, MINO takes a data-driven approach based on machine learning techniques. The main intuition is that there are many aspects of a program which provide information about types, beyond formal semantics. For instance, properties like variable names and certain usage patterns all collectively provide hints as to a variable\u2019s type. As a concrete example, variables named i are usually index variables or counters and thus integers. While this kind of data-driven approach is only an approximation, we believe such a system has several potential strong use cases. Our envisioned use case is for optimizing the generation of native machine code. For instance, many modern dynamic language interpreters implement a form of just-in-time (JIT) compilation, where instead of interpreting programs solely based on abstract syntax trees (ASTs) or high level bytecode, certain fragments of the program are compiled to equivalent native machine code. These fragments are often generated with a fast-path case where a variable is assumed to hold a certain type, and a slow-path case which serves as a fallback if the fast-path assumption does not hold. Thus, predicting a type incorrectly here does not sacrifice correctness, but rather incurs a performance penalty. Therefore, as long as MINO\u2019s type predictions are mostly correct, then we believe that such a system can provide performance gains for native code generation, beyond what a traditional type inference system can. This is the advantage of not being overly conservative with predictions. The remainder of this paper will focus on the design and implementation of MINO, which to the best of our knowledge is the first system which uses machine learning to perform type inference for programs written in a dynamically typed language. We present an evaluation which shows that MINO is able to achieve reasonable predictive performance on a wide variety of different programs.", "year": 2012, "citationCount": 2, "fieldsOfStudy": null}
{"paperId": "7b76f3216cf396ba5eb7ce6c9b21fbe4b3d5d682", "title": "Supporting skill integration in an intelligent tutoring system for code tracing", "abstract": "Background: Skill integration is vital in students' mastery development and is especially prominent in developing code tracing skills which are foundational to programming, an increasingly important area in the current STEM education. However, instructional design to support skill integration in learning technologies has been limited. Objectives: The current work presents the development and empirical evaluation of instructional design targeting students' difficulties in code tracing particularly in integrating component skills in the Trace Table Tutor (T3), an intelligent tutoring system. Methods: Beyond the instructional features of active learning, step-level support, and individualized problem selection of intelligent tutoring systems (ITS), the instructional design of T3 (e.g., hints, problem types, problem selection) was optimized to target skill integration based on a domain model where integrative skills were represented as combinations of component skills. We conducted an experimental study in a university-level introductory Python programming course and obtained three findings. Results and Conclusions: First, the instructional features of the ITS technology support effective learning of code tracing, as evidenced by significant learning gains (medium-to-large effect sizes). Second, performance data supports the existence of integrative skills beyond component skills. Third, an instructional design focused on integrative skills yields learning benefits beyond a design without such focus, such as improving performance efficiency (medium-to-large effect sizes). Major Takeaways: Our work demonstrates the value of designing for skill", "year": 2022, "citationCount": 8, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "837b60227bcce704bf169a1ed54f3ce0866097ce", "title": "Lexos 2017: building reliable software in python", "abstract": "Refactoring software is challenging, but necessary to ensure software correctness and extensibility. We present a plan that blends automated tools and human reviews when refactoring the back-end of a web-based application. The Lexos software, developed by the NEH-funded Lexomics Project, provides a simple, web-based workflow for text processing, statistical analysis, and visualization of results when exploring digitized texts. The development of Lexos spans six years and includes over fifty undergraduate developers, many who assumed leadership roles in architectural design and systems engineering over three software releases. This paper shares our current refactoring effort on the Python backend to produce Lexos v3.2, an effort that includes a transition from Python v2.7 to Python v3.6. Good software engineering practices guide the effort, including the use of type hinting, a Model-View-Control pattern, PEP 8 code and PEP 257 documentation styles, unit testing, and continuous integration.", "year": 2018, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "872a00d8ebfa7f34ff11857ac0ddf07dd7d84d9d", "title": "Tell Me What's Wrong: A Python IDE with Error Messages", "abstract": "Development environments play a crucial role for novice programmers. Not only do they act as interface to type in and execute programs, but a programming environment is also responsible for reporting errors, managing in- and output when the program is running, or offering the programmer access to the underlying notional machine. In recent years several new educational programming environments for Python have been presented. However, the important issue of reporting errors has rarely been addressed and evaluations often hint that students main issue is the poor quality of Python's error messages. We have therefore written an educational Python environment with enhanced error messages. This paper presents the design and rationale of its three primary features: modifications to Python, enhanced error messages, and the visual debugger.", "year": 2020, "citationCount": 14, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "87854f747bb30d431bb408c80fb753385899a873", "title": "A Transient Semantics for Typed Racket", "abstract": "Mixed-typed languages enable programmers to link typed and untyped components in various ways. Some offer rich type systems to facilitate the smooth migration of untyped code to the typed world; others merely provide a convenient form of type Dynamic together with a conventional structural type system. Orthogonal to this dimension, Natural systems ensure the integrity of types with a sophisticated contract system, while Transient systems insert simple first-order checks at strategic places within typed code. Furthermore, each method of ensuring type integrity comes with its own blame-assignment strategy. Typed Racket has a rich migratory type system and enforces the types with a Natural semantics. Reticulated Python has a simple structural type system extended with Dynamic and enforces types with a Transient semantics. While Typed Racket satisfies the most stringent gradual-type soundness properties at a significant performance cost, Reticulated Python seems to limit the performance penalty to a tolerable degree and is nevertheless type sound. This comparison raises the question of whether Transient checking is applicable to and beneficial for a rich migratory type system. This paper reports on the surprising difficulties of adapting the Transient semantics of Reticulated Python to the rich migratory type system of Typed Racket. The resulting implementation, Shallow Typed Racket, is faster than the standard Deep Typed Racket but only when the Transient blame assignment strategy is disabled. For language designers, this report provides valuable hints on how to equip an existing compiler to support a Transient semantics. For theoreticians, the negative experience with Transient blame calls for a thorough investigation of this strategy.", "year": 2021, "citationCount": 8, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "8f0642cad60b338d493f4ea414167b08db6f3e0d", "title": "Automated Data-Driven Hints for Computer Programming Students", "abstract": "Formative feedback is essential for learning computer programming but is also a challenge to automate because of the many solutions a programming exercise can have. Whilst programming tutoring systems can easily generate automated feedback on how correct a program is, they less often provide some personalised guidance on how to improve or fix the code. In this paper, we present an approach for generating hints using previous student data. Utilising a range of techniques such as filtering, clustering and pattern mining, four different types of data-driven hints are generated: input suggestion, code-based, concept and pre-emptive hints. We evaluated our approach with data from 5529 students using the Grok Learning platform for teaching programming in Python. The results show that we can generate various types of hints for over 90% of students with data from only 10 students, and hence, reduce the cold-start problem.", "year": 2017, "citationCount": 27, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "962338d7baf42dd56fcaf8ee07763e875885cae8", "title": "Automatic Parallelization of Python Programs for Distributed Heterogeneous Computing", "abstract": "This paper introduces a novel approach to automatic ahead-of-time (AOT) parallelization and optimization of sequential Python programs for execution on distributed heterogeneous platforms. Our approach enables AOT source-to-source transformation of Python programs, driven by the inclusion of type hints for function parameters and return values. These hints can be supplied by the programmer or obtained by dynamic profiler tools; multi-version code generation guarantees the correctness of our AOT transformation in all cases. Our compilation framework performs automatic parallelization and sophisticated high-level code optimizations for the target distributed heterogeneous hardware platform. It includes extensions to the polyhedral framework that unify user-written loops and implicit loops present in matrix/tensor operators, as well as automated section of CPU vs. GPU code variants. Further, our polyhedral optimizations enable both intra-node and inter-node parallelism. Finally, the optimized output code is deployed using the Ray runtime for scheduling distributed tasks across multiple heterogeneous nodes in a cluster. Our empirical evaluation shows significant performance improvements relative to sequential Python in both single-node and multi-node experiments, with a performance improvement of over 20,000$\\times$ when using 24 nodes and 144 GPUs in the OLCF Summit supercomputer for the Space-Time Adaptive Processing (STAP) radar application.", "year": 2022, "citationCount": 4, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "96bbc4e4d3ca7fe6c681769a381d3ae4da7983c0", "title": "Project Team", "abstract": null, "year": 2019, "citationCount": 46, "fieldsOfStudy": null}
{"paperId": "ae3d28f2a6751e8f42407c645d9580c354531273", "title": "Migrating Legacy Fortran to Python While Retaining Fortran-Level Performance through Transpilation and Type Hints", "abstract": "We propose a method of accelerating Python code by just-in-time compilation leveraging type hints mechanism introduced in Python 3.5. In our approach performance-critical kernels are expected to be written as if Python was a strictly typed language, however without the need to extend Python syntax. This approach can be applied to any Python application, however we focus on a special case when legacy Fortran applications are automatically translated into Python for easier maintenance. We developed a framework implementing two-way transpilation and achieved performance equivalent to that of Python manually translated to Fortran, and better than using other currently available JIT alternatives (up to 5x times faster than Numba in some experiments).", "year": 2016, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "b1a40a6228969cfa192f5923479a57d8a24cea0a", "title": "Python Pocket Reference: Python in Your Pocket", "abstract": "This is the book to reach for when you're coding on the fly and need an answer now. It's an easy-to-use reference to the core language, with descriptions of commonly used modules and toolkits, and a guide to recent changes, new features, and upgraded built-ins -- all updated to cover Python 3.X as well as version 2.6. You'll also quickly find exactly what you need with the handy index. Written by Mark Lutz -- widely recognized as the world's leading Python trainer -- Python Pocket Reference, Fourth Edition, is the perfect companion to O'Reilly's classic Python tutorials, also written by Mark: Learning Python and Programming Python. Built-in object types, including numbers, lists, dictionaries, and more Statements and syntax for creating and processing objects Functions and modules for structuring and reusing code Python's object-oriented programming tools The exception-handling model Built-in functions, exceptions, and attributes Special operator overloading methods Widely used standard library modules and extensions Command-line options and development tools Python idioms and hints", "year": 2009, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "bc0210361535562dac14987da8a91c7839533682", "title": "Python Type Hints are Turing Complete", "abstract": "Grigore showed that Java generics are Turing complete by describing a reduction from Turing machines to Java subtyping. We apply Grigore's algorithm to Python type hints and deduce that they are Turing complete. In addition, we present an alternative reduction in which the Turing machines are simulated in real time, resulting in significantly lower compilation times. Our work is accompanied by a Python implementation of both reductions that compiles Turing machines into Python subtyping machines.", "year": 2022, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "c7a5ecfe76bbf6a42df41200516d98f873ff3bd3", "title": "Python Type Hints Are Turing Complete (Artifact)", "abstract": "The artifact comprises a Docker image (virtual environment) containing the source code and experiments setup mentioned in the paper. The artifact is available on Zenodo 1 . The anonymous version submitted to the ECOOP Artifact Evaluation Committee (AEC) is also available on Zenodo 2 . The project is maintained on GitHub 3 . 2012 ACM Subject Classification Software and its engineering \u2192 General programming languages Keywords and phrases", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "d48e5b6ff8326c72219fd855d0fd92a62903aaff", "title": "TIGER: A Generating-Then-Ranking Framework for Practical Python Type Inference", "abstract": "Python's dynamic typing system offers flexibility and expressiveness but can lead to type-related errors, prompting the need for automated type inference to enhance type hinting. While existing learning-based approaches show promising inference accuracy, they struggle with practical challenges in comprehensively handling various types, including complex generic types and (unseen) user-defined types. In this paper, we introduce TIGER, a two-stage generating-then-ranking (GTR) framework, designed to effectively handle Python's diverse type categories. TIGER leverages fine-tuned pre-trained code models to train a generative model with a span masking objective and a similarity model with a contrastive training objective. This approach allows TIGER to generate a wide range of type candidates, including complex generics in the generating stage, and accurately rank them with user-defined types in the ranking stage. Our evaluation on the ManyTypes4Py dataset shows TIGER's advantage over existing methods in various type categories, notably improving accuracy in inferring user-defined and unseen types by 11.2% and 20.1% respectively in Top-5 Exact Match. Moreover, the experimental results not only demonstrate TIGER's superior performance and efficiency, but also underscore the significance of its generating and ranking stages in enhancing automated type inference.", "year": 2024, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "d584083e224e33bcd0fe55826b49e539e40c2f82", "title": "Using the Expectation Maximization Algorithm with Heterogeneous Mixture Components for the Analysis of Spectrometry Data", "abstract": "Coupling a multi-capillary column (MCC) with an ion mobility (IM) spectrometer (IMS) opened a multitude of new application areas for gas analysis, especially in a medical context, as volatile organic compounds (VOCs) in exhaled breath can hint at a person's state of health. To obtain a potential diagnosis from a raw MCC/IMS measurement, several computational steps are necessary, which so far have required manual interaction, e.g., human evaluation of discovered peaks. We have recently proposed an automated pipeline for this task that does not require human intervention during the analysis. Nevertheless, there is a need for improved methods for each computational step. In comparison to gas chromatography / mass spectrometry (GC/MS) data, MCC/IMS data is easier and less expensive to obtain, but peaks are more diffuse and there is a higher noise level. MCC/IMS measurements can be described as samples of mixture models (i.e., of convex combinations) of two-dimensional probability distributions. So we use the expectation-maximization (EM) algorithm to deconvolute mixtures in order to develop methods that improve data processing in three computational steps: denoising, baseline correction and peak clustering. A common theme of these methods is that mixture components within one model are not homogeneous (e.g., all Gaussian), but of different types. Evaluation shows that the novel methods outperform the existing ones. We provide Python software implementing all three methods and make our evaluation data available at this http URL", "year": 2014, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "d758f324b4aa5a8869dbb9bc599b62a71329a16d", "title": "Evaluating the Impact of Possible Dependencies on Architecture-Level Maintainability", "abstract": "Dependencies among software entities are the foundation for much of the research on software architecture analysis and architecture analysis tools. Dynamically typed languages, such as Python, JavaScript and Ruby, tolerate the lack of explicit type references, making certain dependencies indiscernible by a purely syntactic analysis of source code. We call these possible dependencies, in contrast with the explicit dependencies that are directly manifested in source code. We find that existing architecture analysis tools have not taken possible dependencies into consideration. An important question therefore is: to what extent will these missing possible dependencies impact architecture analysis?To answer this question, we conducted a study of 499 open-source Python projects, employing type inference techniques and type hint practices to discern possible dependencies. We investigated the consequences of possible dependencies in three software maintenance contexts, including capturing co-change relations recorded in revision history, measuring architectural maintainability, and detecting architecture anti-patterns that violate design principles and impact maintainability. Our study revealed that the impact of possible dependencies on architecture-level maintainability is substantial\u2014higher than that of explicit dependencies. Our findings suggest that architecture analysis and tools should take into account, assess, and highlight the impacts of possible dependencies caused by dynamic typing.", "year": 2023, "citationCount": 8, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "dca425a559390e5639d59fdc4501cb31343341c4", "title": "\"Birds and Animals of Australia\u2019s Top End: Darwin, Kakadu, Katherine, and Kununurra\" by Nick Leseberg & Iain Campbell. 2015. [book review]", "abstract": "This latest book by Leseberg and Campbell is specifically aimed at the species you are likely to see in the coastal region of the Northern Territory. This area they call the Top End and they give introductions to the weather and geology. They describe seven specific habitats and give basic hints on how to watch and where to find wildlife. The bird (birds are chordate animals too) portion of the book does not follow taxonomic order, but is divided by habitat. There are sections on wetlands and beach es, raptors, forests and open areas. The book covers over 200 species out of Australia\u2019s current list of 898 \u2013 an amazing quarter of the total in only ten percent of the country\u2019s area. The authors do not give a description of each species, nor do they point out identifying features. The text describes something of the bird\u2019s biology and where it is most common. The mammal section focuses on the larger species. For example bats are represented by two large flying foxes and three small species (out of about three dozen species for the area). Similarly the rodent section has photos of three rat species and the text covers only six. As the book\u2019s stated purpose is \u201cto cover species likely to be seen by the average wildlife watcher\u201d this focus makes good sense. This year, of the dozen types of \u201cmice\u201d in Ontario, I have seen only three (Deer Mouse, MeadowandRed-backed Voles) and I am actively look ing. The authors instead have concentrated on the visible wallabies and other cat-sized marsupials. The reptile and amphibians are headed by Australia\u2019s iconic salty and freshy \u2013 the Estuarine and Freshwater Crocodiles. Turtles and an assortment of lizards follow. The snakes include several pythons (the oddly named Children\u2019s Python is named for zoologist John Children) and some of the most venomous snakes in the world. The book concludes with 15 cute native frogs and the infamous, introduced Cane Toad. The book is well illustrated throughout, starting with some clear maps and habitat photos. Each species is illustrated by at least one large photo of the animal in habitat. Many have additional flight photos. The quality of all these images is excellent as the authors selected the best photos from a large pool of photographers. Another feature I really like is the \u201cWhere to find\u201d boxes for every species. So if you really want to see a gorgeous Gouldian Finch then try \u201cthe road to Edith Falls or the airfield at Timber Creek\u201d. Now I wonder about the book\u2019s purpose. There are week-long tours that cover only this region, but most take in other areas as well. For a North American or European why fly to Australia for a short, limited tour as it is impractical. Our choice was two back-to-back, two-week tours that covered eastern and western Australia. I can see Australians buying this book, but it is limited for the foreign visitor. These authors have produced two other books \u2013 the Wildlife of Australia (By Iain Campbell and Sam Woods. 2013. Princeton University Press) and a Field Guide to the Birds of Australia \u2013 A Photographic Guide. (By Iain Campbell, Sam Woods and Nick Leseberg. 2014. Princeton University Press). These are both similar in style and, to an extent, in content. I liked the Wildlife of Australia as an addition and back up to a conventional bird guide. The photographic bird guide thrilled me less. This newest book is even more limited. I think I would borrow it, make notes of some key items, but would not carry it with me in my precious baggage allowance.", "year": 2015, "citationCount": 1, "fieldsOfStudy": ["Biology"]}
{"paperId": "ddf2e20427e24b422cc11f58a27458b75e1d3cca", "title": "Generative Type Inference for Python", "abstract": "Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze- style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high- quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic. What is more, their predictions are not interpretable, hindering developers' understanding and verification of the results. This paper introduces Typegen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. Typegen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypegEnconstructs example prompts from human annotations. Typeg Enonly requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, Typeg Enenhances the interpretability of results through the use of the input- explanation-output strategy, which generates both explanations and type predictions in COT prompts. Experiments show that Typegen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5 % in return value type prediction in terms of top-l Exact Match by using only five examples. Furthermore, Typeg Enachieves substantial improvements of 27 % to 84 % compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-I Exact Match.", "year": 2023, "citationCount": 6, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f267099d1e2c05d5e33d15fa9476272c6a0861c9", "title": "Practical Programming for NLP", "abstract": "This chapter is aimed at students and researchers who are eager to learn about practical programmatic solutions to natural language processing (NLP) problems. In addition to introducing the readers to programming basics, programming tools, and complete programs, we also hope to pique their interest to actively explore the broad and fascinating field of automatic natural language processing. Part I introduces programming basics and the Python programming language. Part II takes a step by step approach in illustrating the development of a program to solve a NLP problem. Part III provides some hints to help readers initiate their own NLP programming projects. INTRODUCTION Natural language processing (NLP) attempts to automatically analyze the languages spoken by humans (i.e., natural languages). For instance, you can program a computer to automatically identify the language of a text, extract the grammatical structure of sentences (see Chapter XXX of this book), categorize texts by genre (e.g., decide whether a text is a scientific or a narrative text; see Chapter XXX for classification applications), summarize a book (see Chapter XXX), etc. This chapter is aimed at teaching specialized, yet introductory, programming skills that are required to use available NLP tools. We hope that this chapter serves as a catalyst to launch NLP projects by motivating novice programmers to learn more about programming and encouraging more advanced programmers to develop NLP programs. The chapter is aimed at readers from the interdisciplinary arena that encompasses computer science, cognitive psychology, and linguistics. It is geared for individuals who have a practical NLP problem and for curious readers who are eager to learn about practical solutions for such problems. Fortifying students with the requisite programming skills to tackle an NLP problem in a single chapter is a daunting task for two primary reasons. First, along with advanced statistics, programming is probably the most intimidating task that practitioners in disciplines like linguistics or cognitive psychology can undertake. The typical student or researcher in these fields has little formal training in mathematics, logic, and computer science, hence, their first foray into programming can be a bit challenging. Second, although computer scientists have considerable experience with programming and have mastered many computer technologies, they might not be privy to the libraries or packages that are readily and freely available for NLP projects. In other words, there is a lot to cover if we attempt to address both these audiences, and it seems like an impossible challenge to design a chapter extending from the basics of programming to the specifics of NLP. Fortunately, for the reader and us, the availability of state-of-the-art NLP technologies and the enhanced usability available through easy-to-use interfaces alleviates some of these challenges. Because of space limitations, we could not achieve the coverage depth we had hoped for. We originally had planned to include programming projects in several languages such as Python, Perl, Java and PHP, along with numerous screen captures of captivating programming demonstrations. The chapter is now more focused on examples in Python. Fortunately, the materials that could not be included in the chapter (e.g., scripts, examples, screen captures), are available for your convenience on the companion website at http://patrickjeuniaux.info/NLPchapter. It also provides a series of links to NLP resources, as well as detailed instructions about how to execute the programs that are needed for the exercises. A great advantage of having a website is that it can be updated with current content, so do not hesitate to contact us if you wish to give us feedback. This chapter has three parts. Part I offers an introduction to programming. Part II gives a concrete example of programming for a specific NLP project. Part III provides general hints about starting your own NLP programming project. Readers who do not have programming experience or who do not know Python should definitely start with Part I. Individuals who have a working knowledge of Python can skip most of Part I. Among these people, the ones who do not know about NLTK could limit their reading of Part I to the section on functions and onwards. Although Part I covers a lot of material, the topic coverage is far from exhaustive. When you are done with this chapter, we encourage you to read a more complete introduction. We particularly recommend Elkner, Downey, and Meyers (2009). The same can be said of Part II. We also recommend reading Bird, Klein and Loper (2009), who give a thorough treatment of NLP programming with Python\u2019s Natural Language Processing Toolkit (NLTK). PART I. PROGRAMMING BASICS Computers are controlled by sets of instructions called programs. Because they are somewhat simple machines, computers can only follow the most unambiguous instructions. To achieve this ideal of precision, a program is written in a restricted language. Programming languages use a specific vocabulary (i.e. a set of words), a syntax (i.e., a set of rules defining how to use these words), and semantics (i.e., the meaning of the words and rules from a programming point of view). Learning the basic rules of a language is the first step towards writing meaningful and useful programs. But prior to learning a language, it might be good to learn about the history of computer programming. Knowing the historical motivation behind programming will help you grasp what programming is all about. Programming in Context Today we usually think of a computer as a general purpose device. However, this was not always the case. Whether you consider Babbage's Difference Engine (Swade, 2002), which solved polynomial functions, or Colossus, which helped decipher the Enigma codes during World War II (Hodges, 2000) to be computers, the fact remains that a \"computer\" is simply something that performs calculations. In fact, before the 20th century people whose jobs were to perform complex calculations for various purposes were called 'computers' (Anderson, 2009). As the science and technology of computing advanced, man-made computers became more complex and were able to perform more complex calculations. However, the process for doing so was extremely tedious and error prone. Computers were massive beasts of machines in those days, often taking up entire rooms. Programming sometimes meant re-patching cables on a switchboard (Petzold, 2000), a far cry from the text editors and visual interfaces that we are familiar with today. At this time it became clear to the scientists involved with creating and using these machines that the difficulty of using computers was an obstruction to their widespread use and acceptance. Their solution to this problem was to create layers of abstraction between the user and the computer. This process of increasing abstraction has continued to the present day and shows no signs of stopping in the foreseeable future. For example, consider your computer's desktop. \"Desktop\" is just an abstraction and analogy for a physical desktop for pens and paper. Similarly, the folders on your desktop are analogous to physical file cabinets used to store paper documents. Programming languages are just another kind of abstraction over the underlying computer instructions (\"machine code\"). The machine code is simply a very detailed and hard to use programming language. For most applications, programmers do not use machine code but use instead modern programming languages which are designed to simplify the programmer's life (by reducing the size of the program, reducing the likelihood of programming error, etc.). Programming languages have evolved in such a way that programming is no longer restricted to the purview of professional computer scientists. So-called high level languages allow practitioners of other fields (like psychology, and linguistics) to enjoy the power and flexibility of programming. One of the goals of this chapter is to show how this is feasible. Like in all fields, it is not possible to immediately benefit from practical applications without knowing the fundamental principles underlying them. Hence, the next section is aimed at bringing you up to speed with such principles. Fundamental Concepts of Computer Programming Fundamental programming concepts include (a) values and types, (b) syntax and semantics, (c) operations, (d) variables, constants, and assignments, (e) data structures, (f) conditionals, (g) iterations, and (h) functions. We start by presenting these concepts with step-by-step examples of programs written in the Python language \u2013 a language whose simplicity seduces the most unwilling learners. While reviewing these basic ideas we also present some programming constructs that are especially relevant for NLP; these include strings, corpora, text files, input-output (I/O), etc. Before we begin, it is important to consider the two steps involved in writing a program: pseudo-code (planning) and implementation (executing). Finally, we will describe one important aspect of efficient code implementation: incremental programming. Pseudo-code As you will see in the subsequent examples, Python has a quite intuitive syntax. In some respects, Python syntax looks like pseudo-code. Pseudo-code is a high-level description of a program that makes no reference to a particular language. For instance, Table 1 presents pseudo-code for a program that translates sentences in a document. Each line is an instruction. The first line opens an input file and the last line closes it. The lines in between translate each line in the file. Pseudo-code is important because it provides a conceptual representation of what you intend to program, before you do any real programming. Planning by using some kind of pseudo-code (whether purely textual or even graphical) is an important part of conducting a s", "year": 2012, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f801f962c895d5c606cb52db85f099a4ed8c34e2", "title": "Python probabilistic type inference with natural language support", "abstract": "We propose a novel type inference technique for Python programs. Type inference is difficult for Python programs due to their heavy dependence on external APIs and the dynamic language features. We observe that Python source code often contains a lot of type hints such as attribute accesses and variable names. However, such type hints are not reliable. We hence propose to use probabilistic inference to allow the beliefs of individual type hints to be propagated, aggregated, and eventually converge on probabilities of variable types. Our results show that our technique substantially outperforms a state-of-the-art Python type inference engine based on abstract interpretation.", "year": 2016, "citationCount": 85, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f8057f30c36ee171ba9a94ce7c60343a09614107", "title": "Wearable Web Technology : Google Glass and the Mirror API", "abstract": "Glass is a commercial head-mounted display which is currently being developed by Google. This paper provides a concise overview of the history and context of Glass, a technical overview of the hardware and GUI, as well as a overview of the Mirror API in which applications for Glass can be developed. Furthermore, we discuss the advantages and disadvantages of Glass as well as intended and unintended applications. Finally, we provide the reader with a 'getting started' guide in which the basics of development for Glass are explained. 1. PURPOSE, CONTEXT AND HISTORY Google Glass is a wearable computer in the form of a headset that looks like a pair of glasses and lets users accomplish various tasks and receive quick information in a smartphone form. The difference is that Google Glass is used hand-free and does not require the user to look away to receive information, which is officially called a heads-up display [1]. Google Glass was created to ease the user\u2019s daily and social life by taking the bulky and physical part of technology out of the way to help the user connect to the world in a faster way. It is connected to the Internet, and the user can take photographs, record videos, send and receive messages, and stream live recordings, amongst others [2]. It supports Apps that people can develop for Google Glass using its API, which is called Mirror API. The idea of having a wearable computer with a heads-up display is not new. MIT researchers Thad Starner and Steve Mann have been working on these devices for almost half a century. Mann stated the idea behind a wearable computer as \u201cWearable computing, however, is based on the idea that computing is NOT the primary task\u201d. He believes that the wearable computer should augment the user\u2019s senses [3]. Other companies started working on heads-up displays, perhaps sharing Mann\u2019s view, whether knowingly or unknowingly, but with the similar idea to help the user\u2019s life. Car companies were one of the first to include commercially-available heads-up displays on their windshields to give the driver information about directions or speed limits. There are also other entertainment systems that display movies right in front of the user\u2019s eyes on the heads-up display. Other devices similar to Google Glass are currently being developed and also in the form of contact lenses [1]. Despite of other alternatives, Google Glass became the most popular and anticipated heads-up display rather quickly after its announcement. \uf0b7 April 2012: Google Glass was announced on Google+ [4] \uf0b7 June 2012: Google posted videos online that were taken with Google Glass while skydiving, recording, amongst others, to give a hint of what people can do with the technology [4] Also in this month, Google released an \u201cExplorer\u201d version of the Glass for developers to buy for $1,500 [4]. \uf0b7 February 2013: Google started the \u201c#ifihadaglass\u201d contest in which the competitors were asked to post what they would do with Google Glass on Google+ and Twitter. The winners would be awarded with an Explorer edition of Glass with the condition to pay $1,500 for the device and travel to the United States of America to pick it up [5]. \uf0b7 April 2013: Google released the Mirror API for developers to start coding for Google Glass. However, it can only be accessed by people who own a Glass for now [6]. 2. OPERATING PRINCIPLES This section provides an overview of the hardware, software and networking principles of Google Glass and the Mirror API. 2.1 Hardware and Connectivity Since Glass is a wearable device, it should be non-intrusive, lightweight and robust. The device is therefore made out of an extremely flexible titanium frame with all electronics located inside a container underneath the right side arm. This container also includes a touchpad which can be swiped to scroll through menus, or tapped to select items. Another way to control Glass is by using voice-commands starting with the initiation statement 'ok glass' followed by the desired function ('take a picture', 'send a message to..' etcetera). This proves to be very helpful in everyday life situations where hands-free controls are required. In order to provide the Augmented Reality overlay, Glass uses a small projector which beams a 740x360 pixel image into a plastic prism where the rays of light are directly deflected onto the user\u2019s retina. The device contains a microphone and camera capable of taking 5 megapixel pictures and record movies in 720p. The recorded media can be stored on 12 gigabytes of available on-board memory. Glass also comes with a build-in bone conduction transducer, transmitting sounds to the inner-ear through the bones of the user and therefore eliminating the need for headphones or earplugs. The device\u2019s battery is located behind the right ear and, according to Google, has enough capacity to provide the device with power for 'a typical day of use'. Glass runs on Android 4.0.4 (\u2018Ice Cream Sandwich\u2019) and uses the same CPU as the infamous Kindle Reader (OMAP 4430) [7]. The early 'Explorer' edition of Glass contains at least 682MB of RAM memory but Kernel messages suggest a total of 1GB [8]. Google hasn't officially confirmed this. Image: A hardware overview of the Glass device Although Glass is great for recording videos or taking pictures, the real power of the device lies within its possibility to provide the user with information such as navigational instructions, social-media messages and a wide range of location-based services such as public transport information and weather forecasts. However, most of these features require some sort of mobile telecommunications technology or GPS. Glass contains a 802.11b/g WiFi and Bluetooth adapter with which it can connect to mobile devices by using a technique known as 'tethering' in order to access shared resources such as GPS information or a mobile data connection. The 'Explorer' edition of Glass supports tethering on both Androidand iOS devices via a special smartphone app [9]. 2.4 Interaction and interface The Glass interface consists of 640 by 360 pixel 'cards' which are displayed on the right side of the users field of vision. These cards can contain text such as notifications and messages but also HTML, pictures and videos. Cards are always displayed one at a time [10]. The main component of the Glass interface is the so-called 'Timeline', a sorted row of cards through which the user can swipe by using the touchpad. The center of this timeline always shows the clock and the 'ok glass' command. Cards to the left display information about the future, such as the weather or calendar-events. Cards on the right contain information from the past, such as messages, videos and photos. Cards can be combined into so-called 'bundles'. A bundle can be recognized by a white 'fold' in the top-right of the card. By tapping on the bundle card, a new sub-timeline opens and a white bar on the bottom of the card shows the user's location within the timeline [11]. This navigation, consisting of a main timeline, cards and bundles is fixed and provides a intuitive and user-friendly interface. The individual cards however can be styled by using HTML and CSS, although Google strongly suggests using their premade templates in order to ensure a consistent user experience throughout Glass [10]. Image: Examples of Glass 'cards' [f1] 2.2 Glass and Web Technology Although Glass runs on Android, it is not possible to execute services directly onto the device [12]. Applications for Glass are known as 'Glassware' and can be developed by using Google's 'Mirror API'. This API is a set of RESTful services; the style of software architecture several distributed services such as the World Wide Web are based upon [13]. A RESTful service consists of clients that send requests to servers and servers that process these requests and return appropriate responses. Both client and server continuously transfer representations of resources, keeping each other abreast about data available. RESTful services are stateless, which means that they treat each request as an individual transmission. RESTful services use HTTP methods such as 'GET', 'POST', 'PUT' and 'DELETE', to request, supply, replace and delete data [14]. Since the Mirror API is fully web-based, Glassware is always serverside and can be developed in Java, PHP, Python, Go and .NET. Because of this approach, the only way of interacting with Glass is through the cloud [13]. Cloud computing, as used in Glass, is a technology that allows a large number of devices to access and interact with one or multiple services simultaneously [15]. Glassware is deployed to the Google App Engine which is a type of cloud service known as 'PaaS' (Platform as a Service). This distributed development platform allows developers to build and manage their Glassware without having to worry about hosting, scalability and hardware maintenance [16]. The Glassware built within the Google App Engine can be considered 'SaaS' applications (Software as a Service). These applications run within the cloud and can connect to a large number of clients (users) simultaneously. Figure: Stack showing the communication between Glassware and a Glass device using Google's Cloud [f2] Because the applications are approached via the Internet, users don't have to worry about installing and updating Glassware and can easily manage with which applications they want to share information. Users simply select the Glassware they want to use and give the application permission to access their data by sending credentials. The application then stores these credentials and returns a new contact-request to the user through which he or she can access the service. The user is now able to share data such as commands, media and location-based information with the application that runs within the cloud. After receiving data from the user, the application selects an appropriate response which is delivered to the user immediately after Glass syncs wit", "year": 2013, "citationCount": 4, "fieldsOfStudy": null}
{"paperId": "f8ee96ebca1f5a6b467b22efea097c5266ea9193", "title": "Zoom-in on the dust-obscured phase of galaxy formation with gravitational lenses", "abstract": "Over the last 20 years gravitational lensing has become an essential instrument to investigate the structures within the Universe and the Universe itself. It directly traces the gravity of matter, whether baryonic or not, and so it\u2019s essential for a systematic study of dark matter and its distribution on both small and large scales. Moreover, the magnification generated by a foreground lensing system, like a massive elliptical galaxy, on a background source allows us to study high-redshift galaxy structures down to scales difficult to probe with the largest telescope at present, and to detect intrinsically faint objects. \nIn this PhD thesis I describe the advantages that gravitational lensing offers in the study of high-redshift (z > 1.5) dusty star forming galaxies (DSFGs), progenitors of the early-type (ETGs) observed in the local Universe. DSFGs are the major contributors to the cosmic star formation activity in the Universe and, as such, they represent the key to understand the build-up of galaxies. Dust absorption of UV/optical radiation from newborn stars is re-emitted in the far-infrared/sub-mm bands, making DSFGs particularly bright at those wavelengths. \nIn order to extract information from the galaxy-galaxy strong lensing events involving DSFGs, I have written a Python code performing lens modelling and source reconstruction, based on the Regularized Semilinear Inversion method by Warren & Dye (2003), as outlined in Enia et al. (2018). This method reconstructs the intrinsic (i.e. un-lensed) surface brightness of the background galaxy without any analytic pre-assumption on its distribution, while searching in the parameters space for the mass distribution of the lens. Since DSFGs are the main focus of this project, and since they are very bright at FIR/sub-mm wavelengths, I have extended the formalism to the uv plane, in order to deal with interferometric data. In fact, interferometry is the best observational technique to achieve high resolution imaging in the sub-mm/mm bands, thanks to facilities like the Atacama Large Millimeter/sub-millimeter Array (ALMA). Furthermore, since the lens galaxy is usually a massive elliptical, the main emission in the sub-mm/mm is due almost exclusively from the background galaxy. DSFGs show extremely steep number counts, so that any DSFGs with a very high flux density (e.g. above \u223c 100mJy at 500\u03bcm) is expected to be lensed. The selection of gravitationally lensed galaxies based on a simple cut in flux density has proven to be extremely efficient in the search for these sources in wide area extragalactic surveys such as the Herschel Astrophysical Terahertz Large Area Survey (H-ATLAS). This survey found 80 candidate lensed galaxies, 20 of which have already been confirmed to be lensing systems by a number of follow-up observations mainly with the Sub-Millimeter Array (SMA), the Hubble Space Telescope and the Keck telescope. \nIn Enia et al. (2018) I have applied my code to the SMA observations of 12 strongly lensed galaxies from the H-ATLAS in order to derive their morphologies, sizes and magni cations. The derived lens model parameters are in general consistent with previous findings (i.e. Bussmann et al., 2013), however the estimated magnification factors, ranging from 3 to 10, are lower. These discrepancies are observed in particular where the reconstructed source hints at the presence of multiple knots of emission. An appropriate estimate of the magnification factor is essential to properly retrieve the physical properties of the sources, i.e. CO line luminosities, star formation rates, or SFR surface densities. \nIn Massardi, Enia et al., 2018 multiwavelength observations of two strongly lensed sources are presented. H-ATLAS J090740.0-004200, also known as SDP.9, and H-ATLAS J091043.1-000322, also known as SDP.11, both come from the H-ATLAS sample. The observations were carried out with Chandra, HST and ALMA, covering a large portion of the electromagnetic spectrum. These multiwavelength observations probed the presence of highly obscured nuclear activity in the galaxy, with X-ray emissions generated in the nuclear area, allowing an insight on the co-evolution between the central SMBH and the galaxy, as predicted by various evolutionary theories for galaxy formation and evolution. I applied the code to SDP.9, reconstructing the background source in the different bands, obtaining a clear cospatiality in the source plane between the sub-mm emission, tracing the star formation, and the X-ray signal, tracing the nuclear activity, within a circle of \u223c 400 pc diameter. This analysis will be further exploited in the future thanks to the large number of follow-up campaigns in different wavelength ranges currently ongoing. \nIn Rodighiero, Enia et al., ApJL submitted, a study of the statistical properties of a sample of dusty sources with very efficient star formation rates (SFR) is performed, in order to understand the role of enhanced SFR in triggering the Black-Hole Accretion Rate. These sources are Herschel-selected in the COSMOS field, with SFRs elevated 4\u00d7 above the star-forming \u201dmain sequence\u201d, classifying them as starbursts (SB). Here, by means of a multicomponent spectral energy distribution fitting analysis, the contribution of stars, AGN torus, and star formation to the total emission at different wavelengths is separated, spanning the range from the UV to the far-IR. The sample is divided into active SBs (dominated by an AGN emission, SBs-AGN) and purely star-forming SBs (SBs-SFR). From visual inspection of the HST-UV morphology, the two classes have statistically different morphologies: SBs-SFR are generally irregular systems, while a large majority (\u223c 65%) of SBs-AGN are instead dominated by regular compact and symmetric morphologies. Searching in the ALMA public archive, I found continuum counterparts with a secure detection above 3\u03c3 for 24 galaxies (10 SBs-AGN and 14 SBs-SFR). Then, dust and total molecular gas masses are computed, finding that SBs turn to be gas rich systems \n(fgas =45%\u221285%),with similar gas fractions in the two classes, and therefore no direct evidence of AGN feedback depleting the parent hosts. This results are discussed in the context of the co-evolution scenario. The SB population is consistent with a mixture of: low-mass primordial galaxies, rapidly accreting their M\u2217 together with their MBH (mainly the more compact SBs-AGN), and a class of highly star-forming merging systems (dominating the SBs-SFR). Anyway, feedback effects have not reduced yet the fgas of the objects. Alternatively, feedback processes (in form of galactic outflows from the SMBH) are not efficient enough to significantly deplete the gas masses of the host galaxies.", "year": 2019, "citationCount": 0, "fieldsOfStudy": ["Physics"]}
{"paperId": "008e160ba46f49bb0513875525103d0a0bb902b5", "title": "Developing a Comprehensive Framework for Multimodal Feature Extraction", "abstract": "Feature extraction is a critical component of many applied data science workflows. In recent years, rapid advances in artificial intelligence and machine learning have led to an explosion of feature extraction tools and services that allow data scientists to cheaply and effectively annotate their data along a vast array of dimensions--ranging from detecting faces in images to analyzing the sentiment expressed in coherent text. Unfortunately, the proliferation of powerful feature extraction services has been mirrored by a corresponding expansion in the number of distinct interfaces to feature extraction services. In a world where nearly every new service has its own API, documentation, and/or client library, data scientists who need to combine diverse features obtained from multiple sources are often forced to write and maintain ever more elaborate feature extraction pipelines. To address this challenge, we introduce a new open-source framework for comprehensive multimodal feature extraction. Pliers is an open-source Python package that supports standardized annotation of diverse data types (videos, images, audio, and text), and is expressly implemented with both ease-of-use and extensibility in mind. Users can apply a wide range of pre-existing feature extraction tools to their data in just a few lines of Python code, and can also easily add their own custom extractors by writing modular classes. A graph-based API enables rapid development of feature extraction pipelines that output results in a single, standardized format. We describe the package's architecture, detail its advantages over previous feature extraction toolboxes, and use a sample application to a large functional MRI dataset to illustrate how pliers can significantly reduce the time and effort required to construct simple feature extraction workflows while increasing code clarity and maintainability.", "year": 2017, "citationCount": 45, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0140f2155376a20a06d6c7cd698b0ccd8bec136a", "title": "Database Creator for Mass Analysis of Peptides and Proteins, DC-MAPP: A Standalone Tool for Simplifying Manual Analysis of Mass Spectral Data to Identify Peptide/Protein Sequences.", "abstract": "Proteomic studies typically involve the use of different types of software for annotating experimental tandem mass spectrometric data (MS/MS) and thereby simplifying the process of peptide and protein identification. For such annotations, these softwares calculate the m/z values of the peptide/protein precursor and fragment ions, for which a database of protein sequences must be provided as an input file. The calculated m/z values are stored as another database, which the user usually cannot view. Database Creator for Mass Analysis of Peptides and Proteins (DC-MAPP) is a novel standalone software that can create custom databases for \"viewing\" the calculated m/z values of precursor and fragment ions, prior to the database search. It contains three modules. Peptide/Protein sequences as per user's choice can be entered as input to the first module for creating a custom database. In the second module, m/z values must be queried-in, which are searched within the custom database to identify protein/peptide sequences. The third module is suited for peptide mass fingerprinting, which can be used to analyze both ESI and MALDI mass spectral data. The feature of \"viewing\" the custom database can be helpful not only for better understanding the search engine processes, but also for designing multiple reaction monitoring (MRM) methods. Post-translational modifications and protein isoforms can also be analyzed. Since, DC-MAPP relies on the protein/peptide \"sequences\" for creating custom databases, it may not be applicable for the searches involving spectral libraries. Python language was used for implementation, and the graphical user interface was built with Page/Tcl, making this tool more user-friendly. It is freely available at https://vit.ac.in/DC-MAPP/.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "025c576a8afa49a72bcfd2137e2ac5e4a1271c3a", "title": "SNPAAMapper-Python: A highly efficient genome-wide SNP variant analysis pipeline for Next-Generation Sequencing data", "abstract": "Currently, there are many publicly available Next Generation Sequencing tools developed for variant annotation and classification. However, as modern sequencing technology produces more and more sequencing data, a more efficient analysis program is desired, especially for variant analysis. In this study, we updated SNPAAMapper, a variant annotation pipeline by converting perl codes to python for generating annotation output with an improved computational efficiency and updated information for broader applicability. The new pipeline written in Python can classify variants by region (Coding Sequence, Untranslated Regions, upstream, downstream, intron), predict amino acid change type (missense, nonsense, etc.), and prioritize mutation effects (e.g., synonymous > non-synonymous) while being faster and more efficient. Our new pipeline works in five steps. First, exon annotation files are generated. Next, the exon annotation files are processed, and gene mapping and feature information files are produced. Afterward, the python scrips classify the variants based on genomic regions and predict the amino acid change category. Lastly, another python script prioritizes and ranks the mutation effects of variants to output the result file. The Python version of SNPAAMapper accomplished the overall speed by running most annotation steps in a substantially shorter time. The Python script can classify variants by region in 53 s compared to 166 s for the Perl script in a test sample run on a Latitude 7480 Desktop computer with 8GB RAM and an Intel Core i5-6300 CPU @ 2.4Ghz. Steps of predicting amino acid change type and prioritizing mutation effects of variants were executed within 1 s for both pipelines. SNPAAMapper-Python was developed and tested on the ClinVar database, a NCBI database of information on genomic variation and its relationship to human health. We believe our developed Python version of SNPAAMapper variant annotation pipeline will benefit the community by elucidating the variant consequence and speed up the discovery of causative genetic variants through whole genome/exome sequencing. Source codes, test data files, instructions, and further explanations are available on the web at https://github.com/BaiLab/SNPAAMapper-Python.", "year": 2022, "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "031c94ddfab775a727474f5cd55daaddde799e0d", "title": "ChocoPy: a programming language for compilers courses", "abstract": "ChocoPy is a programming language designed for teaching an undergraduate course on programming languages and compilers. ChocoPy is a restricted subset of Python 3.6, using static type annotations to enforce compile-time type safety. ChocoPy is fully specified using formal grammar, typing rules, and operational semantics. Valid ChocoPy programs can be executed in a standard Python interpreter, producing results consistent with ChocoPy semantics. A major component of CS164 at UC Berkeley is the project: students develop a full compiler for ChocoPy, targeting RISC-V, in about twelve weeks. In other exercises, students extend the syntax, type system, and formal semantics to support additional features of Python. In this paper, we outline (1) the motivations for creating the ChocoPy project, (2) salient features of the language, (3) the resources provided to students to develop their compiler, (4) some insights gained from teaching two semesters of ChocoPy-based courses by different instructors. Our assignment resources are available for re-use by other instructors and institutions.", "year": 2019, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "050820fe1062722723f4b759a154021f8ef91502", "title": "Physical Type Tracking through Minimal Source-Code Annotation", "abstract": ": One of many common artefacts of complex software systems that often needs to be tracked through the entirety of the software system is the underlying type to which numerical variables refer. Commonly-used languages used in industry provide complex mechanisms through which general objects are associated to a given type: for example, the class (and template ) mechanisms in Python (and C++) are extremely rich mechanisms for the construction of types with almost entirely arbitrary associated operation sets. However, one often deals with software objects that ultimately represent numerical entities corresponding to real-world measurements, even through standardised SI units: metres per second, kilogram metres per second-squared, etc. In such situations, one can be left with insuf\ufb01cient and ineffective type-checking: for example, the C double type will not prevent the erroneous addition of values representing velocity (with SI units metre per second ) to values representing mass (SI unit kilo-gram ). We present an addition to the C language, de\ufb01ned through the existing attribute mechanism, that allows automatic control of physical types at compile-time; the only requirement is that individual variables be identi\ufb01ed at declaration time with appropriate SI (or similar) units.", "year": 2014, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "062b7d793fe46ca48af5327715d63db9a17898c5", "title": "OpenAnnotateApi: Python and R packages to efficiently annotate and analyze chromatin accessibility of genomic regions", "abstract": "Abstract Summary Chromatin accessibility serves as a critical measurement of physical contact between nuclear macromolecules and DNA sequence, providing valuable insights into the comprehensive landscape of regulatory mechanisms, thus we previously developed the OpenAnnotate web server. However, as an increasing number of epigenomic analysis software tools emerged, web-based annotation often faced limitations and inconveniences when integrated into these software pipelines. To address these issues, we here develop two software packages named OpenAnnotatePy and OpenAnnotateR. In addition to web-based functionalities, these packages encompass supplementary features, including the capability for simultaneous annotation across multiple cell types, advanced searching of systems, tissues and cell types, and converting the result to the data structure of mainstream tools. Moreover, we applied the packages to various scenarios, including cell type revealing, regulatory element prediction, and integration into mainstream single-cell ATAC-seq analysis pipelines including EpiScanpy, Signac, and ArchR. We anticipate that OpenAnnotateApi will significantly facilitate the deciphering of gene regulatory mechanisms, and offer crucial assistance in the field of epigenomic studies. Availability and implementation OpenAnnotateApi for R is available at https://github.com/ZjGaothu/OpenAnnotateR and for Python is available at https://github.com/ZjGaothu/OpenAnnotatePy.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "072fa0d5f856a6931b530e954fb57b362438f916", "title": "GOGrapher: A Python library for GO graph representation and analysis", "abstract": null, "year": 2009, "citationCount": 13, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "07d7305f09d843b8fba7552fa8104b8f188f3f38", "title": "DLInfer: Deep Learning with Static Slicing for Python Type Inference", "abstract": "Python programming language has gained enor-mous popularity in the past decades. While its flexibility signifi-cantly improves software development productivity, the dynamic typing feature challenges software maintenance and quality assurance. To facilitate programming and type error checking, the Python programming language has provided a type hint mechanism enabling developers to annotate type information for variables. However, this manual annotation process often requires plenty of resources and may introduce errors. In this paper, we propose a deep learning type inference technique, namely DLInfer, to automatically infer the type infor-mation for Python programs. DLInfer collects slice statements for variables through static analysis and then vectorizes them with the Unigram Language Model algorithm. Based on the vectorized slicing features, we designed a bi-directional gated recurrent unit model to learn the type propagation information for inference. To validate the effectiveness of DLInfer, we conduct an extensive empirical study on 700 open-source projects. We evaluate its accuracy in inferring three kinds of fundamental types, including built-in, library, and user-defined types. By training with a large-scale dataset, DLInfer achieves an average of 98.79% Top-1 accuracy for the variables that can get type information through static analysis and manual annotation. Further, DLInfer achieves 83.03% type inference accuracy on average for the variables that can only obtain the type information through dynamic analysis. The results indicate DLInfer is highly effective in inferring types. It is promising to apply it to assist in various software engineering tasks for Python programs.", "year": 2023, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "08286aeb9c9c99b4b4767167ac9d35dc31151068", "title": "multiplierz: an extensible API based desktop environment for proteomics data analysis", "abstract": null, "year": 2009, "citationCount": 76, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "08e97a1a373bb9abccff94d02e3d2cd568559b22", "title": "Multifaceted quality assessment of gene repertoire annotation with OMArk", "abstract": "Assessing the quality of protein-coding gene repertoires is critical in an era of increasingly abundant genome sequences for a diversity of species. State-of-the-art genome annotation assessment tools measure the completeness of a gene repertoire, but are blind to other types of errors, such as gene over-prediction or contamination. We developed OMArk, a software relying on fast, alignment-free sequence comparisons between a query proteome and precomputed gene families across the tree of life. OMArk assesses not only the completeness, but also the consistency of the gene repertoire as a whole relative to closely related species. It also reports likely contamination events. We validated OMArk with simulated data, then performed an analysis of the 1805 UniProt Eukaryotic Reference Proteomes, illustrating its usefulness for comparing and prioritizing proteomes based on their quality measures. In particular, we found strong evidence of contamination in 59 proteomes, and identified error propagation in avian gene annotation resulting from the use of a fragmented zebra finch proteome as reference. OMArk is available on GitHub (https://github.com/DessimozLab/OMArk), as a Python package on PyPi, and as an interactive online tool at https://omark.omabrowser.org/.", "year": 2022, "citationCount": 10, "fieldsOfStudy": ["Biology"]}
{"paperId": "09ae0f50b62330fb5d5fe13f76b855eb8bcc67e5", "title": "The evolution of type annotations in python: an empirical study", "abstract": "Type annotations and gradual type checkers attempt to reveal errors and facilitate maintenance in dynamically typed programming languages. Despite the availability of these features and tools, it is currently unclear how quickly developers are adopting them, what strategies they follow when doing so, and whether adding type annotations reveals more type errors. This paper presents the first large-scale empirical study of the evolution of type annotations and type errors in Python. The study is based on an analysis of 1,414,936 type annotation changes, which we extract from 1,123,393 commits among 9,655 projects. Our results show that (i) type annotations are getting more popular, and once added, often remain unchanged in the projects for a long time, (ii) projects follow three evolution patterns for type annotation usage -- regular annotation, type sprints, and occasional uses -- and that the used pattern correlates with the number of contributors, (iii) more type annotations help find more type errors (0.704 correlation), but nevertheless, many commits (78.3%) are committed despite having such errors. Our findings show that better developer training and automated techniques for adding type annotations are needed, as most code still remains unannotated, and they call for a better integration of gradual type checking into the development process.", "year": 2022, "citationCount": 10, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0adaf1037dd7ae4749c4a67198e687fc79443cca", "title": "Mining of extended signal temporal logic specifications with ParetoLib 2.0", "abstract": null, "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0b11c38a6d938916c840e8e5576f206770fd0588", "title": "Enhancing RDM in Galaxy by integrating RO-Crate", "abstract": "We introduce how the Galaxy research environment (Jalili et al. 2020) integrates with RO-Crate as an implementation of Findable Accessible Interoperable Reproducible Digital Objects (FAIR Digital Objects / FDO) (Wilkinson et al. 2016, Schultes and Wittenburg 2018) and how using RO-Crate as an exchange mechanism of workflows and their execution history helps integrate Galaxy with the wider ecosystem of ELIXIR (Harrow et al. 2021) and the European Open Science Cloud (EOSC-Life) to enable FAIR and reproducible data analysis.\n RO-Crate (Soiland-Reyes et al. 2022) is a generic packaging format containing datasets and their description using standards for FAIR Linked Data. The format is based on schema.org (Guha et al. 2016) annotations in JSON-LD, which allows for rich metadata representation. The RO-Crate effort aims to make best-practice in formal metadata description accessible and practical for use in a wider variety of situations, from an individual researcher working with a folder of data, to large data-intensive computational research environments.\n The RO-Crate community brings together practitioners from very different backgrounds, and with different motivations and use cases. Among the core target users are:\n \n \n \n researchers engaged with computation and data-intensive, workflow-driven analysis;\n \n \n digital repository managers and infrastructure providers;\n \n \n individual researchers looking for a straightforward tool or how-to guide to \u201cFAIRify\u201d their data;\n \n \n data stewards supporting research projects in creating and curating datasets.\n \n \n \n researchers engaged with computation and data-intensive, workflow-driven analysis;\n digital repository managers and infrastructure providers;\n individual researchers looking for a straightforward tool or how-to guide to \u201cFAIRify\u201d their data;\n data stewards supporting research projects in creating and curating datasets.\n Given the wide applicability of RO-Crate and the lack of practical implementations of FDOs, ELIXIR (Harrow et al. 2021) co-opted this initiative as the project to define a common format for research data exchange and repository entries. Thus, during the last year it\u2019s been implemented in a wide range of services, such as: WorkflowHub (Goble et al. 2021) (a registry for describing, sharing and publishing scientific computational workflows) uses RO-Crates as an exchange format to improve reproducibility of computational workflows that follow the Workflow RO-Crate profile (Bacall et al. 2022); LifeMonitor (Leo et al. 2022) (a service to support the sustainability of computational workflows being developed as part of the EOSC-Life project) uses RO-Crate as an exchange format for describing test suites associated with workflows. \n Tools have been developed towards aiding the previously mentioned use cases and increasing the general usability of RO-Crates by providing a user-friendly (programmatic) interface for consumption and production of RO-Crates through programmatic libraries for consuming/producing RO-Crates (ro-crate-py De Geest et al. 2022, ro-crate-ruby Bacall and Whitwell 2022, ro-crate-js Lynch et al. 2021).\n The Galaxy project provides a research environment with data analysis and data management functionalities as a multi user platform, aiming to make computational biology accessible to research scientists that do not have computer programming or systems administration experience. As such, it stores not just analysis related data but also the complete analytical workflow, including its metadata. The internal data model involves the history entity, including all steps performed in a specific analysis, and the workflow entity, defining the structure of an analytical pipeline. From the start, Galaxy aims to enable reproducible analyses by providing capabilities to export (and import) all the analysis history details and workflow data and metadata in a FAIR way. As such it helps its users with the daily research data management. The Galaxy community is continuously improving and adding features, the integration of the FAIR Digital Object principles is a natural next step in this. \n To be able to support these FDOs, Galaxy leverages the RO-Crate Python client library (De Geest et al. 2022) and provides multiple entry points to import and export different research data objects representing its internal entities and associated metadata. These objects include:\n \n \n \n a workflow definition, which is used to share/publish the details of an analysis pipeline, including the graph of tools that need to be executed, and metadata about the data types required\n \n \n individual data files or a collection of datasets related to an analysis history\n \n \n a compressed archive of the entire analysis history including the metadata associated with it such as the tools used, their versions, the parameters chosen, workflow invocation related metadata, inputs, outputs, license, author, CWLProv description (Khan et al. 2019) of the workflow, contextual references in the form of Digital Object Identifiers (DOIs), \u2018EMBRACE Data And Methods\u2019 ontology (EDAM) terms (Ison et al. 2013), etc. \n \n \n \n a workflow definition, which is used to share/publish the details of an analysis pipeline, including the graph of tools that need to be executed, and metadata about the data types required\n individual data files or a collection of datasets related to an analysis history\n a compressed archive of the entire analysis history including the metadata associated with it such as the tools used, their versions, the parameters chosen, workflow invocation related metadata, inputs, outputs, license, author, CWLProv description (Khan et al. 2019) of the workflow, contextual references in the form of Digital Object Identifiers (DOIs), \u2018EMBRACE Data And Methods\u2019 ontology (EDAM) terms (Ison et al. 2013), etc. \n The adoption of RO-crate by Galaxy allows a standardised exchange of FDOs with other platforms in the ELIXIR Tools ecosystem, such as WorkflowHub and LifeMonitor. Integrating RO-Crate deeply into Galaxy and offering import and export options of various Galaxy objects such as Research Objects allows for increased standardisation, improved Research Data Management (RDM) functionalities, smoother user experience (UX) as well as improved interoperability with other systems. The integration in a platform used by biologists to do data intensive analysis, facilitates the publication of workflows and workflow invocations for all skill levels and democratises the ability to perform Open Science.", "year": 2022, "citationCount": 3, "fieldsOfStudy": null}
{"paperId": "0b7509abcf9af50ad4e551be35f11b62ce7d2695", "title": "TypeT5: Seq2seq Type Inference using Static Analysis", "abstract": "There has been growing interest in automatically predicting missing type annotations in programs written in Python and JavaScript. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for code. Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model. We also propose an iterative decoding scheme that incorporates previous type predictions in the model's input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, TypeT5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors -- while enabling easy user intervention.", "year": 2023, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0bc844245ec79ea9d036add3474504d45ef9944d", "title": "Towards a Large-Scale Empirical Study of Python Static Type Annotations", "abstract": "Python, as one of the most popular and important programming languages in the era of data science, has recently introduced a syntax for static type annotations with PEP 484, to improve code maintainability, quality, and readability. However, it is still unknown whether and how static type annotations are used in practical Python projects.This paper presents, to the best of our knowledge, the first and most comprehensive empirical study on the defects, evolution and rectification of static type annotations in Python projects. We first designed and implemented a software prototype dubbed PYSCAN, then used it to scan notable Python projects with diverse domains and sizes and type annotation manners, which add up to 19,478,428 lines of Python code. The empirical results provide interesting findings and insights, such as: 1) we proposed a taxonomy of Python type annotation-related defects, by classifying defects into four categories; 2) we investigated the evolution of type annotation-related defects; and 3) we proposed automatic defect rectification strategies, generating rectification suggestions for 82 out of 110 (74.55%) defects successfully. We suggest that: 1) Python language designers should clarify the type annotation specification; 2) checking tool builders should improve their tools to suppress false positives; and 3) Python developers should integrate such checking tools into their development workflow to catch type annotation-related defects at an early development stage.We have reported our findings and suggestions to Python language designers, checking tool builders, and Python developers. They have acknowledged us and taken actions based on our suggestions. We believe these guidelines would improve static type annotation practices and benefit the Python ecosystem in general.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0d5b2c2fa1b905e66440648a8f571495b508f576", "title": "MSIpred: a python package for tumor microsatellite instability classification from tumor mutation annotation data using a support vector machine", "abstract": null, "year": 2018, "citationCount": 45, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "0e33e862d7ba29de60425fef3038824c108ad01f", "title": "Deep audio embeddings for vocalisation clustering", "abstract": "The study of non-human animals\u2019 communication systems generally relies on the transcription of vocal sequences using a finite set of discrete units. This set is referred to as a vocal repertoire, which is specific to a species or a sub-group of a species. When conducted by human experts, the formal description of vocal repertoires can be laborious and/or biased. This motivates computerised assistance for this procedure, for which machine learning algorithms represent a good opportunity. Unsupervised clustering algorithms are suited for grouping close points together, provided a relevant representation. This paper therefore studies a new method for encoding vocalisations, allowing for automatic clustering to alleviate vocal repertoire characterisation. Borrowing from deep representation learning, we use a convolutional auto-encoder network to learn an abstract representation of vocalisations. We report on the quality of the learnt representation, as well as of state of the art methods, by quantifying their agreement with expert labelled vocalisation types from 7 datasets of other studies across 6 species (birds and marine mammals). With this benchmark, we demonstrate that using auto-encoders improves the relevance of vocalisation representation which serves repertoire characterisation using a very limited number of settings. We also publish a Python package for the bioacoustic community to train their own vocalisation auto-encoders or use a pretrained encoder to browse vocal repertoires and ease unit wise annotation.", "year": 2023, "citationCount": 10, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "0eb1e2554d58d8cfb5f2cc53e9d32fe446d7dd15", "title": "scATAcat: Cell-type annotation for scATAC-seq data", "abstract": "Cells whose accessibility landscape has been profiled with scATAC-seq cannot readily be annotated to a particular cell type. In fact, annotating cell-types in scATAC-seq data is a challenging task since, unlike in scRNA-seq data, we lack knowledge of \u201cmarker regions\u201d which could be used for cell-type annotation. Current annotation methods typically translate accessibility to expression space and rely on gene expression patterns. We propose a novel approach, scATAcat, that leverages characterized bulk ATAC-seq data as prototypes to annotate scATAC-seq data. To mitigate the inherent sparsity of single-cell data, we aggregate cells that belong to the same cluster and create pseudobulk. To demonstrate the feasibility of our approach we collected a number of datasets with respective annotations to quantify the results and evaluate performance for scATAcat. scATAcat is available as a python package at https://github.com/aybugealtay/scATAcat.", "year": 2024, "citationCount": 1, "fieldsOfStudy": ["Biology"]}
{"paperId": "0efe1aa1ac10f916ee7f90ad8da0114c3181ffb8", "title": "PanGraphViewer: A Versatile Tool to Visualize Pangenome Graphs", "abstract": "Pangenome graphs provide a powerful way to present both sequence and structural features in a given genome relative to the typical features of a population. There are different methods of building pangenome graphs, but few tools are available to visualize them. To address this problem, we developed PanGraphViewer, which is written in Python 3 and runs on all major operating systems. The PanGraphViewer package contains two separate versions: a desktop-based application and a web-based application. Compared to other graph viewers that are initially designed to visualize individual genome graphs, PanGraphViewer targets pangenome graphs and allows the viewing of pangenome graphs built from multiple genomes in either the (reference) graphical fragment assembly format or the variant call format (VCF). Apart from visualization of different types of structural variations (SV), PanGraphViewer also integrates genome annotations with graph nodes to analyze insertions or deletions in a particular gene model. The graph node shapes in PanGraphViewer can represent different types of genomic variations when a VCF file is used. Notably, PanGraphViewer displays subgraphs from a chromosome or sequence segment based on any given coordinates. This function is absent from most genome graph viewers. PanGraphViewer is freely available at https://github.com/TF-Chan-Lab/panGraphViewer to facilitate pangenome analysis.", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Biology"]}
{"paperId": "0f2ec436de5a3a6a9a817378f918f029972733fb", "title": "Test-Case Generation for Finding Neural Network Bugs", "abstract": "As neural networks are increasingly included as core components of safety-critical systems, developing effective testing techniques specialized for them becomes crucial. The bulk of the research has focused on testing neural-network models (for instance, their robustness and reliability as classifiers). But neural-network models are defined by writing programs (usually written in a programming language like Python), and there is growing evidence that these neural-network programs often have bugs. Thus, being able to effectively test neural-network programs is instrumental to their dependability. Thispaperpresents aNNoTest : an approach to generating test inputs for neural-network programs. A fundamental challenge is that the dynamically-typed languages (e.g., Python) used to program neural networks cannot express detailed constraints about valid function inputs (e.g., vectors and matrices with certain dimensions). Without knowing these constraints, automated test-case generation is prone to producing many invalid inputs, which trigger spurious failures and are useless for identifying real bugs. To address this problem, we introduce a simple annotation language tailored for expressing valid function inputs in neural-network programs. aNNo-Test inputs an annotated program, and uses property-based testing to generate random inputs that satisfy the validity constraints. In the paper, we also outline guidelines that help reduce the effort needed to write aNNoTest annotations. We evaluated aNNoTest on 19 neural-network programs from Is-lam et al.\u2019s survey [13], which we manually annotated following our guidelines\u2014producing 6 annotations per tested function on average. aNNoTest automatically generated test inputs that revealed 94 bugs, including 63 bugs that the survey reported for these projects. These results suggest that aNNoTest can be a cost-effective approach to finding widespread bugs in neural-network programs.", "year": 2021, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0fc9c8681543bead57099426f72eaeb8fa77aed3", "title": "EagerPy: Writing Code That Works Natively with PyTorch, TensorFlow, JAX, and NumPy", "abstract": "EagerPy is a Python framework that lets you write code that automatically works natively with PyTorch, TensorFlow, JAX, and NumPy. Library developers no longer need to choose between supporting just one of these frameworks or reimplementing the library for each framework and dealing with code duplication. Users of such libraries can more easily switch frameworks without being locked in by a specific 3rd party library. Beyond multi-framework support, EagerPy also brings comprehensive type annotations and consistent support for method chaining to any framework. The latest documentation is available online at this https URL and the code can be found on GitHub at this https URL.", "year": 2020, "citationCount": 6, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "10baebd2264a34b9d14428bcb6baa73fece71eb0", "title": "Where to Start: Studying Type Annotation Practices in Python", "abstract": "Dynamic programming languages have been embracing gradual typing, which supports optional type annotations in source code. Type-annotating a complex and long-lasting codebase is indeed a gradual and expensive process, where two issues have troubled developers. First, there is few guidance about how to implement type annotations due to the existence of non-trivial type practices; second, there is few guidance about which portion of a codebase should be type-annotated first. To address these issues, this paper investigates the patterns of non-trivial type-annotation practices and features of type-annotated code files. Our study detected six patterns of type-annotation practices, which involve recovering and expressing design concerns. Moreover, we revealed three complementary features of type-annotated files. Besides, we implemented a tool for studying optional typing practice. We suggest that: 1) design concerns should be considered to improve type annotation implementation by following at least six patterns; 2) files critical to software architecture could be type-annotated in priority. We believe these guidelines would promote a better type annotation practice for dynamic languages.", "year": 2021, "citationCount": 8, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "115b9c3ed8310f33a1e9f338dce935a620cd4c80", "title": "Type-Based Gradual Typing Performance Optimization", "abstract": "Gradual typing has emerged as a popular design point in programming languages, attracting significant interests from both academia and industry. Programmers in gradually typed languages are free to utilize static and dynamic typing as needed. To make such languages sound, runtime checks mediate the boundary of typed and untyped code. Unfortunately, such checks can incur significant runtime overhead on programs that heavily mix static and dynamic typing. To combat this overhead without necessitating changes to the underlying implementations of languages, we present discriminative typing. Discriminative typing works by optimistically inferring types for functions and implementing an optimized version of the function based on this type. To preserve safety it also implements an un-optimized version of the function based purely on the provided annotations. With two versions of each function in hand, discriminative typing translates programs so that the optimized functions are called as frequently as possible while also preserving program behaviors. We have implemented discriminative typing in Reticulated Python and have evaluated its performance compared to guarded Reticulated Python. Our results show that discriminative typing improves the performance across 95% of tested programs, when compared to Reticulated, and achieves more than 4\u00d7 speedup in more than 56% of these programs. We also compare its performance against a previous optimization approach and find that discriminative typing improved performance across 93% of tested programs, with 30% of these programs receiving speedups between 4 to 25 times. Finally, our evaluation shows that discriminative typing remarkably reduces the overhead of gradual typing on many mixed type configurations of programs. In addition, we have implemented discriminative typing in Grift and evaluated its performance. Our evaluation demonstrations that DT significantly improves performance of Grift", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1164269ea18402f44da2b33ed8571dbef9f59b65", "title": "Casts and costs: harmonizing safety and performance in gradual typing", "abstract": "Gradual typing allows programmers to use both static and dynamic typing in a single program. However, a well-known problem with sound gradual typing is that the interactions between static and dynamic code can cause significant performance degradation. These performance pitfalls are hard to predict and resolve, and discourage users from using gradual typing features. For example, when migrating to a more statically typed program, often adding a type annotation will trigger a slowdown that can be resolved by adding more annotations elsewhere, but since it is not clear where the additional annotations must be added, the easier solution is to simply remove the annotation. To address these problems, we develop: (1) a static cost semantics that accurately predicts the overhead of static-dynamic interactions in a gradually typed program, (2) a technique for efficiently inferring such costs for all combinations of inferrable type assignments in a program, and (3) a method for translating the results of this analysis into specific recommendations and explanations that can help programmers understand, debug, and optimize the performance of gradually typed programs. We have implemented our approach in Herder, a tool for statically analyzing the performance of different typing configurations for Reticulated Python programs. An evaluation on 15 Python programs shows that Herder can use this analysis to accurately and efficiently recommend type assignments that optimize the performance of these programs without sacrificing the safety guarantees provided by static typing.", "year": 2018, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "13c6ad940b0f0298ee7f437d8239ccbca6b240ed", "title": "ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference", "abstract": "In this paper, we present ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a light-weight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its tools are publicly available on GitHub.", "year": 2021, "citationCount": 21, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "14e58291f06db7769a51d20cd336afab2b2e30d8", "title": "The Tanl Pipeline", "abstract": "Tanl (Natural Language Text Analytics) is a suite of tools for text analytics based on the software architecture paradigm of data pipelines. Tanl pipelines are data driven, i.e. each stage pulls data from the preceding stage and transforms them for use by the next stage. Since data is processed as soon as it becomes available, processing delay is minimized improving data throughput. The processing modules can be written in C++ or in Python and can be combined using few lines of Python scripts to produce full NLP applications. Tanl provides a set of modules, ranging from tokenization to POS tagging, from parsing to NE recognition. A Tanl pipeline can be processed in parallel on a cluster of computers by means of a modified version of Hadoop streaming. We present the architecture, its modules and some sample applications. Introduction Text analytics involves many tasks ranging from simple text collection, extraction, and preparation to linguistic syntactic and semantic analysis, cross reference analysis, intent mining and finally indexing and search. A complete system must be able to process textual data of any size and structure, to extract words, to classify documents into categories (taxonomies or ontologies), and to identify semantic relationships. A full analytics application requires coordinating and combining several tools designed to handle specific subtasks. This may be challenging since many of the existing tools have been developed independently with different requirements and assumptions on how to process the data. Several suites for NLP (Natural Language Processing) are available for performing syntactic and semantic data analysis, some as open source and other as commercial products. These toolsets can be grouped into two broad software architecture categories: Integrated Toolkits: these provide a set of classes and methods for each task, and are typically bound to a programming language. Applications are programmed using compilers and standard programming environments. Examples in this category are: LingPipe (LingPipe), OpenNlp (OpenNLP), NLTK (NLTK). Component Frameworks: these use generic data structures, described in a language independent formalism, and each tool consumes/produces such data; a special compiler transforms the data descriptions into types for the target programming language. Applications are built using specific framework tools. Examples in this category are: GATE (GATE), UIMA (UIMA). Both GATE and UIMA are based on a workflow software architecture, where the framework handles the workflow among the processing stages of the application, by means of a controller that passes data among the components invoking their methods. Each tool accepts and returns the same type of data and extends the data it receives by adding its own information, as shown using different colors in Figure 1: the Tokenizer adds annotations to represent the start and end of each token, the PosTagger adds annotations representing the POS for each token. Since the controller handles the whole processing in a single flow, each processing component receives the whole collection and returns the whole collection. If the collection is big, this might require large amounts of memory. Figure 1: Workflow Software Architecture. In this paper we present an alternative architecture based on the notion of data pipeline. The Tanl pipeline (Natural Language Text Analytics) uses both generic and specific data structures, and components communicate directly exchanging data through pipes, as shown in Figure 2. Since each tool pulls the data it needs from the previous stage of the pipeline, only the minimum amount of data passes through the pipeline, therefore reducing the memory footprint and improving the throughput. The figure shows single documents being passed along, but the granularity can be even smaller: for instance a module might just require single tokens or single sentences. This would be hard to handle with a workflow architecture, since the controller does not know which amount of data Controller", "year": 2010, "citationCount": 30, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1531e36a20ef3788cf07c318cebd34851c4e17da", "title": "The L3Pilot Common Data Format - Enabling Efficient Automated Driving Data Analysis", "abstract": "Analyzing road-test data is important for developing automated vehicles. L3Pilot is a European pilot project on level 3 automation, including 34 partners among manufacturers, suppliers and research institutions. Targeting around 100 cars and 1000 test subjects, the project will generate large amounts of data. We present a data format, allowing efficient data collection, handling and analysis by multiple organizations. A project of the scope of L3Pilot involves various challenges. Data come from a multitude of heterogeneous sources and are processed by a variety of tools. Recorded data span all data types generated in various vehicular sensors/systems and are enriched with external data sources. Videos supplement time-series data as external files. Derived measures and performance indicators \u2013 required to answer research questions about effectiveness of automated driving \u2013 are processed by analysis partners and included for each test session. As a file format, we chose HDF5, which offers a data model and software libraries for storing and managing data. HDF5 is designed for flexible and efficient I/O and for high volume and complex data. The usage of different computing environments for specific tasks is facilitated by the portability that comes with the format. Portability is also important for exploiting the rising potential within artificial intelligence (e.g. automatic scene detection and video annotation). Based on lessons learned from past field tests, we defined a general frame for the common data format that is aligned with the data processing steps of FESTA \u201cV\u201d evaluation methodology. The definitions include representation of the source signals and a hierarchical structure for including multiple datasets that are gradually supplemented (post-processed or annotated) during the various analysis steps. By using the HDF5 format, analysis partners have the freedom to exploit their familiar tools: MATLAB, Java, Python, R, etc. First comparisons between time-series data in previous projects (e.g. AdaptIVe) and the proposed data format show a reduction in storage size of around 80 %, without losses in performance. Much of that is due to efficient internal compression and structuring of data. Considering the amount of objective data involved in automated driving, this leads to a great benefit, in terms of usability. This paper presents a compact, portable, and extensible format aimed at handling extremely large amounts of field test data collected in automated driving pilots. As a harmonized format between tens of organizations performing tests in the L3Pilot project, the proposed format has the potential to promote data sharing as well as development of common tools and gain popularity for use in other projects. The format is designed to allow efficient storing of data and its iterative processing with analysis and evaluation tools. The format also considers the requirements of AI tools supporting neural network training and use.", "year": 2019, "citationCount": 11, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "15a850e657d16e65a742d219dcff21e4e059ea6d", "title": "Analysis of the mRNA export protein ZC3H11A in HCMV infection and pan-cancer", "abstract": "Background We have previously reported that human cytomegalovirus (HCMV) infection could promote the progression of glioma. Here we discovered a stress-induced nuclear protein ZC3H11A (ZC3) through high-throughput sequencing after HCMV infection, which has been reported recently by our research group in regulating mRNA export under stress conditions. And also, a thorough analysis of ZC3 in pan-cancer and the omics data of ZC3 are yet to be conducted. Methods The transcriptomes of glioma cells after HCMV infection were assessed by RNA sequencing. ZC3 mRNA and protein level following HCMV infection were validated and measured by qRT-PCR and Western-blot. The RNA sequencing and protein expression information of ZC3 across pan-cancer were analyzed and visualized by R packages. The localization of ZC3 protein was assessed by IHC images from HPA. The ZC3 proteomics and transcriptomics data in different cancers were extracted through the CPTAC data portal, and comparisons were conducted with a Python script. The genetic alteration, survival prognosis, immune infiltration analysis of ZC3 in pan-cancer were analyzed by cBioPortal, TCGA, and TIMER2 databases. The protein interaction networks were revealed by STRING, GEPIA2 and TCGA. Results Genes in mRNA processing pathways were upregulated after HCMV infection and ZC3 expression in mRNA and protein level was validated. We also discovered that the status of ZC3 were generally at high levels in cancers, although varied among different cancer types. ZC3 protein in tumor cells localized to the nuclear whereas in normal cells it was mainly found in cytoplasmic/membranous. However, from ZC3 proteomics and transcriptomics data in some cancer types, the increase in ZC3 protein was not accompanied by a significant elevation in mRNA level. Additionally, our analysis indicated that elevated ZC3 expression was primarily linked to a negative prognosis in majority cancers but still varied depending on the cancer types. Our annotation analysis suggested that ZC3-related proteins are mainly involved in mRNA processing clusters. Conclusion We demonstrated that ZC3 significantly impacted by HCMV infection in gliomas. Furthermore, we identified a set of genes exhibiting analogous expression patterns to ZC3H11A in TCGA pan-cancer cohorts, implying a potential functional role for ZC3H11A in mRNA processing. Our study provided valuable insights into the role of a new mRNA export protein ZC3 in HCMV infection and pan-cancer progression. These results lay the foundation for our next research on the regulatory mechanism of ZC3 in virus-infected tumors.", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "16a47e17fa6e4842332423779c4a77be85972242", "title": "RNAvigate: efficient exploration of RNA chemical probing datasets", "abstract": "Abstract Chemical probing technologies enable high-throughput examination of diverse structural features of RNA, including local nucleotide flexibility, RNA secondary structure, protein and ligand binding, through-space interaction networks, and multistate structural ensembles. Deep understanding of RNA structure\u2013function relationships typically requires evaluating a system under structure- and function-altering conditions, linking these data with additional information, and visualizing multilayered relationships. Current platforms lack the broad accessibility, flexibility and efficiency needed to iterate on integrative analyses of these diverse, complex data. Here, we share the RNA visualization and graphical analysis toolset RNAvigate, a straightforward and flexible Python library that automatically parses 21 standard file formats (primary sequence annotations, per- and internucleotide data, and secondary and tertiary structures) and outputs 18 plot types. RNAvigate enables efficient exploration of nuanced relationships between multiple layers of RNA structure information and across multiple experimental conditions. Compatibility with Jupyter notebooks enables nonburdensome, reproducible, transparent and organized sharing of multistep analyses and data visualization strategies. RNAvigate simplifies and accelerates discovery and characterization of RNA-centric functions in biology.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "174adcc7d1df3112c3234d80ecd6d49c14fe6056", "title": "impunity: Enforcing Physical Unit Consistency at Definition Time in Python", "abstract": "We introduce impunity, a Python library that enables static analysis of code annotations to ensure the consistency of physical units. It provides a framework for developers to annotate their Python code with physical units and automatically verifies if the units are compatible and adhere to predefined coherence rules. impunity comes as a decorator to apply on functions: it analyses the source code to check for consistency of physical dimensions, and applies minimal code rewriting if conversions are necessary. Overall, this approach takes the best of type-checking based methods and dynamic methods and provides a robust approach with no overhead at runtime.", "year": 2023, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "180bdbcfa27f50b5b2d959b46b0e95b2324c602d", "title": "Reconstruction of neuronal activity and connectivity patterns in the zebrafish olfactory bulb", "abstract": "In the olfactory bulb (OB), odors evoke distributed patterns of activity across glomeruli that are reorganized by networks of interneurons (INs). This reorganization results in multiple computations including a decorrelation of activity patterns across the output neurons, the mitral cells (MCs). To understand the mechanistic basis of these computations it is essential to analyze the relationship between function and structure of the underlying circuit. \nI combined in vivo twophoton calcium imaging with dense circuit reconstruction from complete serial block-face electron microscopy (SBEM) stacks of the larval zebrafish OB (4.5 dpf) with a voxel size of 9x9x25nm. To address bottlenecks in the workflow of SBEM, I developed a novel embedding and staining procedure that effectively reduces surface charging in SBEM and enables to acquire SBEM stacks with at least a ten-fold increase in both, signal-to-noise as well as acquisition speed. \nI set up a high throughput neuron reconstruction pipeline with >30 professional tracers that is available for the scientific community (ariadne-service.com). To assure efficient and accurate circuit reconstruction, I developed PyKNOSSOS, a Python software for skeleton tracing and synapse annotation, and CORE, a skeleton consolidation procedure that combines redundant reconstruction with targeted expert input. \nUsing these procedures I reconstructed all neurons (>1000) in the larval OB. Unlike in the adult OB, INs were rare and appeared to represent specific subtypes, indicating that different sub-circuits develop sequentially. MCs were uniglomerular whereas inter-glomerular projections of INs were complex and biased towards groups of glomeruli that receive input from common types of sensory neurons. Hence, the IN network in the OB exhibits a topological organization that is governed by glomerular identity. \nCalcium imaging revealed that the larval OB circuitry already decorrelates activity patterns evoked by similar odors. The comparison of inter-glomerular connectivity to the functional interactions between glomeruli indicates that pattern decorrelation depends on specific, non-random inter-glomerular IN projections. Hence, the topology of IN networks in the OB appears to be an important determinant of circuit function.", "year": 2016, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "182c333597ddcdfd2d9c6384af41c9495eeda6a4", "title": "Genome-based characterization of two Colombian clinical Providencia rettgeri isolates co-harboring NDM-1, VIM-2, and other \u03b2-lactamases", "abstract": null, "year": 2020, "citationCount": 10, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "18cf8201acdecfd67259070bd79ffe892306ba7c", "title": "Variant Library Annotation Tool (VaLiAnT): an oligonucleotide library design and annotation tool for saturation genome editing and other deep mutational scanning experiments", "abstract": "Motivation Recent advances in CRISPR/Cas9 technology allow for the functional analysis of genetic variants at single nucleotide resolution whilst maintaining genomic context (Findlay et al., 2018). This approach, known as saturation genome editing (SGE), is a distinct type of deep mutational scanning (DMS) that systematically alters each position in a target region to explore its function. SGE experiments require the design and synthesis of oligonucleotide variant libraries which are introduced into the genome by homology-directed repair (HDR). This technology is broadly applicable to diverse research fields such as disease variant identification, drug development, structure-function studies, synthetic biology, evolutionary genetics and the study of host-pathogen interactions. Here we present the Variant Library Annotation Tool (VaLiAnT) which can be used to generate saturation mutagenesis oligonucleotide libraries from user-defined genomic coordinates and standardised input files. This software package is intentionally versatile to accommodate diverse operability, with species, genomic reference sequences and transcriptomic annotations specified by the user. Genomic ranges, directionality and frame information are considered to allow perturbations at both the nucleotide and amino acid level. Results Coordinates for a genomic range, that may include exonic and/or intronic sequence, are provided by the user in order to retrieve a corresponding oligonucleotide reference sequence. A user-specified range within this sequence is then subject to systematic, nucleotide and/or amino acid saturating mutator functions, with each discrete mutation returned to the user as a separate sequence, building up the final oligo library. If desired, variant accessions from genetic information repositories, such as ClinVar and gnomAD, that fall within the user-specified ranges, will also be incorporated into the library. For SGE library generation, base reference sequences can be modified to include PAM (Protospacer Adjacent Motif) and protospacer \u2018protection edits\u2019 that prevent Cas9 from cutting incorporated oligonucleotide tracts. Mutator functions modify this protected reference sequence to generate variant sequences. Constant regions are designated for non-editing to allow specific adapter annealing for downstream cloning and amplification from the library pool. A metadata file is generated, delineating annotation information for each variant sequence to aid computational analysis. In addition, a library file is generated, which contains unique sequences (any exact duplicate sequences are removed) ready for submission to commercial synthesis platforms. A VCF file listing all variants is also generated for analysis and quality control processes. The VaLiAnT software package provides a novel means to systemically retrieve, mutate and annotate genomic sequences for oligonucleotide library generation. Specific features for SGE library generation can be employed, with other diverse applications possible. Availability and Implementation VaLiAnT is a command line tool written in Python. Source code, testing data, example library input and output files, and executables are available at https://github.com/cancerit/VaLiAnT. A user manual details step by step instructions for software use, available at https://github.com/cancerit/VaLiAnT/wiki. The software is freely available for non-commercial use (see Licence for more details, https://github.com/cancerit/VaLiAnT/blob/develop/LICENSE).", "year": 2021, "citationCount": 5, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "1b50bcdaf6ddbe87fda34300a9331f5cabb188d1", "title": "An automated model annotation system (AMAS) for SBML models", "abstract": "Abstract Motivation Annotations of biochemical models provide details of chemical species, documentation of chemical reactions, and other essential information. Unfortunately, the vast majority of biochemical models have few, if any, annotations, or the annotations provide insufficient detail to understand the limitations of the model. The quality and quantity of annotations can be improved by developing tools that recommend annotations. For example, recommender tools have been developed for annotations of genes. Although annotating genes is conceptually similar to annotating biochemical models, there are important technical differences that make it difficult to directly apply this prior work. Results We present AMAS, a system that predicts annotations for elements of models represented in the Systems Biology Markup Language (SBML) community standard. We provide a general framework for predicting model annotations for a query element based on a database of annotated reference elements and a match score function that calculates the similarity between the query element and reference elements. The framework is instantiated to specific element types (e.g. species, reactions) by specifying the reference database (e.g. ChEBI for species) and the match score function (e.g. string similarity). We analyze the computational efficiency and prediction quality of AMAS for species and reactions in BiGG and BioModels and find that it has subsecond response times and accuracy between 80% and 95% depending on specifics of what is predicted. We have incorporated AMAS into an open-source, pip-installable Python package that can run as a command-line tool that predicts and adds annotations to species and reactions to an SBML model. Availability and implementation Our project is hosted at https://github.com/sys-bio/AMAS, where we provide examples, documentation, and source code files. Our source code is licensed under the MIT open-source license.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "1c1f4d0b49049fd138cd51f3c0f880e691551fa0", "title": "TypeEvalPy: A Micro-Benchmarking Framework for Python Type Inference Tools", "abstract": "In light of the growing interest in type inference research for Python, both researchers and practitioners require a standardized process to assess the performance of various type inference techniques. This paper introduces TYPEEvALPy, a comprehensive micro-benchmarking framework for evaluating type inference tools. TYPE-EVALPy contains 154 code snippets with 845 type annotations across 18 categories that target various Python features. The framework manages the execution of containerized tools, transforms inferred types into a standardized format, and produces meaningful metrics for assessment. Through our analysis, we compare the performance of six type inference tools, highlighting their strengths and limitations. Our findings provide a foundation for further research and optimization in the domain of Python type inference.", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1c5147e3eb4fc9f27303f5e036fa8b2763585cc7", "title": "Static Type Analysis for Python", "abstract": "Python is a kind of dynamic-typed language which provides flexibility but leaves the programmer without the benefits of static typing. This paper describes Type, a tool that works for static type annotation and inference for python. It could simulate the built-in modules, transform the Python source code to IR(Intermediate representation) which we design and annotate, infer and reduce the IR into the type system. Type could provide the explicit type information, detect the type errors at compile time and improve the efficiency of development and the accuracy of point-to analysis. By the evaluation of applying Type to a suite of benchmarks, we find that Type can annotate and infer the types with a good precision and coverage in accept time.", "year": 2014, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1c580446b38e3765a5b120109ba60af8fc9b0548", "title": "Position Paper : Dynamically Inferred Types for Dynamic Languages", "abstract": "Over the past few years we have been developing Diamondback Ruby (DRuby), a tool that brings static type inference to Ruby, a dynamically typed object-oriented language. Developing DRuby required creating a Ruby front-end, which was extremely challenging: like other dynamic languages, Ruby has a complex, yet poorly documented syntax and semantics, which we had to carefully reverse-engineer. Writing our front-end took well over a year, and now that Ruby 1.9 is available, we are faced with the daunting prospect of significant additional effort to discover how the language has changed, and to extend our front-end accordingly. We suspect that maintaining a static analysis system for other dynamic languages, such as Perl or Python, is similarly daunting. To remedy this situation, we recently introduced a new program analysis technique for dynamic languages: constraint-based dynamic type inference, which uses information gathered from dynamic runs to infer static types [1]. More precisely , at run-time we introduce type variables for fields, method arguments, and method return values. As values are passed to those positions, we dynamically wrap them in proxy objects to track the associated type variables. We also allow trusted type annotations for methods, which are stored in Class objects. As wrapped values are used, we generate subtyping constraints on the associated type variables. We solve those constraints at the end of one or more program runs, which produces a satisfying type assignment, if one exists. Importantly, despite relying on dynamic runs, we can prove a soundness theorem: if the dynamic runs from which types are inferred cover every path in the control-flow graph (CFG) of every method of a class, then the inferred types for that class's fields and methods are sound for all possible runs. Note this coverage criterion is in contrast to requiring that every program path is covered. We have implemented this technique for Ruby, as a tool called Rubydust (where \" dust \" stands for dynamic unraveling of static types). An important property of Rubydust is that it requires no front-end; in fact, it is a Ruby library that is loaded at run-time just like any other library. To operate, Rubydust uses Ruby's rich introspection features to wrap objects, intercept method calls, and store and retrieve any type annotations supplied by the programmer. Thus far, we have run Rubydust on a number of small programs, and have found that Rubydust produces correct, readable types. We believe that \u2026", "year": 2010, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "1de53160d48ef6ec31559ee79e21afabc4b12469", "title": "Systematic tissue annotations of genomics samples by modeling unstructured metadata", "abstract": null, "year": 2022, "citationCount": 7, "fieldsOfStudy": ["Medicine"]}
{"paperId": "1e0f707a9ad066664264682078bb1aa25d6f7c74", "title": "QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners", "abstract": "In online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support. However, in real application scenarios, the data resources for training such LLMs are relatively scarce. Therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new Chinese question-and-answer dataset for Python learners. To ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. This annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing the programming teaching assists (TA). Furthermore, we conducted comprehensive evaluations of various LLMs proficient in processing and generating Chinese content, highlighting the potential limitations of general LLMs as intelligent teaching assistants in computer programming courses.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1e655fa69c62b430b051224153f701f1b607fd9c", "title": "Typilus: neural type hints", "abstract": "Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program\u2019s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace \u2014 a continuous relaxation of the discrete space of types \u2014 and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.", "year": 2020, "citationCount": 111, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "20083c79a078b64d4dfe07b4514309ff5cd77581", "title": "Annotation and quality assessment of left ventricular filling and relaxation pattern using one-dimensional convolutional neural network", "abstract": "\n \n \n Type of funding sources: Public grant(s) \u2013 National budget only. Main funding source(s): National Institute For Health Research (NIHR), UK\n \n \n \n Aberrations in left ventricular (LV) filling or relaxation \u2013 known as diastolic dysfunction \u2013 occur in heart failure with preserved ejection fraction. CMR is the reference modality for the assessment of ventricular systolic function, however, its role in evaluation of diastolic function is limited at present. One promising technique to assess diastolic function by CMR is the derivation of LV filling and emptying rates from the volume-time curves of cine images.\n \n \n \n To automatically assess the quality of LV filling-rate curves and annotate the peak emptying and filling rates.\n \n \n \n A previously-described deep-learning network was used to automatically segment the entire cardiac cycle captured by short-axis SSFP cine images from the UK Biobank1. The LV filling-rate curves derived from the volume-time data were smoothed with Savitzky\u2013Golay filter. The peak emptying rate (PER), early peak filling rate (PFR-E) and late peak filling rate (PFR-A) were first annotated by a simple peak finding algorithm from Python Scipy signal module. The preliminary annotated curves were reviewed by five human experts (i) to check for peak-annotation errors and (ii) to provide the curve quality score ranging from 1 to 3 for each peak (score 1 denotes good quality, score 2 represents moderate quality and score 3 indicates poor quality). Higher total score (minimum = 3, maximum = 9), therefore, represents poorer overall curve quality. This expert-annotated dataset was used to train two separate one-dimensional convolutional neural networks (1D-CNN) (Figure\u00a01) for peak annotation and curve quality assessment (QA) using Tensorflow library in Python.\n \n \n \n The data from 6,328 LV filling-rate curves were split into the training and testing sets (80:20). The fine-tuned 1D-CNN comprising six hidden layers with two residual connections annotated the PER, PFR-E and PFR-A with the test-set accuracy of 95%, 95% and 98%, respectively. A second trained 1D-CNN for QA based on similar architecture predicted the overall curve quality score with a small error rate (mean absolute error: 0.46, mean squared error: 0.68). These two networks were used to quality check and label 19,409 UK Biobank CMR studies (See Figure\u00a02 for exemplary results). After removing data from poor-quality curves (quality score \u2265 5), 18,735 studies remained. The mean\u00b1standard deviation of PER, PFR-E and PFR-A are 461\u00b1110 ml/s, 359\u00b1117 ml/s and 336\u00b1120 ml/s, respectively. Ageing is associated with lower PFR-E (\u221258.4 ml/s, 95% confidence interval [CI]: \u221256.1 to \u221260.7 ml/s per decade increment) and higher PFR-A (18.3 ml/s, 95% CI: 15.8 to 20.8 ml/s per decade increment).\n \n \n \n The 1D-CNN models can be used to automatically grade the quality of LV filling rate curves and label important diastolic parameters with a high level of accuracy. The derived data recapitulate impaired LV relaxation pattern associated with ageing and can be used as surrogate indices of diastology by CMR. Figure\u00a01Figure\u00a02\n", "year": 2022, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "220c91ee7634ec51a9b1d14a7816c18cf113c6bc", "title": "Refinement type contracts for verification of scientific investigative software", "abstract": null, "year": 2019, "citationCount": 4, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "2270410d1be75b205715cca5b1743d8250223176", "title": "MACA: Marker-based automatic cell-type annotation for single cell expression data", "abstract": "Summary Accurately identifying cell-types is a critical step in single-cell sequencing analyses. Here, we present marker-based automatic cell-type annotation (MACA), a new tool for annotating single-cell transcriptomics datasets. We developed MACA by testing 4 cell-type scoring methods with 2 public cell-marker databases as reference in 6 single-cell studies. MACA compares favorably to 4 existing marker-based cell-type annotation methods in terms of accuracy and speed. We show that MACA can annotate a large single-nuclei RNA-seq study in minutes on human hearts with ~290k cells. MACA scales easily to large datasets and can broadly help experts to annotate cell types in single-cell transcriptomics datasets, and we envision MACA provides a new opportunity for integration and standardization of cell-type annotation across multiple datasets. Availability and implementation MACA is written in python and released under GNU General Public License v3.0. The source code is available at https://github.com/ImXman/MACA. Contact Yang Xu (yxu71@vols.utk.edu), Sikander Hayat (hayat221@gmail.com)", "year": 2021, "citationCount": 8, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"]}
{"paperId": "22a928de63ee3dc069c6b08d0bd34f89328f68c9", "title": "Evaluating importance of edge types when using graph neural network for predicting return types of Python functions", "abstract": "The static prediction of types for dynamic programming languages is a challenging and important problem. Some success for Python was demonstrated by analyzing docstrings, still, a large portion of code comes without thorough documentation. To target this problem in this work we attempt to predict return type annotations for Python functions by looking at function usage patterns. We analyzed a collection of Python packages and created a graph that captures global relationships between source code elements such as imports, calls, and definitions. Moreover, we train embeddings for functions and evaluate how the performance of predicting return types is affected by removing one of the relationship types from the dataset.", "year": 2020, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "22dc74368c04b983a0494e5fc07da78916145ccf", "title": "Contrastive Learning for Robust Cell Annotation and Representation from Single-Cell Transcriptomics", "abstract": "Batch effects are a significant concern in single-cell RNA sequencing (scRNA-Seq) data analysis, where variations in the data can be attributed to factors unrelated to cell types. This can make downstream analysis a challenging task. In this study, we present a novel deep learning approach using contrastive learning and a carefully designed loss function for learning an generalizable embedding space from scRNA-Seq data. We call this model CELLULAR: CELLUlar contrastive Learning for Annotation and Representation. When benchmarked against multiple established methods for scRNA-Seq integration, CELLULAR outperforms existing methods in learning a generalizable embedding space on multiple datasets. Cell annotation was also explored as a downstream application for the learned embedding space. When compared against multiple well-established methods, CELLULAR demonstrates competitive performance with top cell classification methods in terms of accuracy, balanced accuracy, and F1 score. CELLULAR is also capable of performing novel cell type detection. These findings aim to quantify the meaningfulness of the embedding space learned by the model by highlighting the robust performance of our learned cell representations in various applications. The model has been structured into an open-source Python package, specifically designed to simplify and streamline its usage for bioinformaticians and other scientists interested in cell representation learning.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "22f44225b88b72c5786d2c8148e9aec1a71d58ef", "title": "An Automated Model Annotation System (AMAS) for SBML Models", "abstract": "Motivation Annotations of biochemical models provide details of chemical species, documentation of chemical reactions, and other essential information. Unfortunately, the vast majority of biochemical models have few, if any, annotations, or the annotations provide insufficient detail to understand the limitations of the model. The quality and quantity of annotations can be improved by developing tools that recommend annotations. For example, recommender tools have been developed for annotations of genes. Although annotating genes is conceptually similar to annotating biochemical models, there are important technical differences that make it difficult to directly apply this prior work. Results We present AMAS, a system that predicts annotations for elements of models represented in the Systems Biology Markup Language (SBML) community standard. We provide a general framework for predicting model annotations for a query element based on a database of annotated reference elements and a match score function that calculates the similarity between the query element and reference elements. The framework is instantiated to specific element types (e.g., species, reactions) by specifying the reference database (e.g., ChEBI for species) and the match score function (e.g., string similarity). We analyze the computational efficiency and prediction quality of AMAS for species and reactions in BiGG and BioModels and find that it has sub-second response times and accuracy between 80% and 95% depending on specifics of what is predicted. We have incorporated AMAS into an open-source, pip-installable Python package that can run as a command-line tool that predicts and adds annotations to species and reactions to an SBML model. Availability Our project is hosted at https://github.com/sys-bio/AMAS, where we provide examples, documentation, and source code files. Our source code is licensed under the MIT open-source license. Contact hsauro@uw.edu Supplementary information Supplementary data are available online.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "234de17e61c8d5ee4636fd9407eb9777ad1d506c", "title": "Abstract 4956: A fast and efficient bioinformatics analysis workflow for processing reads from single-cell multiomics assays captured on a microwell-based platform", "abstract": "\n Here, we present a significant update: version 2.0 of our sequencing analysis, it is a comprehensive primary analysis workflow that is up to 7X faster and consumes 2X less disk space than the previous release. We used it to process a library with 20,000 human PBMCs sequenced to a depth of 1.2 billion reads in 2.5 hours of wall-clock time. The pipeline can be used with reads from single-cell whole transcriptome, targeted mRNA, surface antigens (AbSeq), TCR/BCR and Sample Tag libraries captured on the BD Rhapsody\u2122 System platform. The major processing steps include Quality Filtering, Cell Barcode Identification, Read Alignment, Feature Assignment, UMI Error Correction, Identification of Putative Cells, Sample De-Multiplexing, Dimensionality Reduction for Visualization and VDJ Contig Assembly and Annotation. Output files and metrics are available in easy-to-digest formats, including an html report with dynamic visualization. Integrated outputs in Seurat and Scanpy formats combine expression matrices and all cell annotation metadata (e.g., predicted cell type, sample assignment, TCR/BCR sequence and gene segments). These pre-generated files are ready to load into popular single-cell analysis tools. The pipeline uses CWL as the workflow manager and makes use of custom code written in C++ and Python along with various open-source packages for data processing. The pipeline can also process reads from non-human species and includes built-in support for specifying and building custom reference genome indices. In support of high-throughput multiomic discovery studies enabled by the BD Rhapsody\u2122 HT Xpress System, this pipeline has been tested with datasets containing more than 800,000 putative cells and 14.5 billion reads.\n For Research Use Only. Not for use in diagnostic or therapeutic procedures. BD, the BD Logo and BD Rhapsody are trademarks of Becton, Dickinson and Company or its affiliates. \u00a9 2023 BD. All rights reserved. NPM-2535 (v1.0) 1023\n Citation Format: Raghavendra Padmanabhan, Brent Weichel, Amie Radenbaugh, Yuefu Jiang, Charles Weeks, Thomas McCarthy, Youngsook Kim, Anthony Berno, Devon Jensen. A fast and efficient bioinformatics analysis workflow for processing reads from single-cell multiomics assays captured on a microwell-based platform [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2024; Part 1 (Regular Abstracts); 2024 Apr 5-10; San Diego, CA. Philadelphia (PA): AACR; Cancer Res 2024;84(6_Suppl):Abstract nr 4956.", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "2352b3a6e4adc7b010ee5de424079480e3afbfbf", "title": "Goal-driven Answers in the Cards Dialogue Corpus", "abstract": "The resolvedness conditions for questions are not semantically invariant, but rather heavily influenced by the goals and intentions of the discourse participants. One of the chief innovations of recent questionand preference-driven models of pragmatics is to characterize how these high-level contextual features influence language production and comprehension. The goal of this paper is to develop and motivate general techniques for quantitatively and qualitatively assessing such models. The basis for my exploration is the Cards corpus of task-oriented dialogues, a highly structured resource that allows us not only to rigorously interpret the discourse participants\u2019 utterances but also to track their evolving goals and subgoals. The paper\u2019s central experiments provide evidence that the semantic variability of answers to Where are you? is governed by the particular demands of the goal that the question engages. \u00a9 2012 Christopher Potts Cascadilla Proceedings Project Completed July 10, 2012 Proceedings of the 30th West Coast Conference on Formal Linguistics, ed. Nathan Arnett and Ryan Bennett, 1-20. Somerville, MA: Cascadilla Proceedings Project. Goal-driven Answers in the Cards Dialogue Corpus Christopher Potts Stanford University 1. Resolvedness and the task The starting point for this paper is Ginzburg\u2019s (1995a) observation that the resolvedness conditions for questions are not semantically invariant, but rather \u201cfixed in a particular context to a level identified by the goal\u201d of the conversation (p. 466; see also Clark 1979; Clark & Schober 1992; Perrault & Allen 1980; Groenendijk & Stokhof 1984:II; Gibbs & Bryant 2007). Such goal-orientation is especially clear for questions like Where are you?, in which the appropriate level of granularity and sense of location are so highly variable; each of my office, California, the U.S., Linguistics, and still working on the introduction is resolving in some contexts but non-resolving (even downright unhelpful) in others. One of the chief innovations of models like Ginzburg\u2019s is to bring these goals (issues, plans, decision problems) into the pragmatic model, so that they can play a direct role in resolving underspecification, setting bounds on vagueness, and guiding inferences about the discourse participants\u2019 intentions. This is a leading idea of all the recent task-driven and question-driven models of contextual dynamics (Allen, 1991; Hobbs et al., 1993; Roberts, 1996, 2004; Ginzburg, 1995b, 1996; Groenendijk, 1999; Beaver, 2002; B\u00fcring, 2003; Stone et al., 2007; Beaver & Clark, 2008; Groenendijk & Roelofsen, 2009) as well as their game-theoretic and decision-theoretic counterparts (Clark, 1996; Merin, 1997, 1999; Parikh, 2000, 2001; van Rooy, 2003; Benz, 2005; Franke, 2009; J\u00e4ger, To appear; Frank & Goodman, 2012). The goal of this paper is to develop and motivate general techniques for identifying and characterizing this kind of task dependence in real task-oriented dialogue, where the phenomena can be both quantitatively and qualitatively assessed. The basis for my exploration is the Cards corpus, a large collection of dialogues derived from a two-person collaborative game. The most noteworthy feature of this corpus for present purposes is that its transcripts record not just the dialogue exchanged by the players, but also all their actions in the game world, with high enough fidelity that we can faithfully replay the games in their entirety. Of course, we don\u2019t have access to the players\u2019 beliefs and intentions, but we do have enough linguistic and behavioral cues to make confident inferences about them. My focus is on the corpus\u2019s 800+ answers to the (implicit or explicit) questionWhere are you? Even in the highly constrained Cards world, these answers come at numerous levels of granularity, frommiddle of the board to 2 spaces to the left of the gap underneath the entrance to the middle room. Intuitively, the preferred granularity is governed by the players\u2019 subgoal at that point in the game. For example, where they want to divide up the board for general exploration, very general answers are preferred. When they need to meet or find specific things, only highly specific answers are resolving. After describing the Cards corpus in some detail (sec. 2), I look more closely at the nature of these locative answers (sec. 3), and then begin the process of setting up an experiment to test the hypothesis of goal-orientation. This involves some semantic annotation (sec. 4) and, more interestingly, a technique for interpreting these annotations in the Cards world itself (sec. 5). With these preliminaries in place, it is straightforward to define a precise notion of granularity and show that it correlates with the players\u2019 subgoals (sec. 6). The paper closes with a more precise theoretical explanation for this correlation (sec. 7) and some suggestions for additional experiments along these lines that could be conducted fairly easily \u21e4 Thanks to Pranav Anand, Mike Frank, Noah Goodman, Jesse Harris, Dan Lassiter, Sven Lauer, Adam Vogel, and audiences at WCCFL 30, NYU, and the University of Rochester for valuable comments, suggestions, and discussion. Thanks also to Alex Djalali, Sven Lauer, Karl Schultz, and the whole SUBTLE team for help with corpus collection. This research was supported in part by ONR grant No. N00014-10-1-0109 and ARO grant No. W911NF-07-1-0216. \u00a9 2012 Christopher Potts Cascadilla Proceedings Project Completed July 10, 2012 You are on 2D Yellow boxes mark cards in your line of sight. Task description: Six consecutive cards of the same suit TYPE HERE The cards you are holding Move with the arrow keys or these buttons. Figure 1: An annotated version of the Cards gameboard. with the Cards corpus (sec. 8). The corpus is available at http://cardscorpus.christopherpotts.net/. The distribution includes the transcripts and starter code for working with them in Python and R, and the site provides a search and visualization function. I have also posted the annotations and R functions for extracting and visualizing the denotations used in this paper, and for running the experiments of sec. 6.", "year": 2012, "citationCount": 39, "fieldsOfStudy": null}
{"paperId": "23ff29e7a88af7c58fdf4ca18d56d8d57b3a3345", "title": "PyTy: Repairing Static Type Errors in Python", "abstract": "Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based re-pair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.", "year": 2024, "citationCount": 6, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "259ace68b5b776e9e8a3c756eb34b95cb723fdf8", "title": "Abstract 2066: Visualization and analysis of cancer genomics data using UCSC Xena", "abstract": "\n UCSC Xena (http://xena.ucsc.edu/) is a web-based visual integration and exploration tool for multi-omic data and associated clinical and phenotypic annotations. Researchers can easily view and explore public data, their own private data, or both using the Xena Browser. Private data are kept on the researcher's computer and are never uploaded to our public servers. We support Mac, Windows, and Linux.\n Questions Xena can help you answer:\n * Is overexpression of this gene associated with lower/higher survival?\n * What genes are differentially expressed between these two groups of samples?\n * What is the relationship between mutation, copy number, expression, etc for this gene?\n Xena showcases seminal cancer genomics datasets from TCGA, the Pan-Cancer Atlas, GDC, PCAWG, ICGC, and more; a total of more than 1500 datasets across 50 cancer types. We support virtually any type of functional genomics data: SNPs, INDELs, copy number variation, gene expression, ATAC-seq, DNA methylation, exon-, transcript-, miRNA-, lncRNA-expression and structural variants. We also support clinical data such as phenotype information, subtype classifications and biomarkers. All of our data is available for download via python or R APIs, or using our URL links.\n Our signature Visual Spreadsheet view shows multiple data types side-by-side enabling discovery of correlations across and within genes and genomic regions. We also have dynamic Kaplan-Meier survival analysis, powerful filtering and subgrouping, differential gene expression analysis, charts, statistical analyses, genomic signatures, and the ability to generate URLs to live views. We link out to the UCSC Genome Browser as well as MuPIT/CRAVAT and TumorMap.\n New features include:\n * Genome-wide differential gene expression analysis\n * Select samples directly from the screen for filtering and creating subgroups\n * Violin plots on any numerical data\n * Loading of Microsoft Excel files\n Our beta prototype site for visualizing single-cell data delivers million-cell-scale multi-omics data for interactive visualization in a web browser. Contact us for access to our beta prototype site.\n If you use us please cite our publication in Nature Biotechnology: https://www.nature.com/articles/s41587-020-0546-8\n Citation Format: Mary Goldman, Brian Craft, Jingchun Zhu, David Haussler. Visualization and analysis of cancer genomics data using UCSC Xena [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2023; Part 1 (Regular and Invited Abstracts); 2023 Apr 14-19; Orlando, FL. Philadelphia (PA): AACR; Cancer Res 2023;83(7_Suppl):Abstract nr 2066.", "year": 2023, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "26383131a0feb343c2b121a2fe00905f5363d5ae", "title": "Advanced Graph-Based Deep Learning for Probabilistic Type Inference", "abstract": "Dynamically typed languages such as JavaScript and Python have emerged as the most popular programming languages in use. Important benefits can accrue from including type annotations in dynamically typed programs. This approach to gradual typing is exemplified by the TypeScript programming system which allows programmers to specify partially typed programs, and then uses static analysis to infer the remaining types. However, in general, the effectiveness of static type inference is limited and depends on the complexity of the program's structure and the initial type annotations. As a result, there is a strong motivation for new approaches that can advance the state of the art in statically predicting types in dynamically typed programs, and that do so with acceptable performance for use in interactive programming environments. Previous work has demonstrated the promise of probabilistic type inference using deep learning. In this paper, we advance past work by introducing a range of graph neural network (GNN) models that operate on a novel type flow graph (TFG) representation. The TFG represents an input program's elements as graph nodes connected with syntax edges and data flow edges, and our GNN models are trained to predict the type labels in the TFG for a given input program. We study different design choices for our GNN models for the 100 most common types in our evaluation dataset, and show that our best two GNN configurations for accuracy achieve a top-1 accuracy of 87.76% and 86.89% respectively, outperforming the two most closely related deep learning type inference approaches from past work -- DeepTyper with a top-1 accuracy of 84.62% and LambdaNet with a top-1 accuracy of 79.45%. Further, the average inference throughputs of those two configurations are 353.8 and 1,303.9 files/second, compared to 186.7 files/second for DeepTyper and 1,050.3 files/second for LambdaNet.", "year": 2020, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "27a118ef5bce2fc62b97f1c537fa1e31e1f80587", "title": "Julia: dynamism and performance reconciled by design", "abstract": "Julia is a programming language for the scientific community that combines features of productivity languages, such as Python or MATLAB, with characteristics of performance-oriented languages, such as C++ or Fortran. Julia's productivity features include: dynamic typing, automatic memory management, rich type annotations, and multiple dispatch. At the same time, Julia allows programmers to control memory layout and leverages a specializing just-in-time compiler to eliminate much of the overhead of those features. This paper details the design choices made by the creators of Julia and reflects on the implications of those choices for performance and usability.", "year": 2018, "citationCount": 72, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "29fc7ff02e68537a86d32978bfd2e8e45258e77d", "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks", "abstract": "As gradual typing becomes increasingly popular in languages like Python and Typescript, there is a growing need to infer type annotations. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully inferred by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for Typescript based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate information between related type variables and eventually make type predictions. Our neural architecture can predict both standard types, like number or string, as well as user-defined types that have not been encountered during training. Our experimental results show that our approach outperforms prior work in this space by 14% (absolute) on library types, while having the ability to make type predictions that are out of scope for existing techniques.", "year": 2020, "citationCount": 102, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "2bbe7c182af82c09005c32e6692a7de808b48cc3", "title": "Snek: Overloading Python Semantics via Virtualization", "abstract": "The Python language enjoys widespread adoption in a wide variety of domains spanning machine learning, scientific and high performance computing, and beyond. While implemented as libraries in Python, many Python frameworks aspire to be a domain-specific language (DSL), and often aim to bypass the Python interpreter. However, because Python\u2019s inherent ability to overload built-in structures such as declarations, conditionals, or loops is limited, these frameworks are often constrained to a suboptimal, API-centric interface that replicates these built-ins with framework-specific semantics. Such an approach ultimately yields productivity losses for programmers, especially when switching between or mixing frameworks, as users must memorize an ever-expanding list of method calls for performing even simple tasks such as constructing control flow. Furthermore, API designers are forced to create new, substituting abstractions for traditional programming constructs, forcing a steep learning curve for users. In this paper, we propose a structured methodology to allow DSL developers to use the whole of Python as a front-end, rather than creating equivalent APIs or relying on shims. Our methodology provides an extensive operator overloading and virtualization mechanism through the use of source code transformations, and enables powerful mechanisms like type-based multi-stage programming (which is popular in statically typed languages), without requiring explicit type information (e.g., via type annotations). We implement this methodology in a system called Snek, which represents the first type-driven multi-stage programming framework for a dynamic language which does not require extra-linguistic mechanisms, and demonstrate the ability to quickly and easily provide new semantics for Python constructs.", "year": 2019, "citationCount": 2, "fieldsOfStudy": null}
{"paperId": "2bd9b519cb5e6bfe448ba079077f7c14822d5260", "title": "Understanding How CodeLLMs (Mis)Predict Types with Activation Steering", "abstract": "CodeLLMs are transforming software development as we know it. This is especially true for tasks where rule-based approaches fall short, like type prediction. The type prediction task consists in adding a new type annotation to a partially typed program, such that the resulting program is closer to being fully typed. The intractability of rule-based approaches and high cost of manual annotation make CodeLLMs an attractive solution to the problem. However, CodeLLMs are still far from being deployed on the large-scale due to doubts surrounding their reliability. To shed some light on how CodeLLMs approach type prediction, we investigate what happens when a model mispredicts a type. We show that by applying semantics-preserving edits to code, CodeLLMs are eventually misled into mispredicting type annotations. However, by leveraging activation steering we are able to\"steer\"the model back to the correct prediction, making models more robust against semantically irrelevant prompt features. We show that steering achieves comparable performance to fine-tuning directly on the type prediction task. Furthermore, we find that steering vectors computed from Python code are effective at correcting TypeScript mispredictions, and vice versa. To our knowledge, this is the first evidence of its kind to suggest that CodeLLMs learn task representations that transfer across languages.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "2c515e8fd566f84670a94620eee16649a8920b6d", "title": "Getting the Roles Right: Using FrameNet in NLP", "abstract": "The FrameNet lexical database (Fillmore & Baker 2010; Ruppenhofer et al. 2006) http://framenet.icsi. berkeley.edu), covers roughly 13,000 lexical units (word senses) for the core Engish lexicon, associating them with roughly 1,200 fully defined semantic frames; these frames and their roles cover the majority of event types in everyday, non-specialist text, and they are documented with 200,000 manually annotated examples. This tutorial will teach attendees what they need to know to start using the FrameNet lexical database as part of an NLP system. We will cover the basics of Frame Semantics, explain how the database was created, introduce the Python API and the state of the art in automatic frame semantic role labeling systems; and we will discuss FrameNet collaboration with commercial partners. Time permitting, we will present new research on frames and annotation of locative relations, as well as corresponding metaphorical uses, along with information about how frame semantic roles can aid the interpretation of metaphors.", "year": 2015, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "2f70ef1fa023b087ab266b368a3430b51e6621bd", "title": "Data Fusion on the CANDELA Cloud Platform", "abstract": "\n <p>This abstract describes the Data Fusion tool of the Horizon 2020 CANDELA project. Here, Sentinel-1 (synthetic aperture radar) and Sentinel-2 (multispectral) satellite images are fused at feature level. This fusion is made by extracting the features from each type of image; then these features are combined in a new block within the Data Model Generation sub-module of the Data Fusion system.</p><p>The corresponding tool has already been integrated with the CANDELA cloud platform: its Data Model component on the platform is acting as backend, and the user interaction component on the local user machine as frontend. There are four main sub-modules: Data Model Generation for Data Fusion (DMG-DF), DataBase Management System (DBMS), Image Search and Semantic Annotation (ISSA), and multi-knowledge and Query (QE). The DMG-DF and DBMS sub-modules have been dockerized and deployed on the CANDELA platform. The ISSA and QE sub-modules require user inputs for their interactive interfaces. They can be started as a standard Graphical User Interface (GUI) tool which is linked directly to the database on the platform.</p><p>Before using the Data Fusion tool, users have to prepare the already co-registered Sentinel-1 and Sentinel-2 products as inputs. The S1tiling service provided on the platform is able to cut out the overlapping Sentinel-1 area based on Sentinel-2 tile IDs.</p><p>The pipeline of the Data Fusion tool starts from the DMG-DF process on the platform, and the data will be transferred via Internet; then local end users can perform semantic annotations. The annotations will be ingested into the database on the platform via Internet.</p><p>The Data Fusion process consists of three steps:</p><ul><li>On the platform, launch a Jupyter notebook for Python, and start the Data Model Generation for Data Fusion to process the prepared Sentinel-1 and Sentinel-2 products which cover the same area;</li>\n<li>On the local user machine, by clicking the Query button of the GUI, users can get access to the remote database, make image search and queries, and perform semantic annotations by loading quick-look images of processed Sentinel-1 and Sentinel-2 products via Internet. Feature fusion and image quick-look pairing are performed at runtime. The fused features and paired quick-looks help obtain better semantic annotations. When clicking on another ingestion button, the annotations are ingested into the database on the platform;</li>\n<li>On the platform, launch a Jupyter notebook for Python, and the annotations and the processed product metadata can be searched and queried.</li>\n</ul><p>Our preliminary validation results are made based on visual analysis, by comparing the obtained classification maps with already available CORINE land cover maps. In general, our fused results generate more complete classification maps which contain more classes.</p>\n", "year": 2020, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "30b7b1adb6b67ad84183400e1375dc073cdcf1ac", "title": "preon: Fast and accurate entity normalization for drug names and cancer types in precision oncology", "abstract": "Motivation In precision oncology, clinicians are aiming to find the best treatment for any patient based on their molecular characterization. A major bottleneck is the annotation and evaluation of individual variants, for which usually a range of knowledge bases are manually screened. To incorporate and integrate the vast information of different databases, fast and accurate methods for harmonization are necessary. Summary preon is a fast and accurate library for the normalization of drug names and cancer types in large-scale data integration. Availability and Implementation preon is implemented in Python and freely available via the PyPI repository. Source code and gold standard data sets are available at https://github.com/ermshaua/preon/. Contact manuela.benary@bih-charite.de Supplementary information Supplementary data are available online.", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Medicine", "Biology", "Computer Science"]}
{"paperId": "34a1239b736d03d9661089caf4c332335822302a", "title": "PYInfer: Deep Learning Semantic Type Inference for Python Variables", "abstract": "Python type inference is challenging in practice. Due to its dynamic properties and extensive dependencies on third-party libraries without type annotations, the performance of traditional static analysis techniques is limited. Although semantics in source code can help manifest intended usage for variables (thus help infer types), they are usually ignored by existing tools. In this paper, we propose PYInfer, an end-to-end learning-based type inference tool that automatically generates type annotations for Python variables. The key insight is that contextual code semantics is critical in inferring the type for a variable. For each use of a variable, we collect a few tokens within its contextual scope, and design a neural network to predict its type. One challenge is that it is difficult to collect a high-quality human-labeled training dataset for this purpose. To address this issue, we apply an existing static analyzer to generate the ground truth for variables in source code. Our main contribution is a novel approach to statically infer variable types effectively and efficiently. Formulating the type inference as a classification problem, we can handle user-defined types and predict type probabilities for each variable. Our model achieves 91.2% accuracy on classifying 11 basic types in Python and 81.2% accuracy on classifying 500 most common types. Our results substantially outperform the state-of-the-art type annotators. Moreover, PYInfer achieves 5.2X more code coverage and is 187X faster than a state-of-the-art learning-based tool. With similar time consumption, our model annotates 5X more variables than a state-of-the-art static analysis tool. Our model also outperforms a learning-based function-level annotator on annotating types for variables and function arguments. All our tools and datasets are publicly available to facilitate future research in this direction.", "year": 2021, "citationCount": 14, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "3698f2b8e555e8027f09a1140a0fe5942cc5c94b", "title": "A multi-organ transcriptome resource for the Burmese Python (Python molurus bivittatus)", "abstract": null, "year": 2011, "citationCount": 21, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "36d36cbfd2342fd35c57da91be4a2f7c39f6b7ce", "title": "Static Type Recommendation for Python", "abstract": "Recently, Python has adopted optional type annotation to support type checking and program documentation. However, to enjoy the benefits, developers have to manually write type annotations, which is recognized to be a time-consuming task. To alleviate human efforts on manual type annotation, machine-learning-based approaches have been proposed to recommend types based on code features. However, they suffer from the correctness problem, i.e., the recommended types cannot pass type checking. To address the correctness problem of the machine-learning-based approaches, in this paper, we present a static type recommendation approach, named Stray. Stray can recommend types correctly. We evaluate Stray by comparing it against four state-of-art type recommendation approaches, and find that Stray outperforms these baselines by over 30% absolute improvement in both precision and recall.", "year": 2022, "citationCount": 4, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "3a601de3f056e7d82006fe77943ab259b181ce75", "title": "Platform for Analysis and Labeling of Medical Time Series", "abstract": "Reliable and diverse labeled reference data are essential for the development of high-quality processing algorithms for medical signals, such as electrocardiogram (ECG) and photoplethysmogram (PPG). Here, we present the Platform for Analysis and Labeling of Medical time Series (PALMS) designed in Python. Its graphical user interface (GUI) facilitates three main types of manual annotations\u2014(1) fiducials, e.g., R-peaks of ECG; (2) events with an adjustable duration, e.g., arrhythmic episodes; and (3) signal quality, e.g., data parts corrupted by motion artifacts. All annotations can be attributed to the same signal simultaneously in an ergonomic and user-friendly manner. Configuration for different data and annotation types is straightforward and flexible in order to use a wide range of data sources and to address many different use cases. Above all, configuration of PALMS allows plugging-in existing algorithms to display outcomes of automated processing, such as automatic R-peak detection, and to manually correct them where needed. This enables fast annotation and can be used to further improve algorithms. The GUI is currently complemented by ECG and PPG algorithms that detect characteristic points with high accuracy. The ECG algorithm reached 99% on the MIT/BIH arrhythmia database. The PPG algorithm was validated on two public databases with an F1-score above 98%. The GUI and optional algorithms result in an advanced software tool that allows the creation of diverse reference sets for existing datasets.", "year": 2020, "citationCount": 10, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "3ab2ba66e642217f59a0f77984116d0022e69e40", "title": "RNAvigate: Efficient exploration of RNA chemical probing datasets", "abstract": "Chemical probing technologies enable high-throughput examination of diverse structural features of RNA including local nucleotide flexibility, RNA secondary structure, protein- and ligand-binding, through-space interaction networks, and multi-state structural ensembles. Performing these experiments, by themselves, does not directly lead to biological insight. Instead, deep understanding of RNA structure-function relationships typically requires evaluating a system under structure- and function-altering conditions, linking these data with additional information, and visualizing multi-layered relationships. Current platforms lack the broad accessibility, flexibility, and efficiency needed to iterate on integrative analyses of these diverse, complex data. Here, we share the RNA visualization and graphical analysis toolset RNAvigate, a straightforward and flexible Python library. RNAvigate currently automatically parses twenty-one standard file formats (primary sequence annotations, per- and inter-nucleotide data, and secondary and tertiary structures) and outputs eighteen plot types. These features enable efficient exploration of nuanced relationships between chemical probing data, RNA structure, and motif annotations across multiple experimental samples. Compatibility with Jupyter Notebooks enables non-burdensome, reproducible, transparent and organized sharing of multi-step analyses and data visualization strategies. RNAvigate simplifies examination of multi-layered RNA structure information and accelerates discovery and characterization of RNA-centric functions in biology.", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "3b97850da6b4bd4985a3e7e89edbdc9a8b33b0cf", "title": "Root System Markup Language: Toward a Unified Root Architecture Description Language1[OPEN]", "abstract": "Portability of root architecture data with the Root System Markup Language paves the way for central root phenotype repositories. The number of image analysis tools supporting the extraction of architectural features of root systems has increased in recent years. These tools offer a handy set of complementary facilities, yet it is widely accepted that none of these software tools is able to extract in an efficient way the growing array of static and dynamic features for different types of images and species. We describe the Root System Markup Language (RSML), which has been designed to overcome two major challenges: (1) to enable portability of root architecture data between different software tools in an easy and interoperable manner, allowing seamless collaborative work; and (2) to provide a standard format upon which to base central repositories that will soon arise following the expanding worldwide root phenotyping effort. RSML follows the XML standard to store two- or three-dimensional image metadata, plant and root properties and geometries, continuous functions along individual root paths, and a suite of annotations at the image, plant, or root scale at one or several time points. Plant ontologies are used to describe botanical entities that are relevant at the scale of root system architecture. An XML schema describes the features and constraints of RSML, and open-source packages have been developed in several languages (R, Excel, Java, Python, and C#) to enable researchers to integrate RSML files into popular research workflow.", "year": 2015, "citationCount": 100, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "3c623857dae37f08e16564f2abf57a60a478cc8e", "title": "Linking big biomedical datasets to modular analysis with Portable Encapsulated Projects", "abstract": "Organizing and annotating biological sample data is critical in data-intensive bioinformatics. Unfortunately, incompatibility is common between metadata format of a data source and that required by a processing tool. There is no broadly accepted standard to organize metadata across biological projects and bioinformatics tools, restricting the portability and reusability of both annotated datasets and analysis software. To address this, we present Portable Encapsulated Projects (PEP), a formal specification for biological sample metadata structure. The PEP specification accommodates typical features of data-intensive bioinformatics projects with many samples, whether from individual experiments, organisms, or single cells. In addition to standardization, the PEP specification provides descriptors and modifiers for different organizational layers of a project, which improve portability among computing environments and facilitate use of different processing tools. PEP includes a schema validator framework, allowing formal definition of required metadata attributes for any type of biomedical data analysis. We have implemented packages for reading PEPs in both Python and R to provide a language-agnostic interface for organizing project metadata. PEP therefore presents an important step toward unifying data annotation and processing tools in data-intensive biological research projects.", "year": 2020, "citationCount": 14, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "3e01427c091bcdc6ded313be87e35e512776c31e", "title": "\u00c6THEL: Automatically Extracted Typelogical Derivations for Dutch", "abstract": "We present \u00c6THEL, a semantic compositionality dataset for written Dutch. \u00c6THEL consists of two parts. First, it contains a lexicon of supertags for about 900 000 words in context. The supertags correspond to types of the simply typed linear lambda-calculus, enhanced with dependency decorations that capture grammatical roles supplementary to function-argument structures. On the basis of these types, \u00c6THEL further provides 72 192 validated derivations, presented in four formats: natural-deduction and sequent-style proofs, linear logic proofnets and the associated programs (lambda terms) for meaning composition. \u00c6THEL\u2019s types and derivations are obtained by means of an extraction algorithm applied to the syntactic analyses of LASSY Small, the gold standard corpus of written Dutch. We discuss the extraction algorithm and show how \u2018virtual elements\u2019 in the original LASSY annotation of unbounded dependencies and coordination phenomena give rise to higher-order types. We suggest some example usecases highlighting the benefits of a type-driven approach at the syntax semantics interface. The following resources are open-sourced with \u00c6THEL: the lexical mappings between words and types, a subset of the dataset consisting of 7 924 semantic parses, and the Python code that implements the extraction algorithm.", "year": 2019, "citationCount": 7, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "3e1560e86606490ba294b4d21eb32e955b2a8b24", "title": "scDIOR: single cell RNA-seq data IO software", "abstract": null, "year": 2022, "citationCount": 11, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "3e42f47ed61d4baba727c19ec0338b5a4f2761a2", "title": "An Empirical Study of Type-Related Defects in Python Projects", "abstract": "In recent years, <monospace>Python</monospace> has experienced an explosive growth in adoption, particularly among open source projects. While <monospace>Python</monospace>'s dynamically-typed nature provides developers with powerful programming abstractions, that same dynamic type system allows for type-related defects to accumulate in code bases. To aid in the early detection of type-related defects, type annotations were introduced into the <monospace>Python</monospace> ecosystem (i.e., PEP-484) and static type checkers like <monospace>mypy</monospace> have appeared on the market. While applying a type checker like <monospace>mypy</monospace> can in theory help to catch type-related defects before they impact users, little is known about the real impact of adopting a type checker to reveal defects in <monospace>Python</monospace> projects. In this paper, we study the extent to which <monospace>Python</monospace> projects benefit from such type checking features. For this purpose, we mine the issue tracking and version control repositories of 210 <monospace>Python</monospace> projects on GitHub. Inspired by the work of Gao <italic>et al.</italic> on type-related defects in JavaScript, we add type annotations to test whether <monospace>mypy</monospace> detects an error that would have helped developers to avoid real defects. We observe that 15 percent of the defects could have been prevented by <monospace>mypy</monospace>. Moreover, we find that there is no significant difference between the experience level of developers committing type-related defects and the experience of developers committing defects that are not type-related. In addition, a manual analysis of the anti-patterns that most commonly lead to type-checking faults reveals that the redefinition of <monospace>Python</monospace> references, dynamic attribute initialization and incorrectly handled Null objects are the most common causes of type-related faults. Since our study is conducted on fixed public defects that have gone through code reviews and multiple test cycles, these results represent a lower bound on the benefits of adopting a type checker. Therefore, we recommend incorporating a static type checker like <monospace>mypy</monospace> into the development workflow, as not only will it prevent type-related defects but also mitigate certain anti-patterns during development.", "year": 2022, "citationCount": 11, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "40d8d6243149844fb8987fe7faa5300c2e51b083", "title": "The DBCLS BioHackathon : standardization and interoperability for bioinformatics web services and workflows", "abstract": "Web services have become a key technology for bioinformatics, since life science databases are globally decentralized and the exponential increase in the amount of available data demands for efficient systems without the need to transfer entire databases for every step of an analysis. However, various incompatibilities among database resources and analysis services make it difficult to connect and integrate these into interoperable workflows. To resolve this situation, we invited domain specialists from web service providers, client software developers, Open Bio* projects, the BioMoby project and researchers of emerging areas where a standard exchange data format is not well established, for an intensive collaboration entitled the BioHackathon 2008. The meeting was hosted by the Database Center for Life Science (DBCLS) and Computational Biology Research Center (CBRC) and was held in Tokyo from February 11th to 15th, 2008. In this report we highlight the work accomplished and the common issues arisen from this event, including the standardization of data exchange formats and services in the emerging fields of glycoinformatics, biological interaction networks, text mining, and phyloinformatics. In addition, common shared object development based on BioSQL, as well as technical challenges in large data management, asynchronous services, and security are discussed. Consequently, we improved interoperability of web services in several fields, however, further cooperation among major database centers and continued collaborative efforts between service providers and software developers are still necessary for an effective advance in bioinformatics web service technologies. Introduction Web services are software systems designed to be manipulated remotely over a network, often through web-based application programming interfaces (APIs). Through web services, users can take advantage of the latest maintained data and computational resources of remote service providers via a thin client. Web services are increasingly being adopted in the field of bioinformatics as an effective means for data and software access, especially in light of the rapid accumulation of large amounts of information for the life sciences [1]. Most of the major bioinformatics centers, including the National Center for Biotechnology Information (NCBI) in the US [2], the European Bioinformatics Institute (EBI) in the UK [3], and the DNA Data Bank of Japan (DDBJ) [4] / Kyoto Encyclopedia of Genes and Genomes (KEGG) [5] / Protein Data Bank Japan (PDBj) [6] in Japan, provide web service interfaces to their databases and computational resources. Since the web service model is based on open standards, these services are designed and expected to be interoperable [7]. However, many of the services currently available use their own data type definitions and naming conventions, resulting in a lack of interoperability that makes it harder for end users and developers to utilize these services for the creation of biological analysis workflows [8]. Moreover, these services are often not easily usable from programs written in specific computer languages, despite the language-independent specification of web services themselves. Some of the main reasons for that are the use of functionality not supported in a particular web service software implementation, and the lack of compliance with the SOAP/WSDL specification in a programming language's web service libraries. To overcome this situation and to assure interoperability between web services for biology, standardization of exchangeable data types and adoption of compatible interfaces to each service are essential. As a pilot study, the BioMoby project has tried to solve these problems by defining ontologies for data types and methods used in its services, and by providing a centralized repository for service discovery. Additionally, Moby client software exists to allow interconnections of multiple web services [9, 10]. However, there are still many major service providers that are not yet covered by the BioMoby framework and the Open Bio* libraries such as BioPerl [11], BioPython [12], BioRuby [13], and BioJava [14] have independently implemented access modules for some of these services [15]. To address these issues, we organized the BioHackathon 2008 [16], an international workshop sponsored by two Japanese bioinformatics centers, the Database Center for Life Science (DBCLS) [17] and the Computational Biology Research Center (CBRC) [18], focusing on the standardization and interoperability of web services. The meeting consisted of two parts: the first day was dedicated to keynote presentations and \"open space\" style discussions to identify current problems and to decide on strategies for possible solutions in each subgroup. The remaining four days were allotted for an intensive software coding event. Standardization and interoperability of web services were discussed by experts invited from four different domains: 1) web service providers, 2) Open Bio* developers, 3) workflow client developers, and 4) BioMoby project developers. Providers of independent web services were encouraged to address standardization and service integration, and were also asked to implement (and hence increase the number of) SOAP-compliant services for analysis tools and databases. Open Bio* developers focused on the utilization of as many bioinformatics web services as possible in four major computer languages (Perl, Python, Ruby, and Java), and collaborated to create compatible data models for common biological objects such as sequences and phylogenetic trees within the Open Bio* libraries. Workflow client developers were challenged to create and execute bioinformatics workflows combining various web service resources, and BioMoby project developers explored the best solution to define standard objects and ontologies in bioinformatics web services. In the following sections, we review the outcomes of standardization and interoperability discussions as well as the future challenges and directions of web services for bioinformatics that were highlighted in this workshop. Web service technologies Bioinformatics web services can be categorized into two major functional groups: data access and analysis. Access to public database repositories is obviously fundamental to bioinformatics research, and various systems have been developed for this purpose, such as Entrez at NCBI, Sequence Retrieval System (SRS) and EB-eye at EBI [19], Distributed Annotation System (DAS) [20], All-round Retrieval for Sequence and Annotation (ARSA) and getentry at DDBJ [21], DBGET at KEGG [22], and XML-based Protein Structure Search Service (xPSSS) at PDBj [6]. These services provide programmable means for text-based keyword search and entry retrieval from their backend databases, which mostly consist of static entries written either in semi-structured text or XML. As each entry has a unique identifier it is generally assignable to a URI (Uniform Resource Identifiers). The other group of services provides a variety of methods that require a certain amount of computation by implementing various algorithms, and they sometimes have complex input or output data structures. A typical example is a BLAST search, which needs a nucleic or amino acid sequence, as well as numerous optional arguments in order to find homologous sequences from a specified database using a dynamic programming algorithm. Services in this group sometimes require a large amount of computation time, including those providing certain functionalities of the European Molecular Biology Open Software Suite (EMBOSS) [23], 3D structural analysis of proteins, and data mapping on biochemical pathways. Historically, the term web services was associated with SOAP (Simple Object Access Protocol), a protocol that transfers messages in a SOAP XML envelope between a server and a client, usually over the Hypertext Transfer Protocol, HTTP [24]. SOAP services have several accessibility advantages, including an open standard that is independent from computer programming languages, and the use of the HTTP protocol which is usually not filtered by firewalls (SOAP services can therefore be accessed even from institutions having very strict security policies for Internet access). Since all SOAP messages are XML documents and the format of the messages are known in advance from the service description (see below), it is possible to use XML binding to seamlessly convert the messages to language-specific objects and thus avoid any custom-programmed parsing. XML binding is often leveraged by SOAP libraries to provide a programmatic interface to a web service similar to an object oriented API. Operations provided by SOAP services can consume several arguments, thus a service that requires a number of parameters can easily be utilized as an API, as if the method were a function call for a local library of a given programming language. For the purpose of service description, SOAP services usually come with a Web Services Description Language (WSDL) [25] file. A WSDL file is an XML formatted document that is consumed by a SOAP/WSDL library to allow automatic construction of a set of functions for the client program. In addition to the list of methods, WSDL contains descriptions for each method, including the types and numbers of input arguments as well as those of output data. WSDL is also capable of describing complex data models that combine basic data types into nested data objects. In this way, SOAP services can accept various kinds of complex biological objects, such as a protein sequence entry accompanied by several annotation properties like the identifier, description, and source organism. Recently, another kind of web service model named REST (Representational State Transfer) has rapidly gained popularity as an effective alternative approach to SOAP-based web services [1]. REST is an approach whereby an online service ", "year": 2010, "citationCount": 30, "fieldsOfStudy": null}
{"paperId": "40f3de86086a4e938ad977f83f6c1a2eab9d0a2e", "title": "CIViCutils: Matching and downstream processing of clinical annotations from CIViC", "abstract": "Background: With the advent of next-generation sequencing, profiling the genetic landscape of tumors entered clinical diagnostics, bringing the resolution of precision oncology to unprecedented levels. However, the wealth of information generated in a sequencing experiment can be difficult to manage, especially if hundreds of mutations need to be interpreted in a clinical context. Dedicated methods and databases are required that assist in interpreting the importance of a mutation for disease progression, prognosis, and with respect to therapy. Here, the CIViC knowledgebase is a valuable curated resource, however, utilizing CIViC in an efficient way for querying a large number of mutations needs sophisticated downstream methods. Methods: To this end, we have developed CIViCutils, a Python package to query, annotate, prioritize, and summarize information from the CIViC database. Our package provides functionality for performing high-throughput searches in CIViC, automatically matching clinical evidence to input variants, evaluating the accuracy of the extracted variant matches, fully exploiting the available disease-specific information according to cancer types of interest, and in-silico predicting drug-target interactions tailored to individual patients. Results: CIViCutils allows the simultaneous query of hundreds of mutations and is able to harmonize input across different nomenclatures. Moreover, it supports gene expression data, single nucleotide mutations, as well as copy number alterations as input. We utilized CIViCutils in a study on the bladder cancer cohort from The Cancer Genome Atlas (TCGA-BLCA), where it helped to extract clinically relevant mutations for personalized therapy recommendation. Conclusions: CIViCutils is an easy-to-use Python package that can be integrated into workflows for profiling the genetic landscape of tumor samples. It streamlines interpreting large numbers of variants with retrieving and processing curated CIViC information.", "year": 2023, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "42db68957509e83e97f5628c6a99f7a54b324902", "title": "Sifter-T: Um framework escal\u00e1vel para anota\u00e7\u00e3o filogen\u00f4mica probabil\u00edstica funcional de dom\u00ednios prot\u00e9icos", "abstract": "It is known that many software are no longer used due to their complex usability. Even tools known for their task execution quality are abandoned in favour of faster tools, simpler to use or install. In the functional annotation field, Sifter (v2.0) is regarded as one of the best when it comes to annotation quality. Recently it has been considered one of the best tools for functional annotation according to the \"Critical Assessment of Protein Function Annotation\u201d (CAFA) experiment. Nevertheless, it is still not widely used, probably due to issues with usability and suitability of the framework to a high throughput scale. The original workflow SIFTER consists of two main steps: The annotation recovery for a list of genes and the reconciled gene tree generation for the same list. Next, based on the gene tree, Sifter builds a Bayesian network structure in which its leaves represent genes. The known functional annotations are associated to the aforementioned leaves, and then the annotations are probabilistically propagated along the Bayesian network to the leaves without a priori information. At the end of the process, a list of Gene Ontology functions and their occurrence probabilities is generated for each unknown function gene. This work main goal is to optimize the original source code for better performance, potentially allowing it to be used in a genome-wide scale. Studying the pre-processing workflow we found opportunities for improvement and envisioned strategies to address them. Among the implemented strategies we have: The use of parallel threads; CPU load balancing, revised algorithms for best utilization of disk access, memory usage and runtime; source code adaptation to currently used biological databases; improved user accessibility; input types increase; automatic gene and species tree reconciliation process; sequence filtering to reduce analysis dimension, and other minor implementations. With these implementations we achieved great performance speed-ups. For example, we obtained 87-fold performance increase in the annotation recovering module and 72.3% speed increase in the gene tree generation module using quad-core machines. Additionally, significant memory usage decrease during the realignment phase was obtained. This implementation is presented as Sifter-T (Sifter Throughput-optimized), an open source tool with better usability, performance and annotation quality when compared to the Sifter's original workflow implementation. Sifter-T was written in a modular fashion using Python programming language; it is designed to simplify complete genomes and proteomes annotation tasks and the outputs are presented in order to make the researcher's work easier.", "year": 2013, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "43039ecb64821b6c4d0bb1c80b04cdcd882eb4ab", "title": "Posters with Abstracts J : Methods and technologies for computational biology", "abstract": "Next generation sequencing technologies, specifically RNA-seq, promise to give a comprehensive portrait of the whole transcriptome in a given organism, even in the absence of a reference genome (Grabherr et al. 2011), thus optimized tools for the annotation are required. Furthermore, unambiguously identification of long non-coding RNAs (lncRNAs) is challenging since they are being discovered in almost all organisms (e.g. mammals (Cabili et al. 2011), fishes (Pauli et al. 2012), insects (Young et al. 2012), etc.). Current software to annotate transcriptomes, search for homologies with known proteins and domains, but they do not have the ability to identify putative lncRNAs (Conesa and G\u00f6tz 2008; Schmid and Blaxter 2008, 8; Philipp et al. 2012; Koski et al. 2005). We developed Annocript: an automated pipeline to annotate large transcripome datasets and predict putative lncRNAs. The pipeline is developed using PERL, R and MySQL and organized in modules. The first module generates a MySQL database to collect, map, organize and store information from several biological databases such as Uniprot (UniProt Consortium 2013), Conserved Domains Database (Marchler-Bauer et al. 2013), Enzyme Commission (Bairoch 2000), Pathways (Morgat et al. 2012) and GO terms (Ashburner et al. 2000). This task is performed only once. The second module uses BLAST+ (Camacho et al. 2009) to align query sequences against Uniprot proteins and CDD domains. The pipeline allows also to search for ribosomal and other short non-coding RNAs against a database of custom subset of the NCBI nucleotide and RFAM divisions (Burge et al. 2012). Finally, it executes Portrait (Arrial,Togawa and Brigido 2009), to calculate the non-coding potential, and Virtual Ribosome (Wernersson 2006), to extract the longest Open Reading Frame (ORF). The third module generates an user-friendly table containing the annotations: proteins, domains, enzyme, pathways, GO terms and longest ORFs. A final heuristic evaluates all the results to give a binary classification for the query transcript to be coding or lncRNA. A final module generates outputs in GFF3 format (for parsing and visualization) and HTML with statistics and charts. Fasta files are generated for coding, long non-coding transcripts and for the longest ORF translations. Annocript is: flexible, because modules can be run independently and plugins can be added; fully customizable, in fact many programs parameters can be easily modified; optimized for fast computation, since it uses specific BLASTx parameters that speedup the search without any loss in sensitivity (Korf 2003) and our own parallelization of rpsBLAST; offlinesoftware, because internet connection is needed only for generation of the initial annotation database; automatic, because it does not rely on any manual operation. Annocript is distributed under the GNU General Public License and is freely available at https://github.com/frankMusacchia/Annocript. J02: Jerome Mariette, Frederic Escudie, Philippe Bardou and Christophe Klopp. Jflow: A fully scalable Javascript workflow management system Abstract: Building rich WEB environments aimed at helping scientists analyse their data is a Building rich WEB environments aimed at helping scientists analyse their data is a common trend in bioinformatics. These applications are often specialized WEB portals or generic Workflow management systems (WMS). The first class provides multiple services and analysis tools in an integrated interface for a specific experiment or data type. Quite often these systems hide the processing steps in the back-office. The second class, for example Galaxy (Giardine et al., 2005), is mainly focused on workflow creation and provides a rather poor end-user interface but enable to combine tools and data sources as desired. We introduce jflow a fully scalable WMS that can easily be embedded in any WEB site, providing all WMS features and benefits to your project. Jflow core is based upon the Makeflow (Albrecht et al., 2012) workflow engine and weaver (Bui, 2012), its Python API which is embedded within jflow. Using makeflow permits to run jflow under different batch systems such as Condor, SGE, Work Queue or a single multicore machine. Adding a new component in the system requires to write a Python class inheriting from the Component class and to overwrite the process method wrapping the new tool. The class provides access to multiple concepts such as map/reduce or multimap in order to define the way the command line pattern should be applied to the input data. Multiple formats are available as input and output in order to force the workflow creator to combine components the right way. In the same way, writing a workflows consist of inheriting from the Workflow class and overwriting the process method. In this last method, the workflow is defined as the succession of components. Moreover, a property file is to be created to define the workflow parameters. It gathers all the information including the data type which will be checked from the provided interfaces. As an example, a date type will be displayed as a calendar in the graphical interface and the dd/mm/yyyy format compliance will be checked in command line mode. All information about a workflows will be accessible from both jflow command line interface and its REST API. Thus, users can list available workflows and their states, run and monitor them. Accessing those functionalities from the command line interface can easily be done using the jflow command line. The same thing can also be done from a website integrating the jflow plug-in. To do so, jflow offers four modules to retrieve workflows information. As a jquery plugin, these modules are fully scalable and provide multiple methods and events to ease jflow integration. As example, the \u2015click\u2016 event on a specific workflow triggers an event that can be listen to and used to build a workflow form wherever the web site designer wants it to be showed. jflow http://bioinfo.genotoul.fr/jflow is a simple and efficient solution to embed workflow management systems features within any Web application. J03: Jean Philippe Tamby, Rim Zaag, Jean-Paul Bouchet, Cecile Guichard, Philippe Grevet, Marie-Laure Martin-Magniette, S\u00e9bastien Aubourg and V\u00e9ronique Brunaud. Evolution of the FLAGdb++ integrated environment for exploring plant genomes Abstract: Comprehensive researches in genomics, post-genomics or systems biology involve the exploitation of large data sets. Therefore, databases provide key solutions to store and Comprehensive researches in genomics, post-genomics or systems biology involve the exploitation of large data sets. Therefore, databases provide key solutions to store and organize huge bulks of data, and ease the querying of information. The FLAGdb++ information system, i.e. a core database coupled to a distributed user-friendly Java interface, was conceived to gather complete sequences of selected plant genomes and heterogeneous data coming from experimental and/or computer prediction approaches. Using the genome as physical reference, any structural or functional annotation is merged and anchored to it independently. Hence, FLAGdb++ allows considering genes in large contexts like the chromosome, gene family, orthology group or co-expression cluster and functional", "year": 2014, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "44286fe08a2d14560f8035d0a32cc4eb762d624b", "title": "Application of high performance compute technology in bioinformatics", "abstract": "Bioinformatics and computational biology are driven by growing volumes of data in biological systems that also tend to increase in complexity. The research presented in this thesis focuses on the need to analyze such data volumes in such complexity. The results show that the application of high-performance compute technologies, preferably combined with low-cost hardware, is a successful approach to generate new bioinformatics approaches that allow addressing new types of data analyses and research questions in biology. An overview of the technologies and recent developments in biology and computer science relevant for this thesis (Chapter 1) identifies current high-throughput sequencing platforms as a key technology. Sequencing platforms now deliver data sets up to terabytes in size for elucidating genome structure, gene content, gene activity, as well as gene variants. The concepts and technologies from computer science to handle these large amounts of data include (a) grid technologies for compute parallelization while making more efficient use of existing low-cost infrastructure; (b) graphics cards for increased compute power and (c) graph databases for large data volume storage and advanced methods for analyses. This thesis presents novel applications and added value of these three concepts for bioinformatics research. Small RNAs are important regulators of genome function, yet their prediction in genomes is still a major computational challenge (Chapter 2). They tend to have a minimal free energy (MFE) significantly lower than the MFE of non-small RNA sequences with the same nucleotide composition. Evaluation of many MFEs is, however, too compute-intensive for genome-wide screening. With a local grid infrastructure of desktop computers, MFE distributions of a very large collection of sequence compositions were pre-calculated and used to determine the MFE distribution for any given sequence composition by interpolation. This approach allows on-the-fly calculation for any candidate sequence composition and makes genome-wide screening with this characteristic of a pre-miRNA sequence feasible. This way, MFE evaluation can be added as a new parameter for genome-wide selection of potential small RNA candidates (Chapter 2). The concept of large-scale pre-calculation of compute-intensive parameters is one of the options for future bioinformatics analyses. Sequence alignment is essential in the analysis of next-generation sequencing data. The gold standard for sequence alignment is the Smith-Waterman (SW) algorithm. Existing implementations of the full SW algorithm are either not fast enough, or limited to dedicated tasks, usually to optimize for speed, whereas popular heuristic SW versions (such as BLAST) suffer from statistical issues. Graphics hardware is well-suited to speed up SW alignments, but SW on graphics cards does not report the alignment details desired by biologists for further analysis. This thesis presents the CUDA-based Parallel SW Alignment Software (PaSWAS) (Chapter 3). PaSWAS gives (a) easy access to the computational power of NVIDIA-based graphics cards for high-speed sequence alignments, (b) information such as score, number of gaps and mismatches with the accuracy of the full SW algorithm and (c) a report of multiple hits per alignment. Two use cases show the usability and versatility of the new parallel Smith-Waterman implementation for bioinformatics analyses. It demonstrates the added value of the use of low-cost graphics cards in bioinformatics software. To further promote the use of PaSWAS, a new implementation, pyPaSWAS, provides the SW sequence alignment code fully packed in Python and the more widely accepted OpenCL language (Chapter 4). Moreover, pyPaSWAS now supports an affine gap penalty. This way, pyPaSWAS presents an easy Python-based environment for accurate and retrievable parallel SW sequence alignments on GPUs and multi-core systems. The strategy of integrating Python with high-performance parallel compute languages to create a developer- and user-friendly environment is worth to be considered for other computationally-intensive bioinformatics algorithms. Thanks to the accuracy and retrieval characteristics of (py)PaSWAS, it was noted that long sequencing reads on the PacBio platform can contain many artificial palindromic sequences. These palindromes are due to errors introduced by whole-genome amplification (WGA). Next-generation sequencing requires sufficient amounts of DNA. If not available, WGA is routinely used to generate the amounts of DNA required. The introduction of artificial palindromic sequences hampers assembly and severely limits the value of long sequencing reads. Pacasus is a novel software tool to identify and resolve such artificial palindromic sequences in long sequencing reads (Chapter 5). Two use cases show that Pacasus markedly improves read mapping and assembly of WGA DNA. In comparison, the quality of mapping and assembly is similar to the quality obtained with non-amplified DNA. Therefore, with Pacasus, long-read technology becomes feasible for the sequencing of samples for which only very small amounts of DNA are available, such as single cells or single chromosomes. Numerous tools and databases exist to annotate and investigate the functions encoded in properly assembled genomes, such as InterProScan, KEGG, GO and many more. Comparisons of functionalities across multiple genomes is, however, not trivial. The concept of graph databases is a promising novel approach from computer science for such multi-genome comparisons. For a data set of all (> 150,000) genes of 17 fungal species functionally annotated with InterProScan, the associated KEGG, GO and annotation data are imported and interconnected in a new Neo4j graph database (Chapter 6). Relationships in this database are visualized and mined with a newly refurbished and extended Neo4j plugin for Cytoscape. Inspection of (sub)graphs of functional annotations is an attractive way to compare and group functional annotation across species. In the use case of the seventeen fungal genomes, it helped to outline, compare and explain details of the life style of groups of individual species. The general discussion of this thesis provides an outlook on the future of bioinformatics in the context of the results here presented (Chapter 7). A grid infrastructure is recommended as a feasible and attractive cost-effective strategy to create compute power, as is the further inclusion of graphics cards. Full implementation of graph technology is considered necessary for advancing bioinformatics. The work presented in this thesis also shows that use of grids, graphics cards and graph technology imply the redesign of existing software applications. To be able to create novel stable, predictable and user-friendly applications in bioinformatics, formal training in software engineering principles is highly recommended. Courses and other programs are necessary for the life-long learning that will be crucial for the future of bioinformatics. The main challenges for bioinformatics in the years to come are all data centered: issues with growing data volumes, with more data types and with higher data complexity. To deal with these challenges, further integration of now separate fields of science is warranted in ways we cannot even image yet.", "year": 2019, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "4497f6f93bf9e0ec901a8ce04c09fa8162e17c6b", "title": "An Integrated Framework for Analysis and Prediction of Impact of Single Nucleotide Polymorphism Associated with Human Diseases", "abstract": "Single nucleotide polymorphisms are most common type of genetic variation in human genome. Analyzing genetic variants can help us better understand the genetic basis of diseases and develop predictive models which are useful to identify individuals who are at increased risk for certain diseases. Several SNP analysis tools have already been developed. For running these tools, the user needs to collect data from various databases. Secondly, often researchers have to use multiple variant analysis tools for cross validating their results and increase confidence in their findings. Extracting data from multiple databases and running multiple tools at a time, increases complexity and time required for analysis. There are some web-based tools that integrate multiple genetic variant databases and provide variant annotations for a few tools. These approaches have some limitations such as retrieving annotation information, filtering common pathogenic variants. The proposed web-based tool, namely IPSNP: An Integrated Platform for Predicting Impact of SNPs is written in Django which is a python-based framework. It uses RESTful API of MyVariant.info to extract annotation information of variants associated with a given gene, rsID, HGVS format variants specified in a VCF file for 29 tools. The results are in the form of a CSV file of predictions (1) derived from the consensus decision, (2) a file having annotations for the variants associated with the given gene, (3) a file showing variants declared as pathogenic commonly by the selected tools, and (4) a CSV file containing chromosome coordinates based on GRCh37 and GRCh38 genome assemblies, rsIDs and proteomic data, so that users may use tools of their choice and avoiding manual parameter collection for each tool. IPSNP is a valuable resource for researchers and clinicians and it can help to save time and effort in discovering the novel disease-associated variants and the development of personalized treatments.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "44cc31e59a87b438c64241a25e7a4e7bcd4fba09", "title": "Abstract 1932: Pollock: Fishing for cell states", "abstract": "\n The use of single-cell methods is expanding at an ever-increasing rate. While multiple algorithms address the task of cell classification, they are limited in terms of cross platform compatibility, reliance on the availability of a reference dataset, and classification interpretability. Here, we introduce Pollock, a suite of algorithms for cell type identification that is compatible with popular single cell methods and analysis platforms, provides a series of pretrained human cancer reference models, and reports interpretability scores that identify the genes that drive cell type classifications. Our model combines two important approaches, one each from machine learning and deep learning: a variational autoencoder (VAE) and random forest classifier, to make cell type predictions. Pollock is highly versatile, being available as a command line tool, Python library (with scanpy integration), or R library (with Seurat integration), and can be installed as a conda package, or in containerized form via Docker. To allow for easier pan-disease and pan-tissue analyses, Pollock also ships with a library of pretrained cancer type specific and agnostic modules that were trained on expertly-curated single cell data that are ready to \u201cplug and play\u201d with no additional annotation or training required. Conversely, Pollock also allows for the training of custom classification modules, if an annotated reference single cell dataset is available. These pretrained models were fitted on manually curated and annotated single cell data from eight different cancer types spanning three single cell technologies (scRNA-seq, snRNA-seq, and snATAC-seq). Pollock also provides feature importance scores that allow for cell type classifications to be traced back to the genes influencing a particular cell type classification, further promoting biological interpretability. These scores could allow for new, technology-specific biomarker discovery. We also demonstrate the utility of Pollock by applying it in a pan-cancer single cell immune analysis.\n Citation Format: Erik Storrs, Daniel Cui Zhou, Michael C. Wendl, Matthew A. Wyczalkowski, Alla Karpova, Liang-Bo Wang, Yize Li, Austin Southard-Smith, Reyka G. Jayasinghe, Lijun Yao, Ruiyang Liu, Yige Wu, Nadezhda V. Terekhanova, Houxiang Zhu, John M. Herndon, Feng Chen, William E. Gillanders, Ryan C. Fields, Li Ding. Pollock: Fishing for cell states [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2022; 2022 Apr 8-13. Philadelphia (PA): AACR; Cancer Res 2022;82(12_Suppl):Abstract nr 1932.", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "4574654e5c54f174a651ff5565eea64868c73084", "title": "A Guided Tour of a Static Analyzer for Data Science Software", "abstract": ". We present Typpete , a sound type inferencer that automatically infers Python 3 type annotations. Typpete encodes type constraints as a MaxSMT problem and uses optional constraints and spe-ci\ufb01c quanti\ufb01er instantiation patterns to make the constraint solving process e\ufb03cient. Our experimental evaluation shows that Typpete scales to real world Python programs and outperforms state-of-the-art tools.", "year": 2020, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "4613229aff389bf0330cd63efab3733ec12b73e5", "title": "Pgltools: a genomic arithmetic tool suite for manipulation of Hi-C peak and other chromatin interaction data", "abstract": null, "year": 2017, "citationCount": 35, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "463dc5bb6412504eab29192ec77477147e9d31f7", "title": "\u0412\u044b\u0432\u043e\u0434 \u0442\u0438\u043f\u043e\u0432 \u0434\u043b\u044f \u044f\u0437\u044b\u043a\u0430 Python", "abstract": "The article presents type inference for programs written in Python programming language. At first, type inference algorithms for parametric polymorphism that were described in the scientific literature are reviewed. The algorithms evolved from \u201cbasic\u201d algorithm (also known as Palsberg \u2014 Schwartzbach algorithm) to Cartesian product algorithm (CPA). It was shown that CPA is both precise and efficient, however it has to be modified to be used on Python code. Afterwards, we present a new algorithm (a modification of CPA) for Python. It is shown how type inference module using the new algorithm analyses various Python language structures: constants (literals), basic Python collections, variable names, assignments, function and class definitions, attribute and index expressions. It is also shown how the algorithm deals with external (non-Python) functions using special annotations that specify output types depending on input types of the function. Afterwards, the results of work on the prototype (module that implements described type inference algorithm) are presented. The paper is concluded by an overview of possible future work directions such as generating a defect trace, i.e. description that specifies how expression got its incorrect types.", "year": 2013, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "4749aed8228b0bfc7c2f81adfbc3b3e238b2b914", "title": "NGScloud2: optimized bioinformatic analysis using Amazon Web Services", "abstract": "NGScloud was a bioinformatic system developed to perform de novo RNAseq analysis of non-model species by exploiting the cloud computing capabilities of Amazon Web Services. The rapid changes undergone in the way this cloud computing service operates, along with the continuous release of novel bioinformatic applications to analyze next generation sequencing data, have made the software obsolete. NGScloud2 is an enhanced and expanded version of NGScloud that permits the access to ad hoc cloud computing infrastructure, scaled according to the complexity of each experiment. NGScloud2 presents major technical improvements, such as the possibility of running spot instances and the most updated AWS instances types, that can lead to significant cost savings. As compared to its initial implementation, this improved version updates and includes common applications for de novo RNAseq analysis, and incorporates tools to operate workflows of bioinformatic analysis of reference-based RNAseq, RADseq and functional annotation. NGScloud2 optimizes the access to Amazon\u2019s large computing infrastructures to easily run popular bioinformatic software applications, otherwise inaccessible to non-specialized users lacking suitable hardware infrastructures. The correct performance of the pipelines for de novo RNAseq, reference-based RNAseq, RADseq and functional annotation was tested with real experimental data. NGScloud2 code, instructions for software installation and use are available at https://github.com/GGFHF/NGScloud2. NGScloud2 includes a companion package, NGShelper that contains python utilities to post-process the output of the pipelines for downstream analysis at https://github.com/GGFHF/NGShelper.", "year": 2020, "citationCount": 4, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"]}
{"paperId": "47adf2557630323c762dffbd375169662a374e37", "title": "SLACC: Simion-based Language Agnostic Code Clones", "abstract": "Successful cross-language clone detection could enable researchers and developers to create robust language migration tools, facilitate learning additional programming languages once one is mastered, and promote reuse of code snippets over a broader codebase. However, identifying cross-language clones presents special challenges to the clone detection problem. A lack of common underlying representation between arbitrary languages means detecting clones requires one of the following solutions: 1) a static analysis framework replicated across each targeted language with annotations matching language features across all languages, or 2) a dynamic analysis framework that detects clones based on runtime behavior. In this work, we demonstrate the feasibility of the latter solution, a dynamic analysis approach called SLACC for cross-language clone detection. Like prior clone detection techniques, we use input/output behavior to match clones, though we overcome limitations of prior work by amplifying the number of inputs and covering more data types; and as a result, achieve better clusters than prior attempts. Since clusters are generated based on input/output behavior, SLACC supports cross-language clone detection. As an added challenge, we target a static typed language, Java, and a dynamic typed language, Python. Compared to HitoshiIO, a recent clone detection tool for Java, SLACC retrieves 6 times as many clusters and has higher precision (86.7% vs. 30.7%). This is the first work to perform clone detection for dynamic typed languages (precision = 87.3%) and the first to perform clone detection across languages that lack a common underlying representation (precision = 94.1%). It provides a first step towards the larger goal of scalable language migration tools.", "year": 2020, "citationCount": 23, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "47bb33db2ede899c232aace0d7a6b5d7bf843a7e", "title": "Orchid: a novel management, annotation and machine learning framework for analyzing cancer mutations", "abstract": "Motivation: As whole\u2010genome tumor sequence and biological annotation datasets grow in size, number and content, there is an increasing basic science and clinical need for efficient and accurate data management and analysis software. With the emergence of increasingly sophisticated data stores, execution environments and machine learning algorithms, there is also a need for the integration of functionality across frameworks. Results: We present orchid, a python based software package for the management, annotation and machine learning of cancer mutations. Building on technologies of parallel workflow execution, in\u2010memory database storage and machine learning analytics, orchid efficiently handles millions of mutations and hundreds of features in an easy\u2010to\u2010use manner. We describe the implementation of orchid and demonstrate its ability to distinguish tissue of origin in 12 tumor types based on 339 features using a random forest classifier. Availability and implementation: Orchid and our annotated tumor mutation database are freely available at https://github.com/wittelab/orchid. Software is implemented in python 2.7, and makes use of MySQL or MemSQL databases. Groovy 2.4.5 is optionally required for parallel workflow execution. Contact: JWitte@ucsf.edu Supplementary information: Supplementary data are available at Bioinformatics online.", "year": 2018, "citationCount": 15, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "47ee815259beefcf87ad73bc6bab107a50c1a8a4", "title": "Automated Return Type Annotation for Python", "abstract": null, "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "481eaa183680deddb020b9fc7543072d6fb4e124", "title": "Variable Domain-specific Software Languages with DjDSL: Design and Implementation", "abstract": null, "year": 2020, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "48395a87e86b824fcf285030424599d9fba123ed", "title": "Development of a Protein Conservation Analysis Pipeline and Application to Carbonic Anhydrase IV", "abstract": "Background and Aims Conservation is a hallmark of inherent valuable function. Computational analysis of conservation of each amino acid in a protein provides targets for future computational or experimental research. Carbonic anhydrases (CA) reversibly catalyze the carbon dioxide to bicarbonate reaction. Despite the apparent simplicity of the reaction, the \u03b1-CA protein family exists as more than 15 different isoforms in mammals and its members are present in a wide array of tissues and perform a variety of functions. The overall goal of this research is identification of all residues of functional significance in CA-IV through utilization of gene prediction, comparative genomics, and conservation analysis. The expanded goal is to make this process applicable to any protein group. Methods Automated methods were created, using Python scripting, to extract orthologs for human carbonic anhydrases. Predictions were made for incomplete orthologs, and conservation analysis was performed on a codon alignment of the final set. Additional python scripts created three dimensional (3D) models of conservation values, within specific taxa or as comparisons between them. Results A pipeline was created for automated gene annotation, 3D image generation, and comparative analysis between different taxa. A total of 499 residue positions were altered in 55 carbonic anhydrase proteins from 6 isozyme types. Key amino acids have been identified in a region potentially related to CA-IV ion channel binding. Conclusions Ka/Ks conservation analysis can be applied in a quick and automated manner to produce models and images from large numbers of orthologs, allowing for determination of critical amino acid residues in proteins.", "year": 2013, "citationCount": 2, "fieldsOfStudy": ["Chemistry"]}
{"paperId": "48d14a3c23851730a1b0e9470713d75cefab0ffe", "title": "Comprehensive annotations of the mutational spectra of SARS\u2010CoV\u20102 spike protein: a fast and accurate pipeline", "abstract": "In order to explore nonsynonymous mutations and deletions in the spike (S) protein of SARS-CoV-2, we comprehensively analyzed 35,750 complete S protein gene sequences from across six continents and five climate zones around the world, as documented in the GISAID database as of June 24th, 2020. Through a custom Python-based pipeline for analyzing mutations, we identified 27,801 (77.77 % of spike sequences) mutated strains compared to Wuhan-Hu-1 strain. 84.40% of these strains had only single amino-acid (aa) substitution mutations, but an outlier strain from Bosnia and Herzegovina (EPI_ISL_463893) was found to possess six aa substitutions. The D614G variant of the major G clade was found to be predominant across circulating strains in all climates. We also identified 988 unique aa substitution mutations distributed across 660 positions within the spike protein, with eleven sites showing high variability \u2013 these sites had four types of aa variations at each position. Besides, 17 in-frame deletions at four major regions (three in N-terminal domain and one just downstream of the RBD) may have possible impact on attenuation. Moreover, the mutational frequency differed significantly (p= 0.003, Kruskal\u2013Wallis test) among the SARS-CoV-2 strains worldwide. This study presents a fast and accurate pipeline for identifying nonsynonymous mutations and deletions from large dataset for any particular protein coding sequence and presents this S protein data as representative analysis. By using separate multi-sequence alignment with MAFFT, removing ambiguous sequences and in-frame stop codons, and utilizing pairwise alignment, this method can derive nonsynonymus mutations (Reference:Position:Strain). We believe this will aid in the surveillance of any proteins encoded by SARS-CoV-2, and will prove to be crucial in tracking the ever-increasing variation of many other divergent RNA viruses in the future.", "year": 2020, "citationCount": 35, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "4a6f68abc7dcc81691887e8f12c22c6474698423", "title": "Abstract LB548: Genomic analysis of antineoplastic drug-related genetic variations based on large-scale population sequencing", "abstract": "\n Purpose: Cancer has become the second leading cause of death in the world. In 2020, there were 19.29 million new cancer patients and a total of 10 million deaths. Drug are the most popular ways to control the development of cancer, but the effectiveness of anti-tumor drugs is only about 25%. Drug efficacy is affected by many factors, among which the most critical is genetic variants. Genomic analysis based on large-scale population sequencing was performed to calculate the mutation frequency of antineoplastic drug-related genetic variations and systematically analyze general descriptions and population differences.\n Methods: Information of antineoplastic drugs is available from FDA and CFDA, and gene list comes from DrugBank, PharmGKB, FDA biomarker list and clinical guidelines. The maximum likelihood method was used to estimate the population mutation frequency of a single locus, and the calculation process was completed on Ali Cloud platform. GATK, ANNOVAR, PROVEAN, SIFT and other software were used for quality control, annotation and function prediction of mutations. Perl, Python, R and other computer languages were used for statistical analysis.\n Results: we collect 206,604 samples from mainland China for analysis, and identified 104 antineoplastic drugs and 517 drug-related genes at the same time. There were 201,774 genetic variations, most of which were intron mutations, exon mutations account for about 1% of all mutations. We found 41,955 novel mutations, which unrecorded in dbSNP, accounting for about 20.8%. Most of these mutations were rare mutations with frequencies less than 1%, subsequent prediction of mutation function also indicated that a large proportion of these mutations were deleterious. These results suggest that rare mutations may also play an important role in the use of antineoplastic drugs, which has previously been underestimated. The mutation frequency distribution of different types of genes suggests that transporters have a greater impact on personalized medicine during the use of antineoplastic drugs in China. By analyzing the genetic variation of important metabolic enzymes, we found that mutations such as UGT1A1*6, DPYD*9A, CYP2D6*2 and *10 have a high frequency and high detection probability in the Chinese population. Therefore, when patients use drugs metabolized by related metabolic enzymes, genetic testing should be done in advance to prevent the occurrence of adverse reactions. Regional analysis revealed that the overall differences among provinces in China were small, but the differences were large compared with other ethnic groups.\n Conclusion: Results of large-scale population sequencing reveal distribution and discipline of genetic variation of antineoplastic drug-related genes in Chinese population, further confirm the importance of precision medicine in cancer, and illustrate that the use of antineoplastic drugs can also be affected by ethnic differences.\n Citation Format: Yan Zhan, Ji-Ye Yin. Genomic analysis of antineoplastic drug-related genetic variations based on large-scale population sequencing [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2022; 2022 Apr 8-13. Philadelphia (PA): AACR; Cancer Res 2022;82(12_Suppl):Abstract nr LB548.", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "4dd54f3a0276774e3a7b792cdb59a1fc61937190", "title": "Efficient searching and annotation of metabolic networks using chemical similarity", "abstract": "MOTIVATION\nThe urgent need for efficient and sustainable biological production of fuels and high-value chemicals has elicited a wave of in silico techniques for identifying promising novel pathways to these compounds in large putative metabolic networks. To date, these approaches have primarily used general graph search algorithms, which are prohibitively slow as putative metabolic networks may exceed 1 million compounds. To alleviate this limitation, we report two methods--SimIndex (SI) and SimZyme--which use chemical similarity of 2D chemical fingerprints to efficiently navigate large metabolic networks and propose enzymatic connections between the constituent nodes. We also report a Byers-Waterman type pathway search algorithm for further paring down pertinent networks.\n\n\nRESULTS\nBenchmarking tests run with SI show it can reduce the number of nodes visited in searching a putative network by 100-fold with a computational time improvement of up to 10(5)-fold. Subsequent Byers-Waterman search application further reduces the number of nodes searched by up to 100-fold, while SimZyme demonstrates \u223c 90% accuracy in matching query substrates with enzymes. Using these modules, we have designed and annotated an alternative to the methylerythritol phosphate pathway to produce isopentenyl pyrophosphate with more favorable thermodynamics than the native pathway. These algorithms will have a significant impact on our ability to use large metabolic networks that lack annotation of promiscuous reactions.\n\n\nAVAILABILITY AND IMPLEMENTATION\nPython files will be available for download at http://tyolab.northwestern.edu/tools/.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.", "year": 2015, "citationCount": 50, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "4e244b4d58cd223fb99cfb9363741f4bad772e54", "title": "Automated annotation of rare-cell types from single-cell RNA-sequencing data through synthetic oversampling", "abstract": "The research landscape of single-cell and single-nuclei RNA-sequencing is evolving rapidly. In particular, the area for the detection of rare cells was highly facilitated by this technology. However, an automated, unbiased, and accurate annotation of rare subpopulations is challenging. Once rare cells are identified in one dataset, it is usually necessary to generate further specific datasets to enrich the analysis (e.g., with samples from other tissues). From a machine learning perspective, the challenge arises from the fact that rare-cell subpopulations constitute an imbalanced classification problem. We here introduce a Machine Learning (ML)-based oversampling method that uses gene expression counts of already identified rare cells as an input to generate synthetic cells to then identify similar (rare) cells in other publicly available experiments. We utilize single-cell synthetic oversampling (sc-SynO), which is based on the Localized Random Affine Shadowsampling (LoRAS) algorithm. The algorithm corrects for the overall imbalance ratio of the minority and majority class. We demonstrate the effectiveness of our method for three independent use cases, each consisting of already published datasets. The first use case identifies cardiac glial cells in snRNA-Seq data (17 nuclei out of 8635). This use case was designed to take a larger imbalance ratio (~1 to 500) into account and only uses single-nuclei data. The second use case was designed to jointly use snRNA-Seq data and scRNA-Seq on a lower imbalance ratio (~1 to 26) for the training step to likewise investigate the potential of the algorithm to consider both single-cell capture procedures and the impact of \u201cless\u201d rare-cell types. The third dataset refers to the murine data of the Allen Brain Atlas, including more than 1 million cells. For validation purposes only, all datasets have also been analyzed traditionally using common data analysis approaches, such as the Seurat workflow. In comparison to baseline testing without oversampling, our approach identifies rare-cells with a robust precision-recall balance, including a high accuracy and low false positive detection rate. A practical benefit of our algorithm is that it can be readily implemented in other and existing workflows. The code basis in R and Python is publicly available at FairdomHub, as well as GitHub, and can easily be transferred to identify other rare-cell types.", "year": 2021, "citationCount": 9, "fieldsOfStudy": ["Medicine", "Computer Science", "Biology"]}
{"paperId": "50021ec4c1fd676f9f56a74a6caf529d89dd02bb", "title": "A Framework for Collaborative Curation of Neuroscientific Literature", "abstract": "Large models of complex neuronal circuits require specifying numerous parameters, with values that often need to be extracted from the literature, a tedious and error-prone process. To help establishing shareable curated corpora of annotations, we have developed a literature curation framework comprising an annotation format, a Python API (NeuroAnnotation Toolbox; NAT), and a user-friendly graphical interface (NeuroCurator). This framework allows the systematic annotation of relevant statements and model parameters. The context of the annotated content is made explicit in a standard way by associating it with ontological terms (e.g., species, cell types, brain regions). The exact position of the annotated content within a document is specified by the starting character of the annotated text, or the number of the figure, the equation, or the table, depending on the context. Alternatively, the provenance of parameters can also be specified by bounding boxes. Parameter types are linked to curated experimental values so that they can be systematically integrated into models. We demonstrate the use of this approach by releasing a corpus describing different modeling parameters associated with thalamo-cortical circuitry. The proposed framework supports a rigorous management of large sets of parameters, solving common difficulties in their traceability. Further, it allows easier classification of literature information and more efficient and systematic integration of such information into models and analyses.", "year": 2017, "citationCount": 9, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "500d69558cad3ca286ce91b9ef8d50a96991e96e", "title": "Cellenium\u2014a scalable and interactive visual analytics app for exploring multimodal single-cell data", "abstract": "Abstract Summary Multimodal single-cell sequencing data provide detailed views into the molecular biology of cells. To allow for interactive analyses of such rich data and to readily derive insights from it, new analysis solutions are required. In this work, we present Cellenium, our new scalable visual analytics web application that enables users to semantically integrate and organize all their single-cell RNA-, ATAC-, and CITE-sequencing studies. Users can then find relevant studies and analyze single-cell data within and across studies. An interactive cell annotation feature allows for adding user-defined cell types. Availability and implementation Source code and documentation are freely available under an MIT license and are available on GitHub (https://github.com/Bayer-Group/cellenium). The server backend is implemented in PostgreSQL, Python 3, and GraphQL, the frontend is written in ReactJS, TypeScript, and Mantine css, and plots are generated using plotlyjs, seaborn, vega-lite, and nivo.rocks. The application is dockerized and can be deployed and orchestrated on a standard workstation via docker-compose.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "50b51e5064edef8347f573cc50dede952994bdd4", "title": "Scalable taint specification inference with big code", "abstract": "We present a new scalable, semi-supervised method for inferring taint analysis specifications by learning from a large dataset of programs. Taint specifications capture the role of library APIs (source, sink, sanitizer) and are a critical ingredient of any taint analyzer that aims to detect security violations based on information flow. The core idea of our method is to formulate the taint specification learning problem as a linear optimization task over a large set of information flow constraints. The resulting constraint system can then be efficiently solved with state-of-the-art solvers. Thanks to its scalability, our method can infer many new and interesting taint specifications by simultaneously learning from a large dataset of programs (e.g., as found on GitHub), while requiring few manual annotations. We implemented our method in an end-to-end system, called Seldon, targeting Python, a language where static specification inference is particularly hard due to lack of typing information. We show that Seldon is practically effective: it learned almost 7,000 API roles from over 210,000 candidate APIs with very little supervision (less than 300 annotations) and with high estimated precision (67%). Further, using the learned specifications, our taint analyzer flagged more than 20,000 violations in open source projects, 97% of which were undetectable without the inferred specifications.", "year": 2019, "citationCount": 28, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "52481cc394f5aed25410f970a58ad1dd8683f9f2", "title": "STalign: Alignment of spatial transcriptomics data using diffeomorphic metric mapping", "abstract": null, "year": 2023, "citationCount": 15, "fieldsOfStudy": ["Medicine"]}
{"paperId": "5357b6b45ed0c60c12389dc6e59fa4f31256e2db", "title": "Gradvis typat Python med Cython.", "abstract": "In this report Cython, an experimental programming language which adds functionality to Python was studied. One of the functionalities added by Cython is optional static type annotations. How these optional static type declarations affect performance is the main focus of this report. The effect of these type annotations on type safety and software design aspects of Python was also studied, but to a lesser extent.To establish how static type annotations affect performance three test cases were designed. These test cases were then gradually typed in a number of versions with different amounts of static typing. Finally, the execution times of the different versions were measured.It was found that gradual typing using Cython is a excellent optimization strategy for some Python code, particularly numeric calculations. However, Cythons gradually typed type system can not fully be considered an alternative to popular staticly typed languages with respect to software design considerations.", "year": 2011, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "54cd1a33a978ec279f46a671a13c7883f8a706c4", "title": "Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs", "abstract": "The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.", "year": 2018, "citationCount": 82, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "56175ff48db1e4f21063749f39264097ef784d5c", "title": "Python Programmers have GPUs too Automatic Python Loop Parallelization with Staged Dependence Analysis", "abstract": "Python is a popular language for end-user software development in many application domains. End-users want to harness parallel compute resources effectively, by exploiting commodity manycore technology including GPUs. However, existing approaches to parallelism in Python are esoteric, and generally seem too complex for the typical end-user developer. We argue that implicit, or automatic, parallelization is the best way to deliver the benefits of manycore to end-users, since it avoids domain-specific languages, specialist libraries, complex annotations or restrictive language subsets. Auto-parallelization fits the Python philosophy, provides effective performance, and is convenient for non-expert developers. Despite being a dynamic language, we show that Python is a suitable target for auto-parallelization. In an empirical study of 3000+ open-source Python notebooks, we demonstrate that typical loop behaviour \u2018in the wild\u2019 is amenable to auto-parallelization. We show that staging the dependence analysis is an effective way to maximize performance. We apply classical dependence analysis techniques, then leverage the Python runtime\u2019s rich introspection capabilities to resolve additional loop bounds and variable types in a just-in-time manner. The parallel loop nest code is then converted to CUDA kernels for GPU execution. We achieve orders of magnitude speedup over baseline interpreted execution and some speedup (up to 50x, although not consistently) over CPU JIT-compiled execution, across 12 loop-intensive standard benchmarks.", "year": 2019, "citationCount": 7, "fieldsOfStudy": null}
{"paperId": "574041308529471f8fca5fef16522e337196d813", "title": "Incorporating Practical Single Cell and Spatial Transcriptomics Analysis in a Bioinformatics Course", "abstract": "Single-cell RNA sequencing (scRNAseq) and spatial transcriptomics have emerged as a powerful tool for unraveling the heterogeneity of cell populations and understanding cellular dynamics at unprecedented resolution. However, analyzing scRNAseq and spatial data poses unique challenges due to their high dimensionality and inherent noise. In this paper, a comprehensive tutorial on incorporating practical single-cell and spatial transcriptomics analysis is presented using Scanpy in a bioinformatics course at the University of Belgrade, School of Electrical Engineering. Scanpy, a versatile Python toolkit, offers a wide range of functionalities for quality control, dimensionality reduction, clustering, and trajectory inference. Through lectures and hands-on exercises, students learn essential concepts and techniques, including working with annotated data objects, preprocessing scRNAseq data, performing dimensionality reduction using Principal Component Analysis and Uniform Manifold Approximation and Projection, conducting clustering analysis with the Leiden algorithm, and identifying marker genes for cell type annotation. By equipping students with these skills, we aim to empower the next generation of bioinformaticians to explore the complexities of single-cell and spatial transcriptomics data and advance our understanding of cellular biology.", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "58ded57e020a3543580034ab9f2384772fa65c92", "title": "TGStools: A Bioinformatics Suit to Facilitate Transcriptome Analysis of Long Reads from Third Generation Sequencing Platform", "abstract": "Recent analyses show that transcriptome sequencing can be utilized as a diagnostic tool for rare Mendelian diseases. The third generation sequencing de novo detects long reads of thousands of base pairs, thus greatly expanding the isoform discovery and identification of novel long noncoding RNAs. In this study, we developed TGStools, a bioinformatics suite to facilitate routine tasks such as characterizing full-length transcripts, detecting shifted types of alternative splicing, and long noncoding RNAs (lncRNAs) identification in transcriptome analysis. It also prioritizes the transcripts with a visualization framework that automatically integrates rich annotation with known genomic features. TGStools is a Python package freely available at Github.", "year": 2019, "citationCount": 1, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "59589b39f2d054e52cd74bd25327b28d4266dd13", "title": "easyGWAS: An Integrated Computational Framework for Advanced Genome-Wide Association Studies", "abstract": "Recent advances in sequencing technologies have made it possible for the first time to sequence and analyse the genomes of whole populations of individuals in both a cost-effective manner and in a reasonable amount of time. One of the primary applications of this data is to better understand and investigate the genetic basis of common traits or diseases. For this purpose, genome-wide association studies (GWASs) are often used to find loci that are associated with a phenotype of interest. However, conducting GWASs is a challenging endeavour: first, different types of hidden confounding factors, such as population structure, environmental or technical influences, could lead to spurious associations. Second, it has been shown in several studies that associated loci often fail to explain much of the phenotypic variability \u2014 a phenomenon referred to as the problem of missing heritability. Many tools have been developed to partly address these challenges. The large diversity of these tools, however, have led to a highly fragmented and confusing landscape of tools. In addition, most of these tools do not share a common data format and do not provide straightforward solutions to visualise and annotate their results. In this thesis, we aim to explain more of the missing heritability, while simultaneously simplifying the usage of different methods, by providing an integral solution for performing, visualising and annotating GWASs. Therefore, we develop easyGWASCore, an integrated framework for performing GWASs and meta-analyses. Our framework facilitates the use of popular association methods by providing a common data structure, an application programming interface and a Python command line interface. In addition, easyGWASCore offers an out-of-the-box visualisation and annotation pipeline. We compare the runtime of the easyGWASCore framework to other well-established tools and find that it is at least as efficient as the individual software tools. Next, we enrich the easyGWASCore annotation pipeline with pathogenicity prediction scores to prioritise associated loci for further biological investigation, as well as to narrow down potentially causal loci. However, a large variety of such pathogenicity prediction tools exists and it is not obvious which of these tools work best. We therefore investigate the question whether there are systematic differences in the quality of the predictive performance of pathogenicity prediction tools when evaluated on a large number of variant databases. We find that the evaluation is hindered by two types of circularity and that these types of circularity might lead to spurious biological interpretation. Hence, it is important that scientists are aware of these different types of circularity when pathogenicity prediction tools are used for further experiments or analyses. Increasing sample sizes and combining the results of several GWASs could help to explain parts of the missing heritability. For this purpose, we develop a cloud and web-service, called easyGWAS, to provide a platform to share and publish data and results of GWASs and meta-analyses in a straightforward manner. Simultaneously, easyGWAS facilitates the use of the easyGWASCore framework by providing an easy-to-", "year": 2015, "citationCount": 0, "fieldsOfStudy": ["Biology", "Computer Science"]}
{"paperId": "59fac250d79e2527e4295d42a4c09b341f7bb923", "title": "Micromeda: a genome property prediction pipeline and web visualization tool", "abstract": "Understanding the distribution of biochemical pathways across microorganisms is critical to understanding these organism\u2019s evolution, ecology, and industrial applicability. Advances in genome sequencing and pathway databases have made genomically predicting what pathways an organism possesses a common technique. Researchers are moving on to scaling such analyses towards comparing the presence and absence of pathways across multiple microbes from the same environment or lineage. However, performing such analyses at scale is currently bottlenecked by the sheer number of pathways per organism and the lack of powerful tools to facilitate such comparisons. This thesis presents a new set of tools, called Micromeda, that will assist users in making comparative genomic analyses. Micromeda consists of three core components. These components are Micromeda-Client, which generates interactive heat maps that allow users to perform visual pathway comparisons; Micromeda-Server, which provides data to Micromeda-Client; and Pygenprop, which allows users to perform programmatic comparisons of multiple organism pathways. Micromeda uses the Genome Properties database as its pathway information source. This database is unique from other pathway databases because it maps directly between protein domains and pathway steps. The domains that the database uses are those from the InterPro consortium of protein databases. With Micromeda, the process of discovering an organism\u2019s pathways begins with the domain annotation of an organism\u2019s proteins by InterProScan. Afterwards, Pygenprop is used to combine these annotations with information from the Genome Properties database to predict biochemical pathways. This prediction of pathways from domain data results in the creation of a Micromeda file. This novel file type carries both the pathway annotations for multiple organisms and the sequences of proteins that support these annotations. In the context of the Genome Properties database, such pathways are referred to as genome properties, and pathway annotations are referred to as property assignments. The newly created Micromeda file can later be uploaded to Micromeda-Client and Server for heat map-based visualization. Pygenprop uses object orient programming techniques to represent the Genome Properties database as a series of in-memory objects. These objects are used extensively within Pygenprop\u2019s property assignment process and Micromeda as a whole. Pygenprop is written in Python. The library\u2019s tight integration with the Python data science ecosystem, which results in it being compatible with many emerging data science and machine learning tools, lays the foundation for the library becoming the backbone of a new generation of automated pathway analysis tools.", "year": 2020, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "5ad83378d77bd19fd297a35ffbcbc0bfa0db13ac", "title": "Programmer en Java Ed. 9", "abstract": "De la programmation objet en Java au developpement d'applications Web\nDans cet ouvrage, Claude Delannoy applique au langage Java la demarche pedagogique qui a fait le succes de ses livres sur le C et le C++. Il insiste tout particulierement sur la bonne comprehension des concepts objet et sur l'acquisition de methodes de programmation rigoureuses.\nL'apprentissage du langage se fait en quatre etapes: apprentissage de la syntaxe de base, maitrise de la programmation objet en Java, initiation a la programmation graphique et evenementielle avec la bibliotheque Swing, introduction au developpement Web avec les servlets Java et les JSP.\nL'ouvrage met l'accent sur les apports des versions 5 a 8 de Java Standard Edition: programmation generique, types enumeres, annotations... Un chapitre est dedie aux design patterns en Java et cette 9e edition comporte deux chapitres supplementaires sur des nouveautes majeures de Java 8\u00a0: les streams et les expressions lambda; la gestion du temps, des dates et des heures.\nChaque notion nouvelle et chaque fonction du langage sont illustrees de programmes complets dont le code source est en libre telechargement sur le site www.editions-eyrolles.com.\nA qui s'adresse ce livre\u00a0?\n\nAux etudiants de licence et de master, ainsi qu'aux eleves d'ecoles d'ingenieurs\nA tout programmeur ayant deja une experience de la programmation (Python, PHP, C/C++, C#...) et souhaitant s'initier au langage Java", "year": 2017, "citationCount": 1, "fieldsOfStudy": ["Art"]}
{"paperId": "5b50125d5b8334cd234a12a763697c9df0cdf44e", "title": "visPIG - A Web Tool for Producing Multi-Region, Multi-Track, Multi-Scale Plots of Genetic Data", "abstract": "We present VISual Plotting Interface for Genetics (visPIG; http://vispig.icr.ac.uk), a web application to produce multi-track, multi-scale, multi-region plots of genetic data. visPIG has been designed to allow users not well versed with mathematical software packages and/or programming languages such as R [1], Matlab\u00ae, Python, etc., to integrate data from multiple sources for interpretation and to easily create publication-ready figures. While web tools such as the UCSC Genome Browser [2] or the WashU Epigenome Browser [3] allow custom data uploads, such tools are primarily designed for data exploration. This is also true for the desktop-run Integrative Genomics Viewer (IGV) [4],[5]. Other locally run data visualisation software such as Circos [6] require significant computer skills of the user. The visPIG web application is a menu-based interface that allows users to upload custom data tracks and set track-specific parameters. Figures can be downloaded as PDF or PNG files. For sensitive data, the underlying R [1] code can also be downloaded and run locally. visPIG is multi-track: it can display many different data types (e.g association, functional annotation, intensity, interaction, heat map data,\u2026). It also allows annotation of genes and other custom features in the plotted region(s). Data tracks can be plotted individually or on a single figure. visPIG is multi-region: it supports plotting multiple regions, be they kilo- or megabases apart or even on different chromosomes. Finally, visPIG is multi-scale: a sub-region of particular interest can be 'zoomed' in. We describe the various features of visPIG and illustrate its utility with examples. visPIG is freely available through http://vispig.icr.ac.uk under a GNU General Public License (GPLv3).", "year": 2014, "citationCount": 39, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "5c8bd21386bf1dbc172dad40c0bde3724d86f858", "title": "devCellPy is a machine learning-enabled pipeline for automated annotation of complex multilayered single-cell transcriptomic data", "abstract": null, "year": 2022, "citationCount": 19, "fieldsOfStudy": ["Medicine"]}
{"paperId": "5f96dc9fb68cac34c0af276f2a27e9ac2e9ad45c", "title": "Single Cell Explorer, collaboration-driven tools to leverage large-scale single cell RNA-seq data", "abstract": null, "year": 2019, "citationCount": 26, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "5fca4787cfccadd3a519d2e6fdbcb17b830e8df2", "title": "funkea: Functional Enrichment Analysis in Python", "abstract": "Advancements in Genome-wide association studies (GWAS) have led to the discovery of numerous genetic variants potentially linked to various traits, necessitating effective methods to interpret and summarise these vast data sets. We introduce funkea, a Python package designed to fill this need by providing functional enrichment analysis methods. This tool encompasses popular enrichment approaches under a unified interface and leverages Spark for virtually limitless scale. This allows researchers to conduct pathway, cell-type, and tissue enrichment analysis across diverse annotation datasets. Ultimately, the funkea Python package delivers a highly flexible and scalable solution for functional enrichment analysis in the context of modern genetics workflows. https://github.com/BenevolentAI/funkea", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "60ab4984108c51c2c2aa451c7865f3765d1c7b4a", "title": "Predicting Type Annotations for Python using Embeddings from Graph Neural Networks", "abstract": ": An intelligent tool for type annotations in Python would increase the productivity of developers. Python is a dynamic programming language, and predicting types using static analysis is dif\ufb01cult. Existing techniques for type prediction use deep learning models originated in the area of Natural Language Processing. These models depend on the quality of embeddings for source code tokens. We compared approaches for pre-training embeddings for source code. Speci\ufb01cally, we compared FastText embeddings to embeddings trained with Graph Neural Networks (GNN). Our experiments showed that GNN embeddings outperformed FastText embeddings on the task of type prediction. Moreover, they seem to encode complementary information since the prediction quality increases when both types of embeddings are used.", "year": 2021, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "61106307c956e704e5a609f71b344a68bd51a9f2", "title": "Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python", "abstract": "Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing code-bases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type infer-ence based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present TYPE4Py, a deep similarity learning-based hier-archical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neigh-bor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that TYPE4Py achieves an MRR of 77.1 %, which is a substantial improvement of 8.1% and 16.7% over the state-of-the-art approaches Typilus and Typewriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Stu-dio Code extension, which uses TYPE4Py to provide ML-based type auto-completion for Python.", "year": 2021, "citationCount": 48, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "617b84ff8daca43dc78f055c90ffeff932635e3f", "title": "Photography Style Analysis using Convolutional Neural Networks", "abstract": "In this paper, we present a novel dataset of photographs annotated in terms of the respective image aesthetics. We also examine the ability of Convolutional Neural Networks (CNNs) to distinguish between the adopted photography style classes. In particular, we have defined five photography style classification tasks, related to the following aesthetic attributes: Color, Depth of Field (DoF), Palette, Composition and Type. We then followed an annotation procedure using on a set of 1832 photos selected from the Unsplash dataset. Multiple annotators have also been used, in order to measure inter-annotator agreement. As soon as the dataset was compiled, we trained and evaluated a Residual Neural Network (ResNet50). The experimental results prove that, despite the imbalanced dataset, our model was able to achieve acceptable classification results. The dataset is openly provided, along with the trained models and Python code to use them.", "year": 2022, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "61f46081817f058bf2501fc60a162df5b7c73932", "title": "Flora Prepper: Preparing floras for morphological parsing and integration", "abstract": "The increased availability of digital floras and the application of optical character recognition (OCR) to digitized texts has resulted in exciting opportunities for flora data mining. For example, the software package CharaParser has been developed for the semantic annotation of morphological descriptions from taxonomic treatments (Cui 2012). However, after digitization and OCR processing and before parsing of morphological treatments can begin, content types must be annotated (i.e., text represents names, morphology, discussion or distribution). In addition to enabling morphological parsing, content type annotation also facilitates content search and data linkage. For example, by annotating pieces of a floral treatment, assertions from various floras of the same type can be combined into a single document (i.e., a \"mash-up\" floral treatment). Several products and pipelines have been developed for the semantic annotation, or mark-up, of taxonomic documents (e.g., GoldenGATE, FlorML; Sautter et al. 2012, Hamann et al. 2014). However, these products lack a combination of both ease of implementation (e.g., the ability to run as a script in a programmatic workflow) and the use of modern parsing methods, such as text mining and Natural Language Processing (NLP) approaches.\n Here I present a pilot project implementing text mining and NLP approaches to marking-up floras implemented in Python. I will describe the success of the project, and summarize lessons learned, especially in relation to previous flora markup projects. Annotation of existing flora documents is an essential step towards building next-generation floras (i.e., mash-ups and enhanced floras as platforms) and enables automated trait extraction. Building an easy-to-use access point to modern text mining and NLP techniques for botanical literature will allow for more flexible and responsive flora annotation, and is an important step towards realizing botanical data integration goals.", "year": 2019, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "62c00320bcb7d8ffdac31f04a087f78f71e07e1f", "title": "Large Scale Generation of Labeled Type Data for Python", "abstract": "Recently, dynamically typed languages, such as Python, have gained unprecedented popularity. Although these languages alleviate the need for mandatory type annotations, types still play a critical role in program understanding and preventing runtime errors. An attractive option is to infer types automatically to get static guarantees without writing types. Existing inference techniques rely mostly on static typing tools such as PyType for direct type inference; more recently, neural type inference has been proposed. However, neural type inference is data hungry, and depends on collecting labeled data based on static typing. Such tools, however, are poor at inferring user defined types. Furthermore, type annotation by developers in these languages is quite sparse. In this work, we propose novel techniques for generating high quality types using 1) information retrieval techniques that work on well documented libraries to extract types and 2) usage patterns by analyzing a large repository of programs. Our results show that these techniques are more precise and address the weaknesses of static tools, and can be useful for generating a large labeled dataset for type inference by machine learning methods. F1 scores are 0.52-0.58 for our techniques, compared to static typing tools which are at 0.06, and we use them to generate over 37,000 types for over 700 modules.", "year": 2022, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "64ce9de85f354669b8ca8d35b1db09ceb53e2b51", "title": "NU-AIR - A Neuromorphic Urban Aerial Dataset for Detection and Localization of Pedestrians and Vehicles", "abstract": "This paper presents an open-source aerial neuromorphic dataset that captures pedestrians and vehicles moving in an urban environment. The dataset, titled NU-AIR, features 70.75 minutes of event footage acquired with a 640 x 480 resolution neuromorphic sensor mounted on a quadrotor operating in an urban environment. Crowds of pedestrians, different types of vehicles, and street scenes featuring busy urban environments are captured at different elevations and illumination conditions. Manual bounding box annotations of vehicles and pedestrians contained in the recordings are provided at a frequency of 30 Hz, yielding 93,204 labels in total. Evaluation of the dataset's fidelity is performed through comprehensive ablation study for three Spiking Neural Networks (SNNs) and training ten Deep Neural Networks (DNNs) to validate the quality and reliability of both the dataset and corresponding annotations. All data and Python code to voxelize the data and subsequently train SNNs/DNNs has been open-sourced.", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "64f5c1e3da5f7dc21e11759de666d51052928c26", "title": "CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course", "abstract": "We introduce CS1QA, a dataset for code-based question answering in the programming education domain. CS1QA consists of 9,237 question-answer pairs gathered from chat logs in an introductory programming class using Python, and 17,698 unannotated chat data with code. Each question is accompanied with the student\u2019s code, and the portion of the code relevant to answering the question. We carefully design the annotation process to construct CS1QA, and analyze the collected dataset in detail. The tasks for CS1QA are to predict the question type, the relevant code snippet given the question and the code and retrieving an answer from the annotated corpus.Results for the experiments on several baseline models are reported and thoroughly analyzed. The tasks for CS1QA challenge models to understand both the code and natural language. This unique dataset can be used as a benchmark for source code comprehension and question answering in the educational setting.", "year": 2022, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "658c00695a2b6aecda63e973ab93920800a92e2e", "title": "Pfeature: A Tool for Computing Wide Range of Protein Features and Building Prediction Models", "abstract": "In the last three decades, a wide range of protein features have been discovered to annotate a protein. Numerous attempts have been made to integrate these features in a software package/platform so that the user may compute a wide range of features from a single source. To complement the existing methods, we developed a method, Pfeature, for computing a wide range of protein features. Pfeature allows to compute more than 200,000 features required for predicting the overall function of a protein, residue-level annotation of a protein, and function of chemically modified peptides. It has six major modules, namely, composition, binary profiles, evolutionary information, structural features, patterns, and model building. Composition module facilitates to compute most of the existing compositional features, plus novel features. The binary profile of amino acid sequences allows to compute the fraction of each type of residue as well as its position. The evolutionary information module allows to compute evolutionary information of a protein in the form of a position-specific scoring matrix profile generated using Position-Specific Iterative Basic Local Alignment Search Tool (PSI-BLAST); fit for annotation of a protein and its residues. A structural module was developed for computing of structural features/descriptors from a tertiary structure of a protein. These features are suitable to predict the therapeutic potential of a protein containing non-natural or chemically modified residues. The model-building module allows to implement various machine learning techniques for developing classification and regression models as well as feature selection. Pfeature also allows the generation of overlapping patterns and features from a protein. A user-friendly Pfeature is available as a web server python library and stand-alone package.", "year": 2022, "citationCount": 27, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "66f7d39319289e9ad6f263fc977a0247ba129547", "title": "Transmorph: a unifying computational framework for modular single-cell RNA-seq data integration", "abstract": "Abstract Data integration of single-cell RNA-seq (scRNA-seq) data describes the task of embedding datasets gathered from different sources or experiments into a common representation so that cells with similar types or states are embedded close to one another independently from their dataset of origin. Data integration is a crucial step in most scRNA-seq data analysis pipelines involving multiple batches. It improves data visualization, batch effect reduction, clustering, label transfer, and cell type inference. Many data integration tools have been proposed during the last decade, but a surge in the number of these methods has made it difficult to pick one for a given use case. Furthermore, these tools are provided as rigid pieces of software, making it hard to adapt them to various specific scenarios. In order to address both of these issues at once, we introduce the transmorph framework. It allows the user to engineer powerful data integration pipelines and is supported by a rich software ecosystem. We demonstrate transmorph usefulness by solving a variety of practical challenges on scRNA-seq datasets including joint datasets embedding, gene space integration, and transfer of cycle phase annotations. transmorph is provided as an open source python package.", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "69e0c45e06c612ec92e969767af13feaf5c6b321", "title": "Mixed-species RNA-seq for elucidation of non-cell-autonomous control of gene transcription", "abstract": null, "year": 2018, "citationCount": 23, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "6b52a95acb74bf170917328a8a2705ef10a406e2", "title": "SUMA: a lightweight machine learning model-powered shared nearest neighbour-based clustering application interface for scRNA-Seq data", "abstract": "Background/aim Single-cell transcriptomics (scRNA-Seq) explores cellular diversity at the gene expression level. Due to the inherent sparsity and noise in scRNA-Seq data and the uncertainty on the types of sequenced cells, effective clustering and cell type annotation are essential. The graph-based clustering of scRNA-Seq data is a simple yet powerful approach that presents data as a \u201cshared nearest neighbour\u201d graph and clusters the cells using graph clustering algorithms. These algorithms are dependent on several user-defined parameters. Here we present SUMA, a lightweight tool that uses a random forest model to predict the optimum number of neighbours to obtain the optimum clustering results. Moreover, we integrated our method with other commonly used methods in an RShiny application. SUMA can be used in a local environment (https://github.com/hkarakurt8742/SUMA) or as a browser tool (https://hkarakurt.shinyapps.io/suma/). Materials and methods Publicly available scRNA-Seq datasets and 3 different graph-based clustering algorithms were used to develop SUMA, and a large range for number of neighbours and variant genes was taken into consideration. The quality of clustering was assessed using the adjusted Rand index (ARI) and true labels of each dataset. The data were split into training and test datasets, and the model was built and optimised using Scikit-learn (Python) and randomForest (R) libraries. Results The accuracy of our machine learning model was 0.96, while the AUC of the ROC curve was 0.98. The model indicated that the number of cells in scRNA-Seq data is the most important feature when deciding the number of neighbours. Conclusion We developed and evaluated the SUMA model and implemented the method in the SUMAShiny app, which integrates SUMA with different clustering methods and enables nonbioinformatician users to cluster and visualise their scRNA data easily. The SUMAShiny app is available both for desktop and browser use.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "6cb40055bd871ee2178cea3d535c5c52d63ac3af", "title": "Learning Python Code Suggestion with a Sparse Pointer Network", "abstract": "To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.", "year": 2016, "citationCount": 85, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6d31353e041d806d4fd90a052374da61bee56659", "title": "Abstract 250: UCSC Xena for the visualization and analysis of cancer genomics data", "abstract": "UCSC Xena (http://xena.ucsc.edu/) is a web-based visual integration and exploration tool for multi-omic data and associated clinical and phenotypic annotations. Researchers can easily view and explore public data, their own private data, or both using the Xena Browser. Data is kept on the researcher9s computer (we support Mac, Windows, and Linux) and is never uploaded to public servers. Questions Xena can help you answer: * Is overexpression of this gene associated with lower/higher survival? * Do my two subgroups have differential survival? * Is this gene differentially expressed in tumor vs normal samples? * What is the relationship between mutation, copy number, expression, etc for this gene? Xena showcases seminal cancer genomics datasets from TCGA, the Pan-Cancer Atlas, GDC, PCAWG, ICGC, and more; a total of more than 1500 datasets across 50 cancer types. We support virtually any type of functional genomics data: SNPs, INDELs, copy number variation, gene expression, ATAC-seq, DNA methylation, exon-, transcript-, miRNA-, lncRNA-expression and structural variants. We also support clinical data such as phenotypes, subtype classifications and biomarkers. All of our data is available for download via python or R APIs, or URL links. We show multiple data types side-by-side enabling discovery of correlations across and within genes and genomic regions. Other visualizations and analyses include dynamic Kaplan-Meier survival analysis, powerful filtering and subgrouping, charts, statistical analyses, genomic signatures, and the ability to generate URLs to live views. We link out to the UCSC Genome Browser as well as MuPIT/CRAVAT and TumorMap. New features include: * Data from PCAWG, latest data from the GDC, ATAC-seq from TCGA, and other studies like MET500 * New visualizations for ATAC-seq and DNA methylation data * Multiple survival endpoints for Kaplan-Meier analyses from the PanCan Atlas * Export PDFs from Chart View * Genomic signatures now supported for all datasets, including data from the GDC * Updated navigation to make it easier to dive into any genomic region * Better support for probes (e.g. methylation probes like \"cg16203911\") We are now published in Nature Biotechnology! If you use us, cite us here: https://www.nature.com/articles/s41587-020-0546-8 We have also started to visualize scRNA-seq data including data from the HCA and the literature. Our beta prototype site delivers million-cell-scale multi-omics data for interactive visualization in a web browser. Contact us for access to our beta prototype site. Citation Format: Mary J. Goldman, Brian Craft, Jing-chun Zhu, David Haussler. UCSC Xena for the visualization and analysis of cancer genomics data [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2021; 2021 Apr 10-15 and May 17-21. Philadelphia (PA): AACR; Cancer Res 2021;81(13_Suppl):Abstract nr 250.", "year": 2021, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6d839e244536ac79da64b4bfee68c3299cc616a3", "title": "Python 3 types in the wild: a tale of two type systems", "abstract": "Python 3 is a highly dynamic language, but it has introduced a syntax for expressing types with PEP484. This paper explores how developers use these type annotations, the type system semantics provided by type checking and inference tools, and the performance of these tools. We evaluate the types and tools on a corpus of public GitHub repositories. We review MyPy and PyType, two canonical static type checking and inference tools, and their distinct approaches to type analysis. We then address three research questions: (i) How often and in what ways do developers use Python 3 types? (ii) Which type errors do developers make? (iii) How do type errors from different tools compare? Surprisingly, when developers use static types, the code rarely type-checks with either of the tools. MyPy and PyType exhibit false positives, due to their static nature, but also flag many useful errors in our corpus. Lastly, MyPy and PyType embody two distinct type systems, flagging different errors in many cases. Understanding the usage of Python types can help guide tool-builders and researchers. Understanding the performance of popular tools can help increase the adoption of static types and tools by practitioners, ultimately leading to more correct and more robust Python code.", "year": 2020, "citationCount": 21, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6dc7d659b828d42c1a3d9d968824c43123b19ac2", "title": "Rcount: simple and flexible RNA-Seq read counting", "abstract": "SUMMARY\nAnalysis of differential gene expression by RNA sequencing (RNA-Seq) is frequently done using feature counts, i.e. the number of reads mapping to a gene. However, commonly used count algorithms (e.g. HTSeq) do not address the problem of reads aligning with multiple locations in the genome (multireads) or reads aligning with positions where two or more genes overlap (ambiguous reads). Rcount specifically addresses these issues. Furthermore, Rcount allows the user to assign priorities to certain feature types (e.g. higher priority for protein-coding genes compared to rRNA-coding genes) or to add flanking regions.\n\n\nAVAILABILITY AND IMPLEMENTATION\nRcount provides a fast and easy-to-use graphical user interface requiring no command line or programming skills. It is implemented in C++ using the SeqAn (www.seqan.de) and the Qt libraries (qt-project.org). Source code and 64 bit binaries for (Ubuntu) Linux, Windows (7) and MacOSX are released under the GPLv3 license and are freely available on github.com/MWSchmid/Rcount.\n\n\nCONTACT\nmarcschmid@gmx.ch\n\n\nSUPPLEMENTARY INFORMATION\nTest data, genome annotation files, useful Python and R scripts and a step-by-step user guide (including run-time and memory usage tests) are available on github.com/MWSchmid/Rcount.", "year": 2015, "citationCount": 34, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "6e3355b142d426b6d8e1adbf9fffe0cb25bcc044", "title": "Semantic typing of linked geoprocessing workflows", "abstract": "In Geographic Information Systems (GIS), geoprocessing workflows allow analysts to organize their methods on spatial data in complex chains. We propose a method for expressing workflows as linked data, and for semi-automatically enriching them with semantics on the level of their operations and datasets. Linked workflows can be easily published on the Web and queried for types of inputs, results, or tools. Thus, GIS analysts can reuse their workflows in a modular way, selecting, adapting, and recommending resources based on compatible semantic types. Our typing approach starts from minimal annotations of workflow operations with classes of GIS tools, and then propagates data types and implicit semantic structures through the workflow using an OWL typing scheme and SPARQL rules by backtracking over GIS operations. The method is implemented in Python and is evaluated on two real-world geoprocessing workflows, generated with Esri's ArcGIS. To illustrate the potential applications of our typing method, we formulate and execute competency questions over these workflows.", "year": 2018, "citationCount": 22, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6f5228009482d8d6aaed3372d49c35796911abde", "title": "R and Python Annotation Bindings for OMS", "abstract": "OMS3 is an environmental modeling framework designed to support and simplify the development of scientific environmental models. It is implemented in Java, a programming language that allows the framework to be flexible and non-invasive. Consequently, Java is the native language for developing OMS-compliant components. However, OMS3 aims to ensure the longevity of old model implementations by providing C/C++ and Fortran bindings that allow for connecting slightly modified legacy environmental software to newly developed Java components. In the recent years, three scientific programming languages drew the modeling community\u2019s attention: R, Python, and NetLogo. They have a flat learning curve, numerous scientific libraries, and duck typing makes them an attractive solution for fast scripting. Furthermore, they have an active developer community that keep releasing and improving open source scientific packages. This is a relevant aspect when it comes to facilitating and speeding up the implementation of scientific algorithms. Therefore, OMS3 integration capabilities have recently been enhanced to provide R, Python, and NetLogo bindings. As a result, multi-language modeling solutions can be tailored to meet the scientific community\u2019s needs. Thanks to the framework\u2019s non-invasiveness, R, Python and NetLogo scripts must only be slightly modified with source code annotations to become OMS-compliant components. The resulting components are nevertheless still executable from within the original environments. This contribution shows two actual applications of the implemented R and Python bindings, the NetLogo implementation is not addressed in this paper. The Regional Urban Growth (RUG) is implemented in R and the TRansportation ANalysis SIMulation System (TRANSIMS) models require the Python Run Time Environment (RTE) module to run. The RUG model is a landscape model capable of evaluating impacts of new regional urban development on surrounding environment and projecting long-term growth-management plans. TRANSIMS is a software suite based on a cellular automata microsimulator which performs regional transportation system analyses. Both model suites are among OMS enabled models for the FICUS project, the \u201cFramework for Integrating the Complexity of Uncertain Systems\u201d. Furthermore, the model application flexibility was enhanced by introducing Docker containers in the workflow to alleviate the burden of complex software management and setup. Keywords\u200b: OMS3; R; Python; RUG; TRANSIMS, FICUS", "year": 2018, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7174977e7b36b886f74258a7bca1032370fedbc3", "title": "CIViCpy: A Python Software Development and Analysis Toolkit for the CIViC Knowledgebase", "abstract": "Purpose Precision oncology is dependent upon the matching of tumor variants to relevant knowledge describing the clinical significance of those variants. We recently developed the Clinical Interpretations for Variants in Cancer (CIViC; civicdb.org) crowd-sourced, expert-moderated, open-access knowledgebase, providing a structured framework for evaluating genomic variants of various types (e.g., fusions, SNVs) for their therapeutic, prognostic, predisposing, diagnostic, or functional utility. CIViC has a documented API for accessing CIViC records: Assertions, Evidence, Variants, and Genes. Third-party tools that analyze or access the contents of this knowledgebase programmatically must leverage this API, often reimplementing redundant functionality in the pursuit of common analysis tasks that are beyond the scope of the CIViC web application. Methods To address this limitation, we developed CIViCpy (civicpy.org), a software development kit (SDK) for extracting and analyzing the contents of the CIViC knowledgebase. CIViCpy enables users to query CIViC content as dynamic objects in Python. We assess the viability of CIViCpy as a tool for advancing individualized patient care by using it to systematically match CIViC evidence to observed variants in patient cancer samples. Results We used CIViCpy to evaluate variants from 59,437 sequenced tumors of the AACR Project GENIE dataset. We demonstrate that CIViCpy enables annotation of >1,200 variants per second, resulting in precise variant matches to CIViC level A (professional guideline) or B (clinical trial) evidence for 38.6% of tumors. Conclusions The clinical interpretation of genomic variants in cancers requires high-throughput tools for interoperability and analysis of variant interpretation knowledge. These needs are met by CIViCpy, an SDK for downstream applications and rapid analysis. CIViCpy (civicpy.org) is fully documented, open-source, and freely available online.", "year": 2019, "citationCount": 10, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"]}
{"paperId": "72b2806b8dd1ee7523a579bdbc5dac30f40010c4", "title": "ScType enables fast and accurate cell type identification from spatial transcriptomics data", "abstract": "Abstract Summary The limited resolution of spatial transcriptomics (ST) assays in the past has led to the development of cell type annotation methods that separate the convolved signal based on available external atlas data. In light of the rapidly increasing resolution of the ST assay technologies, we made available and investigated the performance of a deconvolution-free marker-based cell annotation method called scType. In contrast to existing methods, the spatial application of scType does not require computationally strenuous deconvolution, nor large single-cell reference atlases. We show that scType enables ultra-fast and accurate identification of abundant cell types from ST data, especially when a large enough panel of genes is detected. Examples of such assays are Visium and Slide-seq, which currently offer the best trade-off between high resolution and number of genes detected by the assay for cell type annotation. Availability and implementation scType source R and python codes for spatial data are openly available in GitHub (https://github.com/kris-nader/sp-type or https://github.com/kris-nader/sc-type-py). Step-by-step tutorials for R and python spatial data analysis can be found in https://github.com/kris-nader/sp-type and https://github.com/kris-nader/sc-type-py/blob/main/spatial_tutorial.md, respectively.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "72b65e3b36d9b727bf5a6873ff6c3ded872ef3cb", "title": "GenomeQC: a quality assessment tool for genome assemblies and gene structure annotations", "abstract": null, "year": 2019, "citationCount": 49, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "72cc0b36560030917971c2e9b77969d0377b031f", "title": "DeepGSR: an optimized deep-learning structure for the recognition of genomic signals and regions", "abstract": "Abstract Motivation Recognition of different genomic signals and regions (GSRs) in DNA is crucial for understanding genome organization, gene regulation, and gene function, which in turn generate better genome and gene annotations. Although many methods have been developed to recognize GSRs, their pure computational identification remains challenging. Moreover, various GSRs usually require a specialized set of features for developing robust recognition models. Recently, deep-learning (DL) methods have been shown to generate more accurate prediction models than \u2018shallow\u2019 methods without the need to develop specialized features for the problems in question. Here, we explore the potential use of DL for the recognition of GSRs. Results We developed DeepGSR, an optimized DL architecture for the prediction of different types of GSRs. The performance of the DeepGSR structure is evaluated on the recognition of polyadenylation signals (PAS) and translation initiation sites (TIS) of different organisms: human, mouse, bovine and fruit fly. The results show that DeepGSR outperformed the state-of-the-art methods, reducing the classification error rate of the PAS and TIS prediction in the human genome by up to 29% and 86%, respectively. Moreover, the cross-organisms and genome-wide analyses we performed, confirmed the robustness of DeepGSR and provided new insights into the conservation of examined GSRs across species. Availability and implementation DeepGSR is implemented in Python using Keras API; it is available as open-source software and can be obtained at https://doi.org/10.5281/zenodo.1117159. Supplementary information Supplementary data are available at Bioinformatics online.", "year": 2018, "citationCount": 53, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "73c29f57e92cc04a50308c7045457a918aa2484b", "title": "Building Family Trees With NooJ", "abstract": "Croatian language uses separate terms for each member in a family tree. These terms may differ among different geographical parts of Croatia as well. We will use NooJ to build a family tree by using syntactic grammars applied to the Croatian obituaries. Obituaries in Croatia have a recognizable form with few alternations. Its structure can be divided into four main sections (notice about the death, name of the deceased, date and place of the funeral, grieving family). For this project, sections two and four are of special interest as they can be used to build a family tree of both living and deceased relatives of the deceased. The first task in the project is to recognize the deceased person since other relationships might differ depending of the gender of the deceased. Than a list of grieving family members, with or without their names (sometimes with a notation such as \u2018the family of deceased brother/sister/\u2026) is annotated, again, in some relationships differing on the gender of the name that follows the relationship type. Across Croatia, the same relationship is not always presented with a same term. For example, a nephew is presented with terms necak, bratanac and sinovac depending on a region. So far, we were able to include all the variations in our grammars. \nThe relationships are provided in three different formats: \n1. Singular relationship and a name of a person: brat Ivan i brat Nikola (brother Ivan and brother Nikola) which are annotated as ; \n2. Plural relationship of the same gender and a list of names: braca Ivan, Petar i Nikola (brothers Ivan, Petar and Nikola) annotated also as singular relationships ; \n3. Plural relationship including both genders and a list of names: djeca Nikola, Ana, Frano, Marija (children Nikola, Ana, Frano, Marija) annotated as , where each person is annotated as a or as a depending on the gender of the name and not as a group . \nHowever, each of these cases can have some additional information such as: obitelj pokojnog brata Nikole (family of the deceased brother Nikola); brat Nikola s obitelji (brother Nikola with his family);kcer Marija sa suprugom i kcerkom (daughter Marija with her husband and her daughter). After the annotation process, an XML file is produced containing tags . From this file, a picture with drown relationships is generated using Python.", "year": 2015, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "73db8bf26af340ff7907957186fa52b8c5d070e9", "title": "INSTINCT: The infrastructure for noise and soundscape tolerant investigation of nonspecific call types", "abstract": "INSTINCT is open-source, command line software for custom data pipelines, developed by the NOAA\u2019s Alaska Fisheries Science Center to formalize AI workflows for passive acoustic monitoring (PAM) of sounds produced by marine mammals and to provide a framework for new algorithm development. INSTINCT is built in Python over the Luigi framework and was designed to be lightweight and Window/Linux compatible. To promote collaboration, the software is partitioned into a core module and organization-governed submodules. The original INSTINCT detection and classification algorithm has been applied to various Alaska region cetacean species and presents a powerful CPU-based approach for those seeking to train, evaluate, and deploy a generalized detector. Furthermore, INSTINCT pipelines are in development to train and deploy GPU-based algorithms, structure annotation workflows, and pioneer complex workflows. Case studies will be presented to highlight the existing capabilities of the software and new workflows that are currently being explored. This talk will also cover design philosophy, technical specifications, and instructions for implementation. Although INSTINCT continues to progress at AFSC, it will benefit greatly from a larger network of collaborators, and ideally, will give back to the community via innovation of new AI approaches in PAM.", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "74134d77b76c6577a8ea939a7c8f04db50fcc3f0", "title": "An Industry Classification Model of Small and Medium-sized Enterprises based on TF-IDF Characteristics", "abstract": "This paper selects the data of the national SME Information Disclosure System, uses the TensorFlow in Python to establish the corresponding learning framework, according to its business scope to carry on the corresponding classification. The Jieba participle in Python is first used to remove extraneous words from the business scope of the enterprise. Secondly, using the simple Bayesian text classification model, using Chi as the basis of feature selection, the multi-dimensional characteristics of each type of business scope are selected and re-weighed. After that, the VSM model is constructed for each business scope, which classifies it according to probability. Then, XG-boost is used to encode all the words one-hot, the tree-based model XG-boost is used to make decisions on the processing capacity of tabular data, and prune categories below the threshold. Then, the convolution neural network is used to encode the vocabulary, the lexical annotation is added to the participle, the Gensim training word vector is used, then the cosine similarity is used to calculate, and the classification results are finally obtained.", "year": 2019, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7638d6dfbf284347fb47691696cc492185b65d17", "title": "Abstract 7406: Visualization and analysis of cancer genomics data using UCSC Xena", "abstract": "\n UCSC Xena (http://xena.ucsc.edu/) is a web-based visual integration and exploration tool for multi-omic data and associated clinical and phenotypic annotations. Researchers can easily view and explore public data, their own private data, or both using the Xena Browser. Private data are kept on the researcher's computer and are never uploaded to our public servers. We support Mac, Windows, and Linux.\n Questions Xena can help you answer:* Is overexpression of this gene associated with lower/higher survival?* What genes are differentially expressed between these two groups of samples?* What is the relationship between mutation, copy number, expression, etc for this gene?\n Xena showcases seminal cancer genomics datasets from TCGA, the Pan-Cancer Atlas, GDC, PCAWG, ICGC, and more; a total of more than 1500 datasets across 50 cancer types. We support virtually any type of functional genomics data: SNPs, INDELs, copy number variation, gene expression, ATAC-seq, DNA methylation, exon-, transcript-, miRNA-, lncRNA-expression and structural variants. We also support clinical data such as phenotype information, subtype classifications and biomarkers. All of our data is available for download via python or R APIs, or using our URL links.\n Our signature Visual Spreadsheet view shows multiple data types side-by-side enabling discovery of correlations across and within genes and genomic regions. We also have dynamic Kaplan-Meier survival analysis, powerful filtering and subgrouping, differential gene expression analysis, GSEA, charts, statistical analyses, genomic signatures, and the ability to generate URLs to live views.\n New features include:\n * Genome-wide differential gene expression analysis\n * GSEA analysis\n * Select samples directly from the screen for filtering and creating subgroups\n * Loading of Microsoft Excel files\n Our beta prototype site for visualizing single-cell data delivers million-cell-scale multi-omics data for interactive visualization in a web browser. Contact us for access to our beta prototype site.\n If you use us please cite our publication in Nature Biotechnology: https://www.nature.com/articles/s41587-020-0546-8\n Citation Format: Mary Goldman, Brian Craft, Jingchun Zhu, David Haussler. Visualization and analysis of cancer genomics data using UCSC Xena [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2024; Part 1 (Regular Abstracts); 2024 Apr 5-10; San Diego, CA. Philadelphia (PA): AACR; Cancer Res 2024;84(6_Suppl):Abstract nr 7406.", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "7672d26ac9eeb63a5841d871468ee02dbdd3b7d7", "title": "Self Containment, a Property of Modular RNA Structures, Distinguishes microRNAs", "abstract": "RNA molecules will tend to adopt a folded conformation through the pairing of bases on a single strand; the resulting so-called secondary structure is critical to the function of many types of RNA. The secondary structure of a particular substring of functional RNA may depend on its surrounding sequence. Yet, some RNAs such as microRNAs retain their specific structures during biogenesis, which involves extraction of the substructure from a larger structural context, while other functional RNAs may be composed of a fusion of independent substructures. Such observations raise the question of whether particular functional RNA substructures may be selected for invariance of secondary structure to their surrounding nucleotide context. We define the property of self containment to be the tendency for an RNA sequence to robustly adopt the same optimal secondary structure regardless of whether it exists in isolation or is a substring of a longer sequence of arbitrary nucleotide content. We measured degree of self containment using a scoring method we call the self-containment index and found that miRNA stem loops exhibit high self containment, consistent with the requirement for structural invariance imposed by the miRNA biogenesis pathway, while most other structured RNAs do not. Further analysis revealed a trend toward higher self containment among clustered and conserved miRNAs, suggesting that high self containment may be a characteristic of novel miRNAs acquiring new genomic contexts. We found that miRNAs display significantly enhanced self containment compared to other functional RNAs, but we also found a trend toward natural selection for self containment in most functional RNA classes. We suggest that self containment arises out of selection for robustness against perturbations, invariance during biogenesis, and modular composition of structural function. Analysis of self containment will be important for both annotation and design of functional RNAs. A Python implementation and Web interface to calculate the self-containment index are available at http://kim.bio.upenn.edu/software/.", "year": 2008, "citationCount": 26, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "76e9b27932f93d0c639c4257734031ee54ea8254", "title": "Building and inferring knowledge bases using biomedical text mining", "abstract": "Biomedical researchers have the overwhelming task of keeping abreast of the latest research. This is especially true in the field of personalized cancer medicine where knowledge from different areas such as clinical trials, preclinical studies, and basic science research needs to be combined. We propose that automated text mining methods should become a commonplace tool for researchers to help them locate relevant research, assimilate it quickly and collate for hypothesis generation. To move towards this goal, we focus on extracting relations from published abstracts and full-text papers. We first explore the use of co-occurrences in sentences and develop a method for inferring new co-occurrences that can be used for hypothesis generation. We next explore more advanced relation extraction methods by developing a supervised learning method, VERSE, which won part of the BioNLP 2016 Shared Task. Our classical method outperforms a deep learning method showing its applicability to text mining problems with limited training data. We develop it further into the Kindred Python package which integrates with other biomedical text mining resources and is easily applied to other biomedical problems. Finally, we examine the applicability of these methods in personalized cancer research. The specific role of genes in different cancer types as drivers, oncogenes, and tumor suppressors is essential information when interpreting an individual cancer genome. We built CancerMine, a high-quality knowledgebase, using the Kindred classifier and annotations from a team of annotators. This allows for quantifiable comparisons of different cancer types based on the importance of different genes. The clinical relevance of cancer mutations is generally locked in the raw text of literature and was the focus of the CIViCmine project. As a collaboration with the Clinical Interpretation of Variants in Cancer (CIViC) project team, we built methods to prioritise relevant papers for curation. Through this work, we have focussed on different ways to extract structured knowledge from individual sentences in biomedical publications. The methods, guidelines, and results developed will aid biomedical text mining research and the personalized cancer treatment community.", "year": 2018, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "77b1916f4ae90f9324c66f5fdcb8bd9875a8d334", "title": "GOAT: Genetic Output Analysis Tool: An open source GWAS and genomic region visualization tool", "abstract": "Genome wide association studies (GWAS) are a widely used approach in genetic research to identify genes or genetic variants involved in human diseases. Each GWAS examines millions of unique single nucleotide polymorphisms (SNPs) for their association to phenotypic traits and diseases. In the context of identifying complex associations in large patient cohorts, this type of study involves a vast amount of clinical and genetic data. In order to analyze these complex datasets efficiently we have developed the Genetic Output Analysis Tool (GOAT) to improve visualization and annotation of GWAS data. GOAT offers interactive search capabilities of GWAS results via specific queries to identify significant associations between multiple SNPs and phenotypes. GOAT was designed to be scalable and operate on top of \"Big Data\" technologies. The software interface offers researchers new visualization tools to help analyze this complex data. It is programmed in python and can be connected directly to any database using an Apache server. This paper outlines some of the GOAT's leading features and characteristics and compares them to existing open source GWAS visualization tools such as Locus Zoom and the Integrative Genomics Viewer (IGV). We also present future development plans for GOAT in order to provide researchers with improved performance, the visualization tools and ability to mine GWAS data for the most interesting and relevant information from their data.", "year": 2016, "citationCount": 4, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7a6eab9a15a5c8e74d7aba2fe6b54f22245ba0c5", "title": "Automated Mechanical Ventilator Waveform Analysis of Patient-Ventilator Asynchrony", "abstract": "AUTOMATED MECHANICAL VENTILATOR WAVEFORM ANALYSIS OF PATIENT-VENTILATOR ASYNCHRONY MAS , MD , Lieng , Jason Adams, MD, Brooks Kuhn, Monica BS, Jean-Pierre Delplanque, PhD , Edward Guo , Sean Peisert, PhD , Nick Anderson, PhD 1 UC Davis Medical Center, Department of Pulmonary and Critical Care Medicine 2 UC Davis School of Medicine 3 UC Davis, Department of Engineering 4 UC Davis, Department of Computer Science 5 UC Davis, Department of Pathology Figure 1: Bedside to Cloud mechanical ventilator waveform pipeline Purpose Simulation Center Mechanical ventilation is a life-saving intervention but is also associated with adverse effects including ventilator-induced lung injury (VILI). Patient-ventilator asynchrony (PVA) is thought to contribute to VILI, but the study of PVA has been hampered by limited access to the high frequency, large volume data streams produced by modern ventilators and a lack of robust PVA analytics. To address these limitations, we developed an automated pipeline for breath-by-breath analysis of mechanical ventilator waveform data. Wireless Micro- Computer Attached to Ventilator Wireless Server Secure Analytic Environment ANALYTICS: \u2022 TDAP \u2022 Python \u2022 Machine Learning \u2022 R/SAS \u2022 Tableau Critical Care Registry ASCEII Medical ICU Methods Double Trigger Ventilator Data Asynchrony Meta-Data Data Acquisition >>>>>>>>>>>>>>>>>> Storage >>>>>>>>>>>>>>>>>>>>>>> Analysis>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Figure 3: Charted tidal volumes compared to continuously recorded tidal volumes of a subject with ARDS, subsequently treated with neuromuscular blockade Cisatracurium started 8 ml/kg PBW 6 ml/kg PBW Recorded Tidal Volumes Suction Common Patient-Ventilator Asynchronies and \u201cArtifacts\u201d Breath Flow Ventilator Stacking Asynchrony Cough Disconnect Waveform Reconstitution Virtual Desktop Access Tidal Volume (mL) Simulated pressure and flow time series data representing normal breaths and common forms of PVA were generated on mechanical ventilators, collected unobtrusively using wireless peripheral devices, and transmitted to a networked server for storage and analysis. Two critical care physicians reviewed waveforms to generate gold standard data sets of PVA events. Rule-based algorithms were developed to quantify inspiratory and expiratory tidal volumes (TV) and identify common PVA subtypes including double trigger and delayed termination asynchrony. Data were split randomly into derivation and validation sets. Algorithm performance was compared with ventilator reported values and clinician annotation. Raw Waveform Data (CSV File) Results The mean difference between algorithm-determined and ventilator- reported TVs was 3.1% (99% CI \u00b1 1.36%). Algorithm agreement with clinician annotation was excellent for double trigger PVA and moderate for delayed termination PVA, with Kappa statistics of 0.85 and 0.58, respectively. In the validation data set (n = 492 breaths), double trigger asynchrony was detected with an overall accuracy of 94.1%, a sensitivity of 100%, and a specificity of 92.8%. Conclusions Tidal Volume (mL) Figure 2: Derivation and validation of double trigger algorithm n=214 n=139 n=492 Rater 1 Rater 2 Algorithm Rater 1 Scan QR code for access to references via Dropbox.com Rater 2 Delayed Termination Algorithm 8 ml/kg PBW 6 ml/kg PBW Double Trigger Delayed Termination Double Trigger Sensitivity Specificity n=90 Charted Tidal Volumes n=179 Validation Breath Number Figure 4: PVA frequency pre and post cisatracurium. Severe breath stacking is defined as TVe:TVi 10 to 33%. Moderate breath stacking defined as as TVe:TVi 33 to 67%. % Asynchronous Breaths Derivation Double trigger dbl_trig A pipeline combining wireless ventilator data acquisition and rule-based signal analysis algorithms informed by the principles of bedside ventilator waveform analysis allows for accurate, automated, quantitative breath-by- breath analysis of patient-ventilator interactions. Clinical Impact Severe breath stacking bs_severe Moderate breath stacking bs_moderate Composite composite_dbl_bs Total (%) Pre-Cisatra (%) During Cisatra (%) Post-Cisatra (%) Phase of Mechanical Ventilation Improved classification and automated identification of double triggers, tidal volume, and other common types of PVA allowing more robust analysis of injurious patient-ventilator interactions Future plans include: \u2022 Expansion to all mechanically ventilated patients at UCDMC, including the emergency department \u2022 Comparison of rules-based algorithms to machine based learning algorithms \u2022 Outcomes correlation to PVA/TV metadata", "year": 2015, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "7b46e9c7bffa3ab6945a3a1c6131ba077e63e3c6", "title": "Exploring Geospatial Data Visualization Based on Python", "abstract": "OpenStreetMap (OSM), as a typical representative of crowdsourced geographic information data, covers a large amount of open source geographic information data, which provides freely editable world-class geographic data for realizing information mining, analysis and visualization of geospatial data. Based on the geographic information data from the official website of OSM, this paper uses Python's geographic information data processing technology, geospatial data visualization technology and layer overlay method based on vector data analysis algorithm to realize visualization and information annotation of various types of geospatial data, providing reliable data processing, visualization and analysis methods for domestic resource exploration, transportation, epidemic prevention and control and other fields.", "year": 2022, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7c5639f770fd072ceb09772ea3bdc4f197a08b0c", "title": "Multi-template matching: a versatile tool for object-localization in microscopy images", "abstract": null, "year": 2019, "citationCount": 28, "fieldsOfStudy": ["Computer Science", "Biology", "Medicine"]}
{"paperId": "7d947fd7b995c787baf4de21ddb2055e2838264d", "title": "Vaeda computationally annotates doublets in single-cell RNA sequencing data", "abstract": "Motivation Single-cell RNA sequencing (scRNA-seq) continues to expand our knowledge by facilitating the study of transcriptional heterogeneity at the level of single cells. Despite this technology\u2019s utility and success in biomedical research, technical artifacts are present in scRNA-seq data. Doublets/multiplets are a type of artifact that occurs when two or more cells are tagged by the same barcode, and therefore they appear as a single cell. Because this introduces non-existent transcriptional profiles, doublets can bias and mislead downstream analysis. To address this limitation computational methods to annotate and remove doublets form scRNA-seq datasets are needed. Results We introduce vaeda, a new approach for computational annotation of doublets in scRNA-seq data. Vaeda integrates a variational auto-encoder and Positive-Unlabeled learning to produce doublet scores and binary doublet calls. We apply vaeda, along with seven existing doublet annotation methods, to sixteen benchmark datasets and find that vaeda performs competitively in terms of doublet scores and doublet calls. Notably, vaeda outperforms other python-based methods for doublet annotation. All together, vaeda is a robust and competitive method for scRNA-seq doublet annotation and may be of particular interest in the context of python-based workflows. Availability Vaeda is available at https://github.com/kostkalab/vaeda Contact kostka@pitt.edukostka@pitt.edu", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Medicine", "Biology", "Computer Science"]}
{"paperId": "809238dddf5be36a7b34c6152fa10393543cd6b8", "title": "PrismEXP: gene annotation prediction from stratified gene-gene co-expression matrices", "abstract": "Background Gene-gene co-expression correlations measured by mRNA-sequencing (RNA-seq) can be used to predict gene annotations based on the co-variance structure within these data. In our prior work, we showed that uniformly aligned RNA-seq co-expression data from thousands of diverse studies is highly predictive of both gene annotations and protein-protein interactions. However, the performance of the predictions varies depending on whether the gene annotations and interactions are cell type and tissue specific or agnostic. Tissue and cell type-specific gene-gene co-expression data can be useful for making more accurate predictions because many genes perform their functions in unique ways in different cellular contexts. However, identifying the optimal tissues and cell types to partition the global gene-gene co-expression matrix is challenging. Results Here we introduce and validate an approach called PRediction of gene Insights from Stratified Mammalian gene co-EXPression (PrismEXP) for improved gene annotation predictions based on RNA-seq gene-gene co-expression data. Using uniformly aligned data from ARCHS4, we apply PrismEXP to predict a wide variety of gene annotations including pathway membership, Gene Ontology terms, as well as human and mouse phenotypes. Predictions made with PrismEXP outperform predictions made with the global cross-tissue co-expression correlation matrix approach on all tested domains, and training using one annotation domain can be used to predict annotations in other domains. Conclusions By demonstrating the utility of PrismEXP predictions in multiple use cases we show how PrismEXP can be used to enhance unsupervised machine learning methods to better understand the roles of understudied genes and proteins. To make PrismEXP accessible, it is provided via a user-friendly web interface, a Python package, and an Appyter. AVAILABILITY. The PrismEXP web-based application, with pre-computed PrismEXP predictions, is available from: https://maayanlab.cloud/prismexp; PrismEXP is also available as an Appyter: https://appyters.maayanlab.cloud/PrismEXP/; and as Python package: https://github.com/maayanlab/prismexp.", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "80fd152033705a15873a5941ec5eaa43673487d2", "title": "msCentipede: Modeling Heterogeneity across Genomic Sites and Replicates Improves Accuracy in the Inference of Transcription Factor Binding", "abstract": "Understanding global gene regulation depends critically on accurate annotation of regulatory elements that are functional in a given cell type. CENTIPEDE, a powerful, probabilistic framework for identifying transcription factor binding sites from tissue-specific DNase I cleavage patterns and genomic sequence content, leverages the hypersensitivity of factor-bound chromatin and the information in the DNase I spatial cleavage profile characteristic of each DNA binding protein to accurately infer functional factor binding sites. However, the model for the spatial profile in this framework fails to account for the substantial variation in the DNase I cleavage profiles across different binding sites. Neither does it account for variation in the profiles at the same binding site across multiple replicate DNase I experiments, which are increasingly available. In this work, we introduce new methods, based on multi-scale models for inhomogeneous Poisson processes, to account for such variation in DNase I cleavage patterns both within and across binding sites. These models account for the spatial structure in the heterogeneity in DNase I cleavage patterns for each factor. Using DNase-seq measurements assayed in a lymphoblastoid cell line, we demonstrate the improved performance of this model for several transcription factors by comparing against the Chip-seq peaks for those factors. Finally, we explore the effects of DNase I sequence bias on inference of factor binding using a simple extension to our framework that allows for a more flexible background model. The proposed model can also be easily applied to paired-end ATAC-seq and DNase-seq data. msCentipede, a Python implementation of our algorithm, is available at http://rajanil.github.io/msCentipede.", "year": 2015, "citationCount": 35, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "8126210233eb8f877f5c994e44b00a05a041d492", "title": "SBOannotator: a Python tool for the automated assignment of systems biology ontology terms", "abstract": "Abstract Motivation The number and size of computational models in biology have drastically increased over the past years and continue to grow. Modeled networks are becoming more complex, and reconstructing them from the beginning in an exchangeable and reproducible manner is challenging. Using precisely defined ontologies enables the encoding of field-specific knowledge and the association of disparate data types. In computational modeling, the medium for representing domain knowledge is the set of orthogonal structured controlled vocabularies named Systems Biology Ontology (SBO). The SBO terms enable modelers to explicitly define and describe model entities, including their roles and characteristics. Results Here, we present the first standalone tool that automatically assigns SBO terms to multiple entities of a given SBML model, named the SBOannotator. The main focus lies on the reactions, as the correct assignment of precise SBO annotations requires their extensive classification. Our implementation does not consider only top-level terms but examines the functionality of the underlying enzymes to allocate precise and highly specific ontology terms to biochemical reactions. Transport reactions are examined separately and are classified based on the mechanism of molecule transport. Pseudo-reactions that serve modeling purposes are given reasonable terms to distinguish between biomass production and the import or export of metabolites. Finally, other model entities, such as metabolites and genes, are annotated with appropriate terms. Including SBO annotations in the models will enhance the reproducibility, usability, and analysis of biochemical networks. Availability and implementation SBOannotator is freely available from https://github.com/draeger-lab/SBOannotator/.", "year": 2023, "citationCount": 8, "fieldsOfStudy": ["Medicine"]}
{"paperId": "81dbff5b9164e6b5245324c8c464e6d5d688e1db", "title": "Programmer en Java : Couvre Java 9 Ed. 10", "abstract": "De la programmation objet en Java au developpement d'applications Web \n \n Dans cet ouvrage, Claude Delannoy applique au langage Java la demarche pedagogique qui a fait le succes de ses livres sur le C et le C++. Il insiste tout particulierement sur la bonne comprehension des concepts objet et sur l'acquisition de methodes de programmation rigoureuses. \n \n L'apprentissage du langage se fait en quatre etapes\u00a0: apprentissage de la syntaxe de base, maitrise de la programmation objet en Java, initiation a la programmation graphique et evenementielle avec la bibliotheque Swing, introduction au developpement Web avec les servlets Java et les JSP \n \n L'ouvrage met l'accent sur les apports des versions 5 a 8 de Java Standard Edition\u00a0: programmation generique, types enumeres, annotations, streams et expressions lambda... Un chapitre est dedie aux design patterns en Java et cette 10e edition comporte deux chapitres supplementaires sur des nouveautes majeures de Java 9\u00a0: \n \n L'outil JShell, tres utile dans le cadre de l'apprentissage du langage puisqu'il permet de tester rapidement des blocs de code sans creer une application complete. \n \n Le Java Platform Module System (ex-projet Jigsaw), qui revolutionne la maniere de structurer ses applications Java. \n \n Chaque notion nouvelle et chaque fonction du langage sont illustrees de programmes complets dont le code source est disponible en telechargement sur le site www.editions-eyrolles.com. \n \n A qui s'adresse ce livre\u00a0? \n \n \n Aux etudiants de licence et de master, ainsi qu'aux eleves d'ecoles d'ingenieurs. \n \n A tout programmeur ayant deja une experience de la programmation (Python, PHR C/C++, C#...) et souhaitant s'initier au langage Java.", "year": 2017, "citationCount": 0, "fieldsOfStudy": ["Art"]}
{"paperId": "82d9e1e496b4849faef89380704a2b17a706cf99", "title": "BioAnalyzer: Bioinformatic Software of Routinely Used Tools for Analysis of Genomic Data", "abstract": "The massive extension in biological data induced a need for user-friendly bioinformatics tools could be used for routine biological data manipulation. Bioanalyzer is a simple analytical software implements a variety of tools to perform common data analysis on different biological data types and databases. Bioanalyzer provides general aspects of data analysis such as handling nucleotide data, fetching different data formats information, NGS quality control, data visualization, performing multiple sequence alignment and sequence BLAST. These tools accept common biological data formats and produce human-readable output files could be stored on local computer machines. Bioanalyzer has a user-friendly graphical user interface to simplify massive biological data analysis and consume less memory and processing power. Bioanalyzer source code was written through Python programming language which provides less memory usage and initial startup time. Bioanalyzer is a free and open source software, where its code could be modified, extended or integrated in different bioinformatics pipelines. Bioinformatics Produce huge data in FASTA and Genbank format which can be used to produce a lot of annotation information which can be done with Python programming language that open the door form bioinformatics tool due to their elasticity in data analysis and simplicity which inspire us to develop new multiple tool software able to manipulate FASTA and Genbank files. The goal Develop new software uses Genomic data files to produce annotated data. Software was written using python programming language and biopython packages.", "year": 2019, "citationCount": 7, "fieldsOfStudy": ["Biology"]}
{"paperId": "851bc40b66451ffd44281a0d140563bf285bb989", "title": "Characterization of Institutional Texts for an Automated Golden Standard: Enhancing Machine Translation Quality Assessment between English and Spanish", "abstract": "The purpose of this paper is to collect a set of features that can contribute to the linguistic characterization of the institutional textual genre. The aim is to describe as exhaustively as possible the archetypal text to be obtained as a target text in this type of specialized translation. The tools used were Orange Data Mining\u00a9 and Google Colab (Python code), and the data was obtained using the following processing mechanisms: word cloud, text preprocessing (cleaning, tokenization, normalization, lemmatization and PoS annotation). With these tools, lexical and grammatical frequencies, lexical and documentary embeddings, cosine distances, hierarchical clustering, and 20-component dimensionality reduction (t-SNE) were extracted. As a result, a series of useful descriptive parameters have been obtained for the characterization of model texts for economic translation of institutional domains into Spain Spanish: lexical and terminological density, phraseological and terminological lexicalizations, grammatical frequencies, and semantic maps. In conclusion, the study provides several quantifiable features that characterize the analyzed register and opens the way for further research to deepen these parameters and develop the research by searching for complementary parameters until a complete and exhaustive picture of the reference model in this genre is obtained. Keywords: Machine Translation, Golden Standard, Translation Quality Assessment, Specialized Translation, AI Processing.", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "85baa64c0c7b690dfbdc6d0643f627dbc6713a66", "title": "Accelerating Dynamically-Typed Languages on Heterogeneous Platforms Using Guards Optimization", "abstract": "Scientific applications are ideal candidates for the \"heterogeneous computing\" paradigm, in which parts of a computation are \"offloaded\" to available accelerator hardware such as GPUs. However, when such applications are written in dynamic languages such as Python or R, as they increasingly are, things become less straightforward. The same flexibility that makes these languages so appealing to programmers also significantly complicates the problem of automatically and transparently partitioning a program's execution between a CPU and available accelerator hardware without having to rely on programmer annotations.\nA common way of handling the features of dynamic languages is by introducing speculation in conjunction with guards to ascertain the validity of assumptions made in the speculative computation. Unfortunately, a single guard violation during the execution of \"offloaded\" code may result in a huge performance penalty and necessitate the complete re-execution of the offloaded computation. In the case of dynamic languages, this problem is compounded by the fact that a full compiler analysis is not always possible ahead of time.\nThis paper presents MegaGuards, a new approach for speculatively executing dynamic languages on heterogeneous platforms in a fully automatic and transparent manner. Our method translates each target loop into a single static region devoid of any dynamic type features. The dynamic parts are instead handled by a construct that we call a mega guard which checks all the speculative assumptions ahead of its corresponding static region. Notably, the advantage of MegaGuards is not limited to heterogeneous computing; because it removes guards from compute-intensive loops, the approach also improves sequential performance.\nWe have implemented MegaGuards along with an automatic loop parallelization backend in ZipPy, a Python Virtual Machine. The results of a careful and detailed evaluation reveal very significant speedups of an order of magnitude on average with a maximum speedup of up to two orders of magnitudes when compared to the original ZipPy performance as a baseline. These results demonstrate the potential for applying heterogeneous computing to dynamic languages.", "year": 2018, "citationCount": 11, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "8645af06a0d4a44303becd27e3808e9d5e792fb8", "title": "Poio APIAn annotation framework to bridge Language Documentation and Natural Language Processing", "abstract": "After 20 years of multimedia data collection from endangered languages and consequent creation of extensive corpora with large amounts of annotated linguistic data, a new trend in Language Documentation is now observable. It can be described as a shift from data collection and qualitative language analysis to quantitative language comparison based on the data previously collected. However, the heterogeneous annotation types and formats in the corpora hinder the application of new developed computational methods in their analysis. A standardized representation is needed. Poio API, a scientific software library written in Python and based on Linguistic Annotation Framework, fulfills this need and establishes the bridge between Language Documentation and Natural Language Processing (NLP). Hence, it represents an innovative approach which will open up new options in interdisciplinary collaborative linguistic research. This paper offers a contextualization of Poio API in the framework of current linguistic and NLP research as well as a description of its development.", "year": 2012, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "881e0a20da9b6a597a608a7602d4d51330aa072d", "title": "Automated fragment formula annotation for electron ionisation, high resolution mass spectrometry: application to atmospheric measurements of halocarbons", "abstract": null, "year": 2021, "citationCount": 4, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "88689b365b7cb5eca6b83b5ad3b5d30923142124", "title": "Describing models on the web using OGC standards", "abstract": "Environmental decision support systems normally require a data processing workflow based on models to explore alternatives. The typical workflow to handle environmental modelling involves several steps covering data discovery, access, pre-processing, model execution and validation, concluded by result visualization. These time consuming steps are usually setup for a particular scenario and set of input data. Scientists normally create their models in specific languages such as R or MATLAB. A challenge is understanding the data model of the scientists and getting the data into the model. In general, scientists are also not able to make their models available as web services. To make scientific models that fuse sensor data fit better into a service-oriented architecture, a software framework called Fusion4Decision was developed. The software framework provides a standard interface to processing algorithms, the so-called Fusors. The term Fusor refers to a general fusion or processing of input data, including through a model based computation. A common fusor is the spatiotemporal interpolation of measurement data. The Fusor is written in any software code that can be integrated into a Java environment, such as MATLAB, R, Python, and C variants. This framework based on Open Geospatial Consortium (OGC) standards can make scientific models available as a web service with standardized interfaces. The OGC services used in Fusion4Decision are: (a) Sensor Observation Service (SOS) to access sensor observations with queries filtering on the phenomenon (property) and the spatial and temporal domains of the observations, and (b) Sensor Planning Service (SPS) to parameterize and task (schedule and execute) assets such as sensors, sensor platforms (e.g. satellites), models or even persons (e.g. to conduct ex-situ measurements). The OGC information models Observation & Measurement Model (O&M) and Sensor Model Language (SensorML) also play a fundamental role. The main operations of the SPS are DescribeTasking (to get the tasking parameters), GetFeasibility (to ascertain if the asset can be tasked with the given parameters) and Submit (to actually execute the task). During the execution the operations GetStatus and Cancel are available. In Fusion4Decision we apply the SPS to models and the model result(s) become new observations for a SOS, i.e. the model is considered to be sensor and its meta-data is described in OGC SensorML. The SPS operations are functionally richer than those of the Web Processing Service (WPS) that is also often used to wrap processing modules as a web service. The formal description of the input and output arguments of the models in a language suitable both for scientists and client software is essential. The model description is encoded as a JSON object and consists of fields for the model name, a human readable descriptive text as well as formal descriptions of the inputs and outputs. The inputs and outputs allow for arrays of the basic variable types scalar, string, time, URL and file. Their description includes a) units of scalars, b) default, minimum and maximum values of scalars and optionally c) an annotation as a URI linking to an authoritative definition in an ontology. This covers the requirements of typical scientific models and also encourages the inclusion of comprehensive meta-data needed to convey full understanding of the model algorithm and its limitations. The JSON description of a model can be automatically translated into SensorML for use by the OGC services SOS and SPS. The increasing proliferation of sources of geospatial data on the web as well as models to process the data and derive new information underlines the need for a standardised framework to better link data, models and their results. Standards of the Open Geospatial Consortium can be used to integrate data access and models into web services, thus being a step towards the Model Web in which scientists and decision makers can work together effectively. The paper proposes a simple way of describing the input and output arguments of a model using JSON. This JSON description can be readily understood and generated by model providers and also translated into the sensor description language SensorML. The latter is the basis for applying the sensor concept in the OGC standards SOS and SPS to models (\u201cmodel as a sensor\u201d). This approach bridges the gap between scientists and IT specialists.", "year": 2013, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "891c62e3e73f67d0c52901b7c75201d81fb4bf27", "title": "Insane in the vembrane: filtering and transforming VCF/BCF files", "abstract": "Data from sequencing of DNA or RNA samples is routinely scanned for variation. Such variation data is stored in the standardized VCF/BCF format with additional annotations. Analyses of variants usually involve steps where filters are applied to narrow down the list of candidates for further analysis. A number of tools for this task exist, differing in functionality, speed, syntax and supported annotations. Thus, users have to switch between tools depending on the filtering task, and have to adapt to the respective filtering syntax. We present vembrane as a command line VCF/BCF filtering tool that consolidates and extends the filtering functionality of previous software to meet any imaginable filtering use case. To this end, vembrane exposes the VCF/BCF file type specification and its inofficial extensions by the annotation tools VEP and SnpEff as Python data structures. vembrane filter enables filtration by arbitrary Python expressions over (combinations of) annotations, requiring only basic knowledge of the Python programming language. vembrane table allows users to generate tables from subsets of annotations or functions thereof. Finally, it is fast, thanks to pysam, a Python wrapper around htslib, and by relying on Python\u2019s lazy evaluation. Availability and Implementation Source code and installation instructions are available at github.com/vembrane/vembrane, DOI: 10.5281/zen-odo.7003981.", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "8927f4e3da20bd47f27bec9e505e4b6aeeb5d5d6", "title": "How Well Static Type Checkers Work with Gradual Typing? A Case Study on Python", "abstract": "Python has become increasingly popular and widely used in many fields. Dynamic features of Python provide much convenience for developers. However, they can also cause many type-related bugs undetected until runtime, which increases the cost of maintenance. Static type checking is essential to find bugs early, and the introduction of gradual typing and type annotations makes it easier to perform static type analysis. However, it remains to be investigated how well gradual typing improves real bug detection. Therefore, we conducted a comprehensive study on three widely used checkers: MyPy, PyRight, and PyType. We used a benchmark containing 10 popular Python projects with 40 real type-related bugs. First, we performed static type checking on the projects with and without type annotations to evaluate the effectiveness of finding real bugs. Second, we manually analyzed the missing bugs and investigated the reasons. The results show that the three tools can detect 29 of the 40 studied bugs after annotating, while only 14 bugs are detected before annotating. We also found that type annotations can substantially improve the ability of static type checkers to detect real bugs. A detailed analysis of bugs missed by the checkers shows that: (i) the accuracy of type analysis is challenged when it comes to programs with complicated dynamic features, such as dynamically changing object\u2019s attributes, even with annotations; (ii) the inaccurate type annotations can undermine the ability of static type checkers to detect real bugs; (iii) static type checkers have different checking strategies in some cases, which has an impact on real bug detection. Our study can not only enable developers to better understand static type checking and make better use of them but also guide future research.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "894b4768b116913e19506b3f5f3542def3c5b001", "title": "SEGUID v2: Extending SEGUID checksums for circular, linear, single- and double-stranded biological sequences", "abstract": "Background Synthetic biology involves combining different DNA fragments, each containing functional biological parts, to address specific problems. Fundamental gene-function research often requires cloning and propagating DNA fragments, such as those from the iGEM Parts Registry or Addgene, typically distributed as circular plasmids. Addgene\u2019s repository alone offers over 100,000 plasmids. To ensure data integrity, cryptographic checksums can be calculated for the sequences. Each sequence has a unique checksum, making checksums useful for validation and quick lookups of associated annotations. For example, the SEGUID checksum, uniquely identifies protein sequences with a 27-character string. Objectives The original SEGUID, while effective for protein sequences and single-stranded DNA (ssDNA), is not suitable for circular and double-stranded DNA (dsDNA) due to topological differences. Challenges include how to uniquely represent linear dsDNA, circular ssDNA, and circular dsDNA. To meet these needs, we propose SEGUID v2, which extends the original SEGUID to handle additional types of sequences. Conclusions SEGUID v2 produces strand and rotation invariant checksums for single-stranded, double-stranded, possibly staggered, linear, and circular DNA and RNA sequences. Customizable alpha-bets allows for other types of sequences. In contrast to the original SEGUID, which uses Base64, SEGUID v2 uses Base64url to encode the SHA-1 hash. This ensures SEGUID v2 checksums can be used as-is in filenames, regardless of platform, and in URLs, with minimal friction. Availability SEGUID v2 is readily available for major programming languages distributed under the MIT license. JavaScript package seguid is available on NPM, Python package seguid on PyPi, and R package seguid on CRAN.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "896d5b782e4e09dd0d415b9d0eb59eb006ba2c89", "title": "Machine-Learning Classification Suggests That Many Alphaproteobacterial Prophages May Instead Be Gene Transfer Agents", "abstract": "Many of the sequenced bacterial and archaeal genomes encode regions of viral provenance. Yet, not all of these regions encode bona fide viruses. Gene transfer agents (GTAs) are thought to be former viruses that are now maintained in genomes of some bacteria and archaea and are hypothesized to enable exchange of DNA within bacterial populations. In Alphaproteobacteria, genes homologous to the \u2018head-tail\u2019 gene cluster that encodes structural components of the Rhodobacter capsulatus GTA (RcGTA) are found in many taxa, even if they are only distantly related to Rhodobacter capsulatus. Yet, in most genomes available in GenBank RcGTA-like genes have annotations of typical viral proteins, and therefore are not easily distinguished from their viral homologs without additional analyses. Here, we report a \u2018support vector machine\u2019 classifier that quickly and accurately distinguishes RcGTA-like genes from their viral homologs by capturing the differences in the amino acid composition of the encoded proteins. Our open-source classifier is implemented in Python and can be used to scan homologs of the RcGTA genes in newly sequenced genomes. The classifier can also be trained to identify other types of GTAs, or even to detect other elements of viral ancestry. Using the classifier trained on a manually curated set of homologous viruses and GTAs, we detected RcGTA-like \u2018head-tail\u2019 gene clusters in 57.5% of the 1,423 examined alphaproteobacterial genomes. We also demonstrated that more than half of the in silico prophage predictions are instead likely to be GTAs, suggesting that in many alphaproteobacterial genomes the RcGTA-like elements remain unrecognized. Data deposition Sequence alignments and phylogenetic trees are available in a FigShare repository at DOI 10.6084/m9.figshare.8796419. The Python source code of the described classifier and additional scripts used in the analyses are available via a GitHub repository at https://github.com/ecg-lab/GTA-Hunter-v1", "year": 2019, "citationCount": 21, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "8a9846283e556406b0e78b203c087c4821071137", "title": "Mass2SMILES: deep learning based fast prediction of structures and functional groups directly from high-resolution MS/MS spectra", "abstract": "Modern mass spectrometry-based metabolomics generates vast amounts of mass spectral data as part of the chemical inventory of biospecimens. Annotation of the resulting MS/MS spectra remains a challenging task that mostly relies on database interrogations, in silico prediction and interpretation of diagnostic fragmentation schemes and/or expert knowledge-based manual interpretations. A key limitation is additionally that these approaches typically leave a vast proportion of the (bio)chemical space unannotated. Here we report a deep neural network method to predict chemical structures solely from high-resolution MS/MS spectra. This novel approach initially relies on the encoding of SMILES strings from chemical structures using a continuous chemical descriptor space that had been previously implemented for molecule design. The deep neural network was trained on 83,358 natural product-derived MS/MS spectra of the GNPS library and of the NIST HRMS database with addition of the calculated neutral losses for those spectra. After this training and parameter optimization phase, the deep neural network approach was then used to predict structures from MS/MS spectra not included in the training data-set. Our current version, implemented in the Python programming language, accurately predicted 7 structures from 744 validation structures and the following 14 structures had a Tanimoto similarity score above 0.9 when compared to the true structure. It was also able to correctly identify two structures from the CASMI 2022 international contest. On average the Tanimoto similarity is of 0.40 for data of the CASMI 2022 international contest and of 0.39 for the validation data-set. Finally, our deep neural network is also able to predict the number of 60 functional groups as well as the molecular formula of chemical structures and adduct type for the analyzed MS/MS spectra. Importantly, this deep neural network approach is extremely fast, in comparison to currently available methods, making it suitable to predict on regular computers structures for all substances within large metabolomics datasets.", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Biology"]}
{"paperId": "8c30b4ef53fcd8e381201097346c7867fe6a736f", "title": "Static analysis driven enhancements for comprehension in machine learning notebooks", "abstract": null, "year": 2023, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "8e7f3c4b73a9cf2fe933b528e012ce5081533884", "title": "DeepLontar dataset for handwritten Balinese character detection and syllable recognition on Lontar manuscript", "abstract": null, "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "8ea7972b8148b532890fcb4b85d4d69c9228c11c", "title": "3-dimensional electron microscopic imaging of the zebrafish olfactory bulb and dense reconstruction of neurons", "abstract": null, "year": 2016, "citationCount": 34, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "8fdc16c0ab39e0a655b2488eced120e619d783af", "title": "Reproducible Humanities Research: Developing Extensible Databases for Recording \"Messy\" Categorisation, Annotation and Provenance Data", "abstract": "Although the Digital Humanities is fundamentally interdisciplinary in nature, all humanities research questions require a degree of interdisciplinary thinking. History, for example, draws upon most other social sciences and humanities for obtaining and analysing source materials in different contexts. The multi-modal nature of these sources, the mixing of methodologies into bespoke, project-specific frameworks and the collaboration of researchers with overlapping but distinct interpretations all require a flexible workspace. Moreover, growing calls for open research methods put pressure on humanities researchers to rethink how they document the provenance of their source materials as well as their interpretations. Individual scholars often develop extensive, single-use taxonomies to categorise, encode and describe their conclusions; stored in a variety of document, spreadsheet and database systems, these are rarely disseminated and remain offline penumbra of the research process. Moreover, the prescriptive nature of out-of-the-box software may constrain the annotation process. Larger collaborations may spend significant time developing extensive coding criteria resulting in over-fitted schema with little reusability or reach despite often herculean efforts of dissemination. Even when reusable, these schema may require a degree of familiarity with the bespoke systems that makes them inaccessible to those outside the project. In order to overcome these difficulties, we have developed a highly extensible database development interface, Nisaba. Rather than prescribe a new database structure or encoding format, Nisaba was developed in order to accommodate a wide variety of source materials, encoding schema and dissemination formats. To achieve this, Nisaba leverages World Wide Consortium (W3C) standards and Linked Data publishing practices, which encourage the explicit provision and reuse of vocabulary terms. Written in Python 3.6 using TKinter, a cross-platform graphical user interface (Linux, OS, Windows), Nisaba functions as both an input and retrieval mechanism. Users input data including text transcriptions, images and audio/visual files and apply user-created 1 For more on the importance of evidence-interpretation-argumentation provenance, see D. M. Godden, \u201cArguing at Cross-Purposes: Discharging the Dialectical Obligations of the Coalescent Model of Argumentation\u201d, Argumentation, 17:2 (2013): 219\u201343. 2 Noam Chomsky, Language and Mind (Cambridge: Cambridge University Press, 2006): 19. 3 The open-source code is currently available at http://purl.org/nisaba 4 See https://www.w3.org/TR/?tag=data 5 Tom Heath and Christian Bizer, Linked Data: Evolving the Web into a Global Data Space (1st edition). Synthesis Lectures on the Semantic Web: Theory and Technology, 1:1 (Morgan & Claypool: 2011). controlled-vocabularies, free-text annotations and an extensible selection of metadata. Once inputted, users create a segment (a selection of words, pixels or seconds of audio-visual information) and apply further metadata or annotations, allowing a single item to have multiple overlapping annotations using different schema by different users. In order to facilitate the documentation and exportation of data that is restricted or within copyright, the database encodes these segments by word number (text), or relative position (image), allowing precise locators without necessarily exporting the original materials. All data inputs are time-stamped and attached to individual user records, allowing for multiple researchers to annotate the same segments while maintaining unambiguous lines of provenance and allowing longitudinal use of the databases by multiple projects. Once inputted, the material can be retrieved through a simple browsing mechanism (controlled vocabulary) or by exporting layers of the data to nonproprietary formats, currently JSON or Turtle (RDF), allowing for deeply humanistic forms of knowledge representation in a format suitable for computational analysis. Figure 1:Text Segmentation 6 See Dominic Oldman, Martin Doerr and Stefan Gradmann, \"Zen and the Art of Linked Data: New Strategies for a Semantic Web of Humanist Knowledge\" in Schreibmann, Siemens and Unsworth, eds. A New Companion to Digital Humanities (Oxford: Wiley-Blackwell, 2016): 251\u201373; David M. Berry and Anders Fagerjord, Digital Humanities: Knowledge and Critique in a Digital Age (Cambridge: Polity, 2017): 77. Figure 2: Browsing notes This paper will demonstrate the use of Nisaba for various project types and provide guidance on how to develop an open, highly documented dataset to accompany humanities research.", "year": 2020, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "924bf2a33fd04040b13b4913be73c220a59e98b5", "title": "Taming type annotations in gradual typing", "abstract": "Gradual typing provides a methodology to integrate static and dynamic typing, harmonizing their often conflicting advantages in a single language. When a user wants to enjoy the advantages of static typing, most gradual languages require that they add type annotations. Many nontrivial tasks must be undertaken while adding type annotations, including understanding program behaviors and invariants. Unfortunately, if this is done incorrectly then the added type annotations can be wrong\u2013leading to inconsistencies between the program and the type annotations. Gradual typing implementations detect such inconsistencies at runtime, raise cast errors, and generate messages. However, solely relying on such error messages for understanding and fixing inconsistencies and their resulting cast errors is often insufficient for multiple reasons. One reason is that while such messages cover inconsistencies in one execution path, fixing them often requires reconciling information from multiple paths. Another is that users may add many wrong type annotations that they later find difficult to identify and fix, when considering all added annotations. Recent studies provide evidence that type annotations added during program migration are often wrong and that many programmers prefer compile-time warnings about wrong annotations. Motivated by these results, we develop exploratory typing to help with the static detection, understanding, and fixing of inconsistencies. The key idea of exploratory typing is that it systematically removes dynamic types and explores alternative types for static type annotations that can remedy inconsistencies. To demonstrate the feasibility of exploratory typing, we have implemented it in PyHound, which targets programs written in Reticulated Python, a gradual variant of Python. We have evaluated PyHound on a set of Python programs, and the evaluation results demonstrate that our idea can effectively detect inconsistencies in 98% of the tested programs and fix 93% of inconsistencies, significantly outperforming pytype, a widely used Python tool for enforcing type annotations.", "year": 2020, "citationCount": 6, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "92b4ea53b4ed3b832628fa5df4d5e49f0434d679", "title": "Fast clustering and cell-type annotation of scATAC data using pre-trained embeddings", "abstract": "Motivation Data from the single-cell assay for transposase-accessible chromatin using sequencing (scATAC-seq) is now widely available. One major computational challenge is dealing with high dimensionality and inherent sparsity, which is typically addressed by producing lower-dimensional representations of single cells for downstream clustering tasks. Current approaches produce such individual cell embeddings directly through a one-step learning process. Here, we propose an alternative approach by building embedding models pre-trained on reference data. We argue that this provides a more flexible analysis workflow that also has computational performance advantages through transfer learning. Results We implemented our approach in scEmbed, an unsupervised machine learning framework that learns low-dimensional embeddings of genomic regulatory regions to represent and analyze scATAC-seq data. scEmbed performs well in terms of clustering ability and has the key advantage of learning patterns of region co-occurrence that can be transferred to other, unseen datasets. Moreover, pre-trained models on reference data can be exploited to build fast and accurate cell-type annotation systems without the need for other data modalities. scEmbed is implemented in Python and it is available to download from GitHub. We also make our pre-trained models available on huggingface for public use. Availability scEmbed is open source and available at https://github.com/databio/geniml. Pre-trained models from this work can be obtained on huggingface: https://huggingface.co/databio.", "year": 2024, "citationCount": 3, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "92cbdcfb77adbe5a5f1ef18dfdb6e785344b2cc0", "title": "From Text-based Genome, Population Variations, and Transcriptome Datafiles to SQLite Database and Web Application: A Bioinformatical Study on Alfalfa", "abstract": "In this study, a web database application with the Flask framework was developed to implement three types of queries and visualize the results over a bioinformatical dataset from Alfalfa (Medicago sativa). A backend SQLite database was constructed from genome FASTA, population variations, transcriptome, and annotation files with extensions \u201c.fasta\u201d, \u201c.gff\u201d, \u201cvcf\u201d, \u201c.annotate\u201d, etc. Further, a supplementary command-line-based Java application was also developed for faster access to the database without direct SQL programming. Overall, Python, Java, and HTML were the main programming languages used in this application. Those scripts and the development procedures are valuable for bioinformaticians to build online databases from similar raw datasets of other species.", "year": 2021, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "93107809381d75143ec3bac1ed1339ed78740c92", "title": "Type Prediction With Program Decomposition and Fill-in-the-Type Training", "abstract": "TypeScript and Python are two programming languages that support optional type annotations, which are useful but tedious to introduce and maintain. This has motivated automated type prediction: given an untyped program, produce a well-typed output program. Large language models (LLMs) are promising for type prediction, but there are challenges: fill-in-the-middle performs poorly, programs may not fit into the context window, generated types may not type check, and it is difficult to measure how well-typed the output program is. We address these challenges by building OpenTau, a search-based approach for type prediction that leverages large language models. We propose a new metric for type prediction quality, give a tree-based program decomposition that searches a space of generated types, and present fill-in-the-type fine-tuning for LLMs. We evaluate our work with a new dataset for TypeScript type prediction, and show that 47.4% of files type check (14.5% absolute improvement) with an overall rate of 3.3 type errors per file. All code, data, and models are available at: https://github.com/GammaTauAI/opentau.", "year": 2023, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "941e9f91f9dfb29404284db2d51d7a26fbceb5e8", "title": "Statically typed string sanitation inside a python", "abstract": "Web applications must ultimately command systems like web browsers and database engines using strings. Strings derived from improperly sanitized user input can as a result be a vector for command injection attacks. In this paper, we introduce regular string types, which classify strings constrained statically to be in a regular language specified by a regular expression. Regular strings support standard string operations like concatenation and substitution, as well as safe coercions, so they can be used to implement, in an essentially conventional manner, the pieces of a web application or framework that handle strings arising from user input. Simple type annotations at function interfaces can be used to statically verify that sanitization has been performed correctly without introducing redundant run-time checks. We specify this type system first as a minimal typed lambda calculus, lambdaRS. To be practical, adopting a specialized type system like this should not require the adoption of a new programming language. Instead, we advocate for extensible type systems: new type system fragments like this should be implemented as libraries atop a mechanism that guarantees that they can be safely composed. We support this with two contributions. First, we specify a translation from lambdaRS to a calculus with only standard strings and regular expressions. Then, taking Python as a language with these constructs, we implement the type system together with the translation as a library using typy, an extensible static type system for Python.", "year": 2014, "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Psychology"]}
{"paperId": "95d2d9cb0750a03833f1ed8bfb8cec07c8e5469e", "title": "SWAAT Bioinformatics Workflow for Protein Structure-Based Annotation of ADME Gene Variants", "abstract": "Recent genomic studies have revealed the critical impact of genetic diversity within small population groups in determining the way individuals respond to drugs. One of the biggest challenges is to accurately predict the effect of single nucleotide variants and to get the relevant information that allows for a better functional interpretation of genetic data. Different conformational scenarios upon the changing in amino acid sequences of pharmacologically important proteins might impact their stability and plasticity, which in turn might alter the interaction with the drug. Current sequence-based annotation methods have limited power to access this type of information. Motivated by these calls, we have developed the Structural Workflow for Annotating ADME Targets (SWAAT) that allows for the prediction of the variant effect based on structural properties. SWAAT annotates a panel of 36 ADME genes including 22 out of the 23 clinically important members identified by the PharmVar consortium. The workflow consists of a set of Python codes of which the execution is managed within Nextflow to annotate coding variants based on 37 criteria. SWAAT also includes an auxiliary workflow allowing a versatile use for genes other than ADME members. Our tool also includes a machine learning random forest binary classifier that showed an accuracy of 73%. Moreover, SWAAT outperformed six commonly used sequence-based variant prediction tools (PROVEAN, SIFT, PolyPhen-2, CADD, MetaSVM, and FATHMM) in terms of sensitivity and has comparable specificity. SWAAT is available as an open-source tool.", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "95eaff33497595509baa7a637989178b5575758b", "title": "SeismiQB - a novel framework for deep learning with seismic data", "abstract": "In recent years, Deep Neural Networks were successfully adopted in numerous domains to solve various image-related tasks, ranging from simple classification to fine borders annotation. Naturally, many researches proposed to use it to solve geological problems. Unfortunately, many of the seismic processing tools were developed years before the era of machine learning, including the most popular SEG-Y data format for storing seismic cubes. Its slow loading speed heavily hampers experimentation speed, which is essential for getting acceptable results. Worse yet, there is no widely-used format for storing surfaces inside the volume (for example, seismic horizons). To address these problems, we've developed an open-sourced Python framework with emphasis on working with neural networks, that provides convenient tools for (i) fast loading seismic cubes in multiple data formats and converting between them, (ii) generating crops of desired shape and augmenting them with various transformations, and (iii) pairing cube data with labeled horizons or other types of geobodies.", "year": 2020, "citationCount": 0, "fieldsOfStudy": ["Physics", "Computer Science"]}
{"paperId": "96a50949fb3846fabde4d99d1f97eeed61301722", "title": "scGAD: a new task and end-to-end framework for generalized cell type annotation and discovery", "abstract": "The rapid development of single-cell RNA sequencing (scRNA-seq) technology allows us to study gene expression heterogeneity at the cellular level. Cell annotation is the basis for subsequent downstream analysis in single-cell data mining. As more and more well-annotated scRNA-seq reference data become available, many automatic annotation methods have sprung up in order to simplify the cell annotation process on unlabeled target data. However, existing methods rarely explore the fine-grained semantic knowledge of novel cell types absent from the reference data, and they are usually susceptible to batch effects on the classification of seen cell types. Taking into consideration the limitations above, this paper proposes a new and practical task called generalized cell type annotation and discovery for scRNA-seq data whereby target cells are labeled with either seen cell types or cluster labels, instead of a unified 'unassigned' label. To accomplish this, we carefully design a comprehensive evaluation benchmark and propose a novel end-to-end algorithmic framework called scGAD. Specifically, scGAD first builds the intrinsic correspondences on seen and novel cell types by retrieving geometrically and semantically mutual nearest neighbors as anchor pairs. Together with the similarity affinity score, a soft anchor-based self-supervised learning module is then designed to transfer the known label information from reference data to target data and aggregate the new semantic knowledge within target data in the prediction space. To enhance the inter-type separation and intra-type compactness, we further propose a confidential prototype self-supervised learning paradigm to implicitly capture the global topological structure of cells in the embedding space. Such a bidirectional dual alignment mechanism between embedding space and prediction space can better handle batch effect and cell type shift. Extensive results on massive simulation datasets and real datasets demonstrate the superiority of scGAD over various state-of-the-art clustering and annotation methods. We also implement marker gene identification to validate the effectiveness of scGAD in clustering novel cell types and their biological significance. To the best of our knowledge, we are the first to introduce this new and practical task and propose an end-to-end algorithmic framework to solve it. Our method scGAD is implemented in Python using the Pytorch machine-learning library, and it is freely available at https://github.com/aimeeyaoyao/scGAD.", "year": 2023, "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "979d5677b001b5d253241ce437cf31a1d734ff39", "title": "MEMOTE for standardized genome-scale metabolic model testing", "abstract": null, "year": 2020, "citationCount": 309, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "99a76876b78588eca0895cd36435202f2a3c5f2c", "title": "Charge cluster occurrence in land plants' mitochondrial proteomes with functional and structural insights.", "abstract": "The Charge Clusters (CCs) are involved in key functions and are distributed according to the organism, the protein's type, and the charge of amino acids. In the present study, we have explored the occurrence, position, and annotation as a first large-scale study of the CCs in land plants mitochondrial proteomes. A new python script was used for data curation. The Finding Clusters Charge in Protein Sequences Program was performed after adjusting the reading window size. A 44316 protein sequences belonging to 52 species of land plants were analysed. The occurrence of Negative Charge Clusters (NCCs) (1.2%) is two times more frequent than the Positive Charge Clusters (PCCs) (0.64%). Moreover, 39 and 30 NCCs were conserved in 88 and 41 proteins in intra and in inter proteomes respectively, while 14 and 21 PCCs were conserved in 53 and 85 protein sequences in intra and inter proteomes consecutively. Sequences carrying mixed CCs are rare (0.12%). Despite this low abundance, CCs play a crucial role in protein function. The CCs tend to be located mainly in the terminal regions of proteins which guarantees specific protein targeting and import into the mitochondria. In addition, the functional annotation of CCs according to Gene Ontology shows that CCs are involved in binding functions of either proteins or macromolecules which are deployed in different metabolic and cellular processes such as RNA editing and transcription. This study may provide valuable information while considering the CCs in understanding the environmental adaptation of plants.Communicated by Ramaswamy H. Sarma.", "year": 2024, "citationCount": 1, "fieldsOfStudy": ["Medicine"]}
{"paperId": "9b14d84a6cd68b7fd829c884d0897fa9efa04e15", "title": "PyAnalyzer: An Effective and Practical Approach for Dependency Extraction from Python Code", "abstract": "Dependency extraction based on static analysis lays the ground-work for a wide range of applications. However, dynamic language features in Python make code behaviors obscure and nondeter-ministic; consequently, it poses huge challenges for static analyses to resolve symbol-level dependencies. Although prosperous techniques and tools are adequately available, they still lack sufficient capabilities to handle object changes, first-class citizens, varying call sites, and library dependencies. To address the fundamental difficulty for dynamic languages, this work proposes an effective and practical method namely PyAnalyzer for dependency extraction. PyAnalyzer uniformly models functions, classes, and modules into first-class heap objects, propagating the dynamic changes of these objects and class inheritance. This manner better simulates dynamic features like duck typing, object changes, and first-class citizens, resulting in high recall results without compromising pre-cision. Moreover, PyAnalyzer leverages optional type annotations as a shortcut to express varying call sites and resolve library depen-dencies on demand. We collected two micro-benchmarks (278 small programs), two macro-benchmarks (59 real-world applications), and 191 real-world projects (10MSLOC) for comprehensive comparisons with 7 advanced techniques (i.e., Understand, Sourcetrail, Depends, ENRE19, PySonar2, PyCG, and Type4Py). The results demonstrated that PyAnalyzer achieves a high recall and hence improves the F1 by 24.7% on average, at least 1.4x faster without an obvious compromise of memory efficiency. Our work will benefit diverse client applications.", "year": 2024, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "9be12c00628ff471cc2e18bedcf48a67e77a054f", "title": "On the cost of type-tag soundness", "abstract": "Gradual typing systems ensure type soundness by transforming static type annotations into run-time checks. These checks provide semantic guarantees, but may come at a large cost in performance. In particular, recent work by Takikawa et al. suggests that enforcing a conventional form of type soundness may slow a program by two orders of magnitude. Since different gradual typing systems satisfy different notions of soundness, the question then arises: what is the cost of such varying notions of soundness? This paper answers an instance of this question by applying Takikawa et al.'s evaluation method to Reticulated Python, which satisfies a notion of type-tag soundness. We find that the cost of soundness in Reticulated is at most one order of magnitude, and increases linearly with the number of type annotations.", "year": 2017, "citationCount": 25, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "9cdaac878f9f051d06d1cf78f946e979c9a55781", "title": "A Complete Descritpion of the UnPython and Jit4GPU Framework", "abstract": "A new compilation framework enables the execution of numerical-intensive applications in an execution environment that is formed by multi-core Central Processing Units (CPUs) and Graphics Processing Units (GPUs). A critical innovation is the use of a variation of Linear Memory Access Descriptors (LMADs) to analyze loop nests and determine automatically which memory locations must be transferred between the CPU address space and the GPU address space. In this programming model, the application is written in a combination of Python and NumPy, a rich numerical extension for Python. Inobstrusive light annotation is introduced to identify the type of function parameters and return values, and to indicate which loop nests should be parallelized and executed in the GPU. The new compilation system is a combination of an ahead-of-time compiler, unPython, to transform Python/NumPy code into a restricted C programming language notation, and a just-in-time compiler, jit4GPU, that converts this restricted C notation into the AMD CAL interface. The experimental evaluation using a collection of well-known benchmarks indicates that there is very significant performance advantages to execute important loops of numerical applications in GPUs.", "year": 2011, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "9e2078e7ada3d1e1f66bcd632fb3c58a82d8e30c", "title": "Static Type Analysis by Abstract Interpretation of Python Programs (Artifact)", "abstract": "Python is an increasingly popular dynamic programming language, particularly used in the scientific community and well-known for its powerful and permissive high-level syntax. Our work aims at detecting statically and automatically type errors. As these type errors are exceptions that can be caught later on, we precisely track all exceptions (raised or caught). We designed a static analysis by abstract interpretation able to infer the possible types of variables, taking into account the full control-flow. It handles both typing paradigms used in Python, nominal and structural, supports Python's object model, introspection operators allowing dynamic type testing, dynamic attribute addition, as well as exception handling. We present a flow-and context-sensitive analysis with special domains to support containers (such as lists) and infer type equalities (allowing it to express parametric polymorphism). The analysis is soundly derived by abstract interpretation from a concrete semantics of Python developed by Fromherz et al. Our analysis is designed in a modular way as a set of domains abstracting a concrete collecting semantics. It has been implemented into the MOPSA analysis framework, and leverages external type annotations from the Typeshed project to support the vast standard library. We show that it scales to benchmarks a few thousand lines long, and preliminary results show it is able to analyze a small real-life command-line utility called PathPicker. Compared to previous work, it is sound, while it keeps similar efficiency and precision.", "year": 2020, "citationCount": 27, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "9e30b21ec05756775e0b7ce354f70968d76eb130", "title": "BioTEA: Containerized Methods of Analysis for Microarray-Based Transcriptomics Data", "abstract": "Tens of thousands of gene expression data sets describing a variety of model organisms in a many different pathophysiological conditions are currently stored in publicly available databases such as Gene Expression Omnibus (GEO) and ArrayExpress. As microarray technology is giving way to RNA-Seq, it becomes strategic to develop high-level tools of analysis to preserve access to this huge amount of information through the most sophisticated methods of data preparation and processing developed over the years, while ensuring at the same time the reproducibility of the results. To meet this need, here we present bioTEA, a novel software tool that combines the ease of use with the versatility and power of an R/Bioconductor-based differential expression analysis, starting from raw data retrieval and preparation to gene annotation. BioTEA is an R-coded pipeline, wrapped in a Python-based command line interface and containerized with Docker technology. The user can choose among multiple options\u2014including gene filtering, batch effect handling, sample pairing, statistical test type\u2014to adapt the algorithm flow to the structure of the particular data set. All these options are saved in single text file which can be easily shared between different laboratories to deterministically reproduce the results. In addition, a detailed log file provides accurate information about each step of analysis. Overall, these features make bioTEA an invaluable tool for both bioiformaticians and wet-lab biologists interested in transcriptomics.", "year": 2022, "citationCount": 3, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "9f5425fd6d95de795bf6280afdb9cdc54220f025", "title": "Evaluating embedded semantics for accessibility description of web crawl data", "abstract": "The Web is ever expanding, even more by the need for content consumption derived from the pandemic. This fact highlights the need for equity in access to Web content by all people, regardless of their disabilities. To this end, it is essential to focus on web accessibility issues. The World Wide Web Consortium (W3C), the leading organization responsible for ensuring the growth of the social value of the Web, establishes standards, protocols, and recommendations to improve the reach extent of web content for people. For instance, Web Content Accessibility Guidelines (WCAG) promote the achievement of web accessibility. Furthermore, other W3C recommendations foster embedded semantic into the web content to help browsers build a machine-readable data structure aiming to produce an enriched description in search results supporting people to find the right content for their queries and, consequently, improving user experience. Searching for specific web content is especially striving for people with disabilities because they could be forced to explore many search results before finding some content that matches their accessibility requirements. If embedded semantic communicate the accessibility properties of the content, the search will be more productive for everyone but even more for people with special needs. For embedded semantic, two components are required, a vocabulary and an encoding format. Schema.org vocabulary has experienced high growth and encompasses plenty of descriptors for each type of web information, including the set of descriptors for accessibility conditions information. Regarding the format, JSON-LD is the latest W3C recommendation for encoding due to its ability to make JSON data interoperate at Web-scale. It provides a quickly transforming for Linked Data format and is simple enough to be read and written by people. This research conducts a quantitative analysis of the embedded semantic into the web content by processing a dataset obtained from millions of web crawl data for 2021. The data arrive from distinct provenance and purposes at a global scale. In this web content, each annotation is made through script JSON-LD of embedded semantic with Schema's vocabulary. The analysis defines how the accessibility descriptors are used in conjunction with other classes and properties to describe the web information on personal blogs, organizations, events, educational content, universities, persons, commerce, sports, medicine, entertainment, and more. The results provide a perspective of the awareness for accessibility in the different purposes of the Web.The processing was performed on collected zip files that contain over three hundred million records. This analysis was conducted using massive data analysis techniques such as key-value modeling with Python for processing and a NoSQL database such as MongoDB for storage. A new dataset with normalized data was generated with information about domains, types of web content, and properties associated with the accessibility descriptor. The collection and storage layers were implemented on a computing platform with 30GB of RAM, 10 CPUs, and 2TB of storage.This research delivers two main contributions. Firstly, the analysis of the interest in the Web for using accessibility descriptors in embedded semantic. The quantitative results enable us to appreciate the concern about equity and inclusion made visible through accessibility issues in different entities, according to the web domains. Moreover, these results reveal how the W3C recommendation of embedded semantic is being adopted to create a more organized and better-documented Web. Second, processing the raw dataset result in a new normalized dataset in JSON format with information about domains, web content types, and properties associated with the accessibility descriptor. This new dataset will be available for further analysis of the embedded semantic.", "year": 2023, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "a429c6c8407b10a89c9610ab5e9682ec89772356", "title": "TypeWriter: neural type prediction with search-based validation", "abstract": "Maintaining large code bases written in dynamically typed languages, such as JavaScript or Python, can be challenging due to the absence of type annotations: simple data compatibility errors proliferate, IDE support is limited, and APIs are hard to comprehend. Recent work attempts to address those issues through either static type inference or probabilistic type prediction. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents TypeWriter, the first combination of probabilistic type prediction with search-based refinement of predicted types. TypeWriter\u2019s predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, TypeWriter invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the TypeWriter approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 1,137 popular open-source projects. We show that TypeWriter\u2019s type predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5) predictions for return types, and 0.57 (0.80) for argument types, which clearly outperforms prior type prediction models. By combining predictions with search-based validation, TypeWriter can fully annotate between 14% to 44% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that TypeWriter adds many more non-trivial types. TypeWriter currently suggests types to developers at Facebook and several thousands of types have already been accepted with minimal changes.", "year": 2019, "citationCount": 102, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "aa2f4c6bc7805ec43fb7b307043d360262eacea1", "title": "Multi-Modal Dataset Creation for Federated~Learning with DICOM Structured Reports", "abstract": "Purpose: Federated training is often hindered by heterogeneous datasets due to divergent data storage options, inconsistent naming schemes, varied annotation procedures, and disparities in label quality. This is particularly evident in the emerging multi-modal learning paradigms, where dataset harmonization including a uniform data representation and filtering options are of paramount importance. Methods: DICOM structured reports enable the standardized linkage of arbitrary information beyond the imaging domain and can be used within Python deep learning pipelines with highdicom. Building on this, we developed an open platform for data integration and interactive filtering capabilities that simplifies the process of assembling multi-modal datasets. Results: In this study, we extend our prior work by showing its applicability to more and divergent data types, as well as streamlining datasets for federated training within an established consortium of eight university hospitals in Germany. We prove its concurrent filtering ability by creating harmonized multi-modal datasets across all locations for predicting the outcome after minimally invasive heart valve replacement. The data includes DICOM data (i.e. computed tomography images, electrocardiography scans) as well as annotations (i.e. calcification segmentations, pointsets and pacemaker dependency), and metadata (i.e. prosthesis and diagnoses). Conclusion: Structured reports bridge the traditional gap between imaging systems and information systems. Utilizing the inherent DICOM reference system arbitrary data types can be queried concurrently to create meaningful cohorts for clinical studies. The graphical interface as well as example structured report templates will be made publicly available.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "ae0139736bd11d3a36a49b726fa6e40b2930b211", "title": "A Comprehensive WebScraping of IMDb\u2019s Top 50 Movies using Beautiful Soup", "abstract": "Data encompasses all factual and measurable information that is accessible, quantifiable, or recordable. This information can take on a variety of forms, including numerical figures, written content, visual depictions, or symbolic annotations. The primary goal behind this undertaking is to extract such data from the official website of the Internet Movie Database, IMDb\u2019s top 50 movies of all time using the BeautifulSoup package available in the Python package library which can be used for HTML parsing to create a parse tree for parsed pages. These parsed pages can in turn be utilised to extract data from the HTML code snippets used to code the official website\u2019s data aiding in further analysis and research studies. For the main objective here, we have used IMDb\u2019s official website as the foundation, with the language Python serving as a programming link to scrape data using the built-in functions in the Python package library such as Numpy, Pandas, Requests and BeautifulSoup.In this paper, we create an accurate Data Frame using a Python package called Pandas, this Data Frame can enable users to search for any required attribute from the variety of attributes available in the data frame such as release year, meta scores of each film, the type of genre they belong to and parental guidance suggestion. The objective of the paper mainly focuses on easing the user\u2019s search process by providing them with different attributes they can select from. The example website utilisation presented herein serves as a fundamental illustration of our research efforts. Extrapolating this methodology across various websites spanning multiple domains opens up an immense amount of opportunities for insightful analysis and strategic data utilization, considering the transformative potential of employing such a thorough strategy across a range of digital platforms.", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "b2052463bf09968804f41547cecd866ca9b2e3d8", "title": "alona: a web server for single-cell RNA-seq analysis", "abstract": "Abstract Summary Single-cell RNA sequencing (scRNA-seq) is a technology to measure gene expression in single cells. It has enabled discovery of new cell types and established cell type atlases of tissues and organs. The widespread adoption of scRNA-seq has created a need for user-friendly software for data analysis. We have developed a web server, alona that incorporates several of the most popular single-cell analysis algorithms into a flexible pipeline. alona can perform quality filtering, normalization, batch correction, clustering, cell type annotation and differential gene expression analysis. Data are visualized in the web browser using an interface based on JavaScript, allowing the user to query genes of interest and visualize the cluster structure. alona accepts a compressed gene expression matrix and identifies cell clusters with a graph-based clustering strategy. Cell types are identified from a comprehensive collection of marker genes or by specifying a custom set of marker genes. Availability and implementation The service runs at https://alona.panglaodb.se and the Python package can be downloaded from https://oscar-franzen.github.io/adobo/. Supplementary information Supplementary data are available at Bioinformatics online.", "year": 2020, "citationCount": 32, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "b41462ba6251c4be3956143c33eea5d5e6f926fb", "title": "Exosomal circRNA in Digestive System Tumors: The Main Player or Coadjuvants?", "abstract": "Exosomes are a type of extracellular microvesicles with a diameter of 40\u2013160 nm. Circular RNA (circRNA) is a type of closed circular RNA molecule that is highly conserved in evolution. Exosomal circRNA plays a vital role in the proliferation, invasion, migration, and drug resistance of digestive system tumors. In this study, we used The Cancer Genome Atlas (TCGA) database, UALCAN, Python crawler, miRTargetLink Human, Database for Annotation, Visualization, and Integrated Discovery (DAVID), micBioinformatic online tool, and Cytoscape software (3.7.1). The results showed that circ-RanGAP1 in gastric cancer, circUHRF1 in hepatocellular carcinoma, and circFMN2 in colorectal cancer regulate the malignant behavior of tumors and affect the expression of their host gene through sponging miR-877-3p, miR-449c-5p, and miR-1182, respectively. Twenty exosomal circRNAs regulate 6,570 target genes through sponging 23 miRNAs. Firstly, 270 of those target genes are regulated by two or more miRNAs, which are highly correlated with 83 tumor-related pathways and six Kyoto Encyclopedia of Genes and Genomes pathways. Secondly, 1,146 target genes were significantly differentially expressed in corresponding digestive system tumors, and functional enrichment analysis revealed that 78 of those were involved in 20 cancer-related pathways. In short, the bioinformatics analysis showed that these exosomal circRNAs are stably expressed in body fluids, and regulate the occurrence and development of gastric cancer, hepatocellular carcinoma, colorectal cancer, and other digestive system tumors through sponging miRNAs. Exosomal circRNAs may be used as biomarkers for the diagnosis of disease and identification of effective therapeutic targets in the future, as well as improve the prognosis of patients with digestive system tumors.", "year": 2021, "citationCount": 10, "fieldsOfStudy": ["Medicine"]}
{"paperId": "b6011964c28c9962ab64e79fe15d4f1a94ad09d7", "title": "ScanExitronLR: characterization and quantification of exitron splicing events in long-read RNA-seq data", "abstract": "Summary Exitron splicing is a type of alternative splicing where coding sequences are spliced out. Recently, exitron splicing has been shown to increase proteome plasticity and play a role in cancer. Long-read RNA-seq is well suited for quantification and discovery of alternative splicing events; however, there are currently no tools available for detection and annotation of exitrons in long-read RNA-seq data. Here we present ScanExitronLR, an application for the characterization and quantification of exitron splicing events in long-reads. From a BAM alignment file, reference genome and reference gene annotation, ScanExitronLR outputs exitron events at the transcript level. Outputs of ScanExitronLR can be used in downstream analyses of differential exitron splicing. A companion tool, AnnotateExitron, reports exitron annotations such as truncation or frameshift type, nonsense-mediated decay status, and Pfam domain interruptions. We demonstrate that ScanExitronLR performs better on noisy long-reads than currently published exitron detection algorithms designed for short-read data. Availability and Implementation ScanExitronLR is freely available at https://github.com/ylab-hi/ScanExitronLR and distributed as a pip package on the Python Package Index. Contact yang4414@umn.edu Supplementary Information Supplementary data are available at Bioinformatics online.", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"]}
{"paperId": "b6c960e92d24fdc1f2e19151278c94957388aaeb", "title": "Type4Py: Deep Similarity Learning-Based Type Inference for Python", "abstract": "\u2014Dynamic languages, such as Python and Javascript, trade static typing for developer \ufb02exibility. While this allegedly enables greater productivity, lack of static typing can cause run-time exceptions, type inconsistencies, and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retro\ufb01tting types to existing codebases is error-prone and laborious, learning-based approaches have been proposed to enable automatic type annotations based on existing, partially annotated codebases. However, the prediction of rare and user-de\ufb01ned types is still challenging. In this paper, we present Type4Py, a deep similarity learning-based type inference model for Python. We design a hierarchical neural network model that learns to discriminate between types of the same kind and dissimilar types in a high-dimensional space, which results in clusters of types. Nearest neighbor search suggests likely type signatures of given Python functions. The types visible to analyzed modules are surfaced using lightweight dependency analysis. The results of quantitative and qualitative evaluation indicate that Type4Py signi\ufb01cantly outperforms state-of-the-art approaches at the type prediction task. Considering the Top-1 prediction, Type4Py obtains 19.33% and 13.49% higher precision than Typilus and TypeWriter, respectively, while utilizing a much bigger vocabulary.", "year": 2021, "citationCount": 14, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "b8e711b9b12612fa922163c6ec2935b966edb454", "title": "Fast Lexical-Semantic Analysis of Syntactic n-grams in Myria CSE 544 Final Project", "abstract": "The Google syntactic n-grams dataset is a linguistic resource containing billions of detailed annotations on short strings of text and the relationships between words within them. The dataset is available for download as a large (hundreds of GBs) set of flat files, but no open tools for working with the data have been released to date. As a result, extracting information from this data currently requires custom solutions by any user attempting to analyze it. In this project, we show that by loading this data into a distributed DBMS and using an efficient query plan, it is possible to make a wide range of general queries over it in short amounts of time. We also create a Python API for building common types of queries over the data, effectively abstracting the database itself away from the user while still allowing them to perform a wide range of queries, making it both faster and simpler to analyze and collect information about this dataset.", "year": 2015, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "b95c50af765557b6428d596ce285b5ef1e4da109", "title": "Technical report: CSVM dictionaries", "abstract": "CSVM (CSV with Metadata) is a simple file format for tabular data. The possible application domain is the same as typical spreadsheets files, but CSVM is well suited for long term storage and the inter-conversion of RAW data. CSVM embeds different levels for data, metadata and annotations in human readable format and flat ASCII files. As a proof of concept, Perl and Python toolkits were designed in order to handle CSVM data and objects in workflows. These parsers can process CSVM files independently of data types, so it is possible to use same data format and parser for a lot of scientific purposes. CSVM-1 is the first version of CSVM specification, an extension of CSVM-1 for implementing a translation system between CSVM files is presented in this paper. The necessary data used to make the translation are also coded in another CSVM file. This particular kind of CSVM is called a CSVM dictionary, it is also readable by the current CSVM parser and it is fully supported by the Python toolkit. This report presents a proposal for CSVM dictionaries, a working example in chemistry, and some elements of Python toolkit usable to handle these files.", "year": 2012, "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Biology"]}
{"paperId": "ba77a003796d2f89317736d62a8812da4273cd79", "title": "AS-Quant: Detection and Visualization of Alternative Splicing Events with RNA-seq Data", "abstract": "A simplistic understanding of the central dogma falls short in correlating the number of genes in the genome to the number of proteins in the proteome. Post-transcriptional alternative splicing contributes to the complexity of proteome and are critical in understanding gene expression. mRNA-sequencing (RNA-seq) has been widely used to study the transcriptome and provides opportunity to detect alternative splicing events among different biological conditions. Despite the popularity of studying transcriptome variants with RNA-seq, few efficient and user-friendly bioinformatics tools have been developed for the genome-wide detection and visualization of alternative splicing events. We have developed AS-Quant (Alternative Splicing Quantitation), a robust program to identify alternative splicing events and visualize the short-read coverage with gene annotations. AS-Quant works in three steps: (i) calculate the read coverage of the potential splicing exons and the corresponding gene; (ii) categorize the splicing events into five different types based on annotation, and assess the significance of the events between two biological conditions; (iii) generate the short reads coverage plot with a complete gene annotation for user specified splicing events. To evaluate the performance, two significant alternative splicing events identified by AS-Quant between two biological contexts were validated by RT-PCR. Implementation AS-Quant is implemented in Python. Source code and a comprehensive user\u2019s manual are freely available at https://github.com/CompbioLabUCF/AS-Quant", "year": 2020, "citationCount": 8, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "bd43d44eec45e922303f94fa19d79b859c72b17c", "title": "Bio.Phylo: A unified toolkit for processing, analyzing and visualizing phylogenetic trees in Biopython", "abstract": null, "year": 2012, "citationCount": 124, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "bd4823dca61900a6a47220644d207fdc5f90f664", "title": "Scikit-talk: A toolkit for processing real-world conversational speech data", "abstract": "We present Scikit-talk, an open-source toolkit for processing collections of real-world conversational speech in Python. First of its kind, the toolkit equips those interested in studying or modeling conversations with an easy-to-use interface to build and explore large collections of transcriptions and annotations of talk-in-interaction. Designed for applications in speech processing and Conversational AI, Scikit-talk provides tools to custom-build datasets for tasks such as intent prototyping, dialog flow testing, and conversation design. Its preprocessor module comes with several pre-built interfaces for common transcription formats, which aim to make working across multiple data sources more accessible. The explorer module provides a collection of tools to explore and analyse this data type via string matching and unsupervised machine learning techniques. Scikit-talk serves as a platform to collect and connect different transcription formats and representations of talk, enabling the user to quickly build multilingual datasets of varying detail and granularity. Thus, the toolkit aims to make working with authentic conversational speech data in Python more accessible and to provide the user with comprehensive options to work with representations of talk in appropriate detail for any downstream task. For the latest updates and information on currently supported languages and language resources, please refer to: https://pypi.org/project/scikit-talk/", "year": 2021, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "be0ccf6a34f46caa4e28f72b3c15bcf39452fc47", "title": "Annot: a Django-based sample, reagent, and experiment metadata tracking system", "abstract": null, "year": 2019, "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "be7f9f690bf283d354624c9ced9e4adc913844c0", "title": "Database Creator for Protein/Peptide Mass Analysis, DC-PPMA: A novel standalone computational tool for simplifying the analysis of MS/MS data to identify protein/polypeptide sequences by di\ufb00erent proteomic approaches", "abstract": "Rationale: Proteomic studies typically involve use of di\ufb00erent types of softwares for annotating experimental tandem mass spectrometric data (MS/MS) and thereby simplify the process of peptide and protein identi\ufb01cation. For such annotations, these softwares calculate the m/z values of the peptide/protein precursor and fragment ions, for which a database of protein sequences must be provided as input \ufb01le. The calculated m/z values are stored as another database, which the user usually cannot view. \u2018Database Creator for Protein/Peptide Mass Analysis\u2019 (DC-PPMA) is a novel standalone software that can create custom databases and the user can view the custom database containing the calculated m/z values of precursor and fragment ions. Methods: Python language was used for implementation and the graphical user interface was built with Page/Tcl, making this tool more user-friendly and easier to analyze. DC-PPMA is freely available at https://vit.ac.in/PPMA/. Results: DC-PPMA contains three modules. Protein/peptide sequences as per user\u2019s choice can be entered as input to the \ufb01rst module for creating custom database. In the second module, m/z values must be queried-in, which are searched within the custom database to identify protein/peptide sequences. The third module is suited for peptide mass \ufb01ngerprinting, for which data arising from both ESI and MALDI MS can be utilized. Conclusions: Mass spectral data acquired from any proteomic approach: bottom-up, middle-down and top-down can be interrogated with DC-PPMA. A major facet of DC-PPMA is that the user can \u2018view\u2019 the custom database containing the m/z values of the precursor ions (e.g., proteolytic peptides) and the respective fragment ions (e.g., b & y ions), prior to the database search. The feature of \u2018viewing\u2019 the custom database cannot only be helpful for better understanding the search engine processes; but also, for \u2018designing multiple reaction monitoring (MRM) methods\u2019. Post-translational modi\ufb01cations and protein isoforms too can be analyzed.", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "becc2a1a45a01f81c5cbf2353d364e1a43c95896", "title": "Polyglot and Speech Corpus Tools: A System for Representing, Integrating, and Querying Speech Corpora", "abstract": "Speech datasets from many languages, styles, and sources exist in the world, representing significant potential for scientific studies of speech\u2014particularly given structural similarities among all speech datasets. However, studies using multiple speech corpora remain difficult in practice, due to corpus size, complexity, and differing formats. We introduce open-source software for unified corpus analysis: integrating speech corpora and querying across them. Corpora are stored in a custom \u2018polyglot persistence\u2019 scheme that combines three sub-databases mirroring different data types: a Neo4j graph database to represent temporal annotation graph structure, and SQL and InfluxDB databases to represent metaand acoustic data. This scheme abstracts away from the idiosyncratic formats of different speech corpora, while mirroring the structure of different data types improves speed and scalability. A Python API and a GUI both allow for: enriching the database with positional, hierarchical, temporal, and signal measures (e.g. utterance boundaries, f0) that are useful for linguistic analysis; querying the database using a simple query language; and exporting query results to standard formats for further analysis. We describe the software, summarize two case studies using it to examine effects on pitch and duration across languages, and outline planned future development.", "year": 2017, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "bf53ef5d186a7e6d5540667ef188050b841595d2", "title": "BioThings SDK: a toolkit for building high-performance data APIs in biomedical research", "abstract": "Summary To meet the increased need of making biomedical resources more accessible and reusable, Web APIs or web services have become a common way to disseminate knowledge sources. The BioThings APIs are a collection of high-performance, scalable, annotation as a service APIs that automate the integration of biological annotations from disparate data sources. This collection of APIs currently includes MyGene.info, MyVariant.info, and MyChem.info for integrating annotations on genes, variants, and chemical compounds, respectively. These APIs are used by both individual researchers and application developers to simplify the process of annotation retrieval and identifier mapping. Here, we describe the BioThings Software Development Kit (SDK), a generalizable and reusable toolkit for integrating data from multiple disparate data sources and creating high-performance APIs. This toolkit allows users to easily create their own BioThings APIs for any data type of interest to them, as well as keep APIs up-to-date with their underlying data sources. Availability and implementation The BioThings SDK is built in Python and released via PyPI (https://pypi.org/project/biothings/). Its source code is hosted at its github repository (https://github.com/biothings/biothings.api).", "year": 2021, "citationCount": 15, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"]}
{"paperId": "bf7b426c0fe087f2d31749f286cbd771060bbca3", "title": "Staphylococcus aureus viewed from the perspective of 40,000+ genomes", "abstract": "Low-cost Illumina sequencing of clinically-important bacterial pathogens has generated thousands of publicly available genomic datasets. Analyzing these genomes and extracting relevant information for each pathogen and the associated clinical phenotypes requires not only resources and bioinformatic skills but organism-specific knowledge. In light of these issues, we created Staphopia, an analysis pipeline, database and application programming interface, focused on Staphylococcus aureus, a common colonizer of humans and a major antibiotic-resistant pathogen responsible for a wide spectrum of hospital and community-associated infections. Written in Python, Staphopia\u2019s analysis pipeline consists of submodules running open-source tools. It accepts raw FASTQ reads as an input, which undergo quality control filtration, error correction and reduction to a maximum of approximately 100\u00d7 chromosome coverage. This reduction significantly reduces total runtime without detrimentally affecting the results. The pipeline performs de novo assembly-based and mapping-based analysis. Automated gene calling and annotation is performed on the assembled contigs. Read-mapping is used to call variants (single nucleotide polymorphisms and insertion/deletions) against a reference S. aureus chromosome (N315, ST5). We ran the analysis pipeline on more than 43,000 S. aureus shotgun Illumina genome projects in the public European Nucleotide Archive database in November 2017. We found that only a quarter of known multi-locus sequence types (STs) were represented but the top 10 STs made up 70% of all genomes. methicillin-resistant S. aureus (MRSA) were 64% of all genomes. Using the Staphopia database we selected 380 high quality genomes deposited with good metadata, each from a different multi-locus ST, as a non-redundant diversity set for studying S. aureus evolution. In addition to answering basic science questions, Staphopia could serve as a potential platform for rapid clinical diagnostics of S. aureus isolates in the future. The system could also be adapted as a template for other organism-specific databases.", "year": 2018, "citationCount": 78, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "bfcedc002a70132e69be732347cf89ccc17de93b", "title": "scMomentum: Inference of Cell-Type-Specific Regulatory Networks and Energy Landscapes", "abstract": "Recent progress in single-cell genomics has generated multiple tools for cell clustering, annotation, and trajectory inference; yet, inferring their associated regulatory mechanisms is unresolved. Here we present scMomentum, a model-based data-driven formulation to predict gene regulatory networks and energy landscapes from single-cell transcriptomic data without requiring temporal or perturbation experiments. scMomentum provides significant advantages over existing methods with respect to computational efficiency, scalability, network structure, and biological application. Availability scMomentum is available as a Python package at https://github.com/larisa-msoto/scMomentum.git", "year": 2020, "citationCount": 2, "fieldsOfStudy": ["Biology"]}
{"paperId": "c1225d221909509b8ce0bdbaa881cc7e2d74ed2f", "title": "UROPA: a tool for Universal RObust Peak Annotation", "abstract": null, "year": 2017, "citationCount": 44, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "c2e5c0e08f454757c4c0a745fc693b1b1bbd332e", "title": "miEAA 2.0: integrating multi-species microRNA enrichment analysis and workflow management systems", "abstract": "Gene set enrichment analysis has become one of the most frequently used applications in molecular biology research. Originally developed for gene sets, the same statistical principles are now available for all omics types. In 2016, we published the miRNA enrichment analysis and annotation tool (miEAA) for human precursor and mature miRNAs. Here, we present miEAA 2.0, supporting miRNA input from Homo sapiens, Mus musculus, and Rattus norvegicus. To facilitate inclusion of miEAA in workflow systems, we implemented an Application Programming Interface (API). Users can perform miRNA set enrichment analysis using either the web-interface, a dedicated Python package, or custom remote clients. Moreover, the number of category sets was raised by an order of magnitude. We implemented novel categories like annotation confidence level or localisation in biological compartments. In combination with the miR-Base miRNA-version and miRNA-to-precursor converters, miEAA supports research settings where older releases of miRBase are in use. The web server also offers novel comprehensive visualisations such as heatmaps and running sum curves with background distributions. Lastly, additional methods to correct for multiple hypothesis testing were implemented. We demonstrate the new features using case studies for human kidney cancer and mouse samples. The tool is freely accessible at: https://www.ccb.uni-saarland.de/mieaa2.", "year": 2020, "citationCount": 138, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"]}
{"paperId": "c50656e86e7e6a1041c01e6f9a680ab6e5f1fe35", "title": "Transcriptomic analysis of benznidazole-resistant and susceptible Trypanosoma cruzi populations", "abstract": null, "year": 2023, "citationCount": 6, "fieldsOfStudy": ["Medicine"]}
{"paperId": "c57017f5df871714a02a8e765f285dfee4c0edb4", "title": "Generating Python Type Annotations from Type Inference: How Far Are We?", "abstract": "In recent years, dynamic languages such as Python have become popular due to their flexibility and productivity. The lack of static typing makes programs face the challenges of fixing type errors, early bug detection, and code understanding. To alleviate these issues, PEP 484 introduced optional type annotations for Python in 2014, but unfortunately, a large number of programs are still not annotated by developers. Annotation generation tools can utilize type inference techniques. However, several important aspects of type annotation generation are overlooked by existing works, such as in-depth effectiveness analysis, potential improvement exploration, and practicality evaluation. And it is unclear how far we have been and how far we can go. In this paper, we set out to comprehensively investigate the effectiveness of type inference tools for generating type annotations, applying three categories of state-of-the-art tools on a carefully-cleaned dataset. First, we use a comprehensive set of metrics and categories, finding that existing tools have different effectiveness and cannot achieve both high accuracy and high coverage. Then, we summarize six patterns to present the limitations in type annotation generation. Next, we implement a simple but effective tool to demonstrate that existing tools can be improved in practice. Finally, we conduct a controlled experiment showing that existing tools can reduce the time spent annotating types and determine more precise types, but cannot reduce subjective difficulty. Our findings point out the limitations and improvement directions in type annotation generation, which can inspire future work.", "year": 2024, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "c639fc12e0add4fc2da38ff70b58071143b90ccd", "title": "QuakeLabeler: A Fast Seismic Data Set Creation and Annotation Toolbox for AI Applications", "abstract": "\n The production and preparation of data sets are essential steps in machine learning (ML) applications. With the increasing volume and scale of available ML techniques in seismology, annotating seismograms or seismic features has become time consuming and tedious for many researchers. Furthermore, most methods train and validate on unique data subsets, which hampers independent performance evaluation and comparison. To address this problem, we have developed the software QuakeLabeler, an open-source Python package to customize, build, and manage earthquake training data sets, including processing and visualization. QuakeLabeler has tight pipeline functions, which include retrieving seismograms from multiple online data centers, querying online human-reviewed catalogs, signal processing, annotating (labeling), and analyzing data distribution. In addition, relevant statistical graphics and human-readable output files can be generated. Various file export formats are supported, such as Seismic Analysis Code (*.sac), mini Standard for Exchange of Earthquake Data (*.mseed), NumPy (*.npz), MATLAB (*.mat), and the Hierarchical Data Format version 5 (*.hdf5). This toolbox is packaged with an interactive command-line interface. Three alternative running modes (beginner, advanced, and benchmark) are implemented, intended to offer specific data set solutions for different types of applications, that is, quick-start recipes for simple ML solutions, advanced design for customized project training, and benchmark bulletins for model comparison.", "year": 2022, "citationCount": 3, "fieldsOfStudy": null}
{"paperId": "c6cb292b4ba78e7b52aa5472c92be7078774035a", "title": "Systematic analysis of deep semantic segmentation architecture PSPNet on land cover ISPRS Vinhingen dataset", "abstract": "This paper provides a systematic review of Pyramid Scene Parsing Network deep learning semantic segmentation architecture applied to remotely sensed areas in imagery. Firstly, the state-of-the-arts architecture of deep learning for image-based semantic segmentation is reviewed, highlighting its contribution and its significance in the field of image segmentation. Secondly, the ISPRS benchmark dataset (Vaihingen) is used in testing with a detailed experimental setting and analysis of challenges. Then, quantitative results of the pooling layers against pooling type are investigated for the described deep learning architecture, following up with a discussion of the results. Interesting findings are summarised and a recommendation of the wider implementation is also pointed out.\u00a0 The main contribution of the research reveals that the deep learning architecture (PSPNET) can be efficiently applied to land cover classification of remotely sensed imagery with a classification rate up to 0794218%% as an average accuracy of the test set of Vaihingen dataset using four pooling layers against average pooling type which shows superior performance to small object segmentation such as the car class by 0.861777%. Moreover a comparison result of the four pooling layers against max pooling type architectures is also provided by achieving 0.7963976%.\u00a0From a practical point of view, all the experiments were run using NVIDIA GeForce GTX 1080 Ti GPU. For coding the architectures, Python on tensorflow as the most sophisticated deep learning programming language was used. The implementation of the selected recently developed deep semantic segmentation methods has shown a very high level of detecting efficiency of all the annotation limitations in the evaluated data sets where a revisit is strongly recommended", "year": 2020, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "c7c3895ffdb07a09616df5b8ec4c18dffa9457c4", "title": "as Data in the Context of Anonymising Clinical Study Reports", "abstract": "In the expanding data sharing landscape, there is an increasing expectation and demand to share clinical trial results and data. Techniques and standards are being developed to support anonymisation of patient-level and aggregate data stored in tabular form. One task that remains largely manual is anonymising/ redacting clinical study reports. This problem can be tackled by turning human-readable documents into data form that can be processed by machines. This paper explains techniques helpful in finding redaction candidates within text. From simple search, through regular expressions (RegEx, or RE), to Natural Language Processing (NLP). It introduces the ideas of parameterising and contextualising text to increase the accuracy of the findings. It discusses the challenges of extracting text from documents and explores ways to automatically apply the necessary edits and redactions. INTRODUCTION Anonymisation of data has been a hot topic over the last few years. There is a growing expectation and demand to make the clinical trial results and data available for wider audiences, from patients themselves and patient groups, to researchers, to general public. The recent changes and new requirements in the regulatory landscape necessitate development of techniques, and standards which allow for appropriate, defendable, and secure anonymisation of data ahead of sharing and using it for secondary purposes. The term \u201cdata\u201d is typically associated with clinical database and datasets, results in tabular forms, however, data can also be provided as documents, such as Clinical Study Report and similar. While there is an increasing understanding of assumptions, rules and methods in the pharmaceutical industry, which leads to better efficiency and automation, anonymisation of clinical documents and reports presents a completely different set of challenges and problems. The process can be split into four individual modules as shown on Figure 1. This paper describes the individual steps in detail, including the necessary assumptions, methodology and challenges. Figure 1. Process flow. PhUSE EU Connect 2018 2 EXTRACTING TEXT FROM DOCUMENTS Clinical data can be rendered into document form using various formats such as Adobe Portable Document Format (PDF), Microsoft Word (DOCX), and Rich Text Format (RTF). These documents include narrative text, tables and charts, thus making it easier to communicate clinical information to a human reader. However, in the process of creating these easy-to-read documents, the underlying structure and order of the data is typically rearranged in monolithic blocks of text, making it difficult for a computer program to tag, extract and manipulate the data embedded in document form. To further complicate the problem, Adobe PDF, the most prevalent file type that is used to render clinical data, is the most difficult to manipulate. The PDF was created in 1993 by Adobe Systems with the goal of creating an electronic document format that would appear the same on different devices independent of the environment on which they were created. While the PDF format achieved this goal, it did not contain any functionality for easy tagging or retrieval of text or tabular content. Therefore, extracting text from a PDF document is one of the first challenges one faces, as PDFs are not structured for data and most PDF creation programs do not add sufficient meta-data or tags to allow for easy retrieval of embedded textual information. A PDF file usually consists of text, vector graphics, and raster graphics. Vector graphics are used to store illustrations and designs, while raster graphics are used for images and text stored as content streams. Specific libraries are required to programmatically extract text stored as content streams. Existing PDF specifications only deal with annotations, encryption etc. and not with extracting and reusing data from a PDF, except to allow for accessibility for use by people with disabilities. There are quite a few open source and commercial libraries available to extract data from PDFs such as PyPDF, PDFMiner, PDFNet as well as the Acrobat SDK from Adobe. For this project, we used PDFNet SDK from PDFTron to extract text from CSRs in PDF format. PDFNet, while not great at extracting data from tables, does offer the best support for overall text parsing as well as redacting, annotating, highlighting of text and also has a WebViewer component which enables displaying PDF files in a web browser. We used the Python programming language in conjunction with PDFNet to iterate through the pages in a PDF document and extract blocks of text using the TextExtractor class. The extracted text is then subjected to a variety of methods to identify candidates for anonymisation or redaction. FINDING CANDIDATES FOR ANONYMISATION/REDACTION Before focusing on how to find data points within the text, the first step is to establish what type of information should be looked for. At this stage, we are not deciding if and how to anonymise, we are simply extracting the information so it can be processed in subsequent steps. The following details should be identified within text for later processing: Personal information like names, phone numbers, addresses, social security numbers, etc., IDs, whether they are classified as direct identifiers (e.g. subject ID, test ID, etc.) or quasi-identifiers (e.g. site ID, lab ID, etc.), Demographic information: sex, age, race, ethnicity, Geographic information: country, region, continent, Body measurements: height, weight, BMI, etc., Adverse events and medical history, Medications, therapies, and procedures, Mentions of family, Sensitive info, like pregnancy, abortions, mental health, substance abuse, etc. Dates. Some details will be fairly straight-forward to distinguish in the text, since they will follow certain patterns (IDs, dates, etc.), or use a finite list of values (sex, country, race, etc.), while others will require more complex rules and algorithms to identify them in the text. Another challenge is to attribute each piece of information to a specific subject. This may not be necessary if redaction is used to anonymise the document. However, when values are to be replaced with their anonymised versions, it may be required to understand which subject they belong to. PhUSE EU Connect 2018", "year": 2018, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "c8dce2bea9bb73058b8310e367fbb1a0a5af8a1b", "title": "FlaiMapper: computational annotation of small ncRNA-derived fragments using RNA-seq high-throughput data", "abstract": "MOTIVATION\nRecent discoveries show that most types of small non-coding RNAs (sncRNAs) such as miRNAs, snoRNAs and tRNAs get further processed into putatively active smaller RNA species. Their roles, genetic profiles and underlying processing mechanisms are only partially understood. To find their quantities and characteristics, a proper annotation is essential. Here, we present FlaiMapper, a method that extracts and annotates the locations of sncRNA-derived RNAs (sncdRNAs). These sncdRNAs are often detected in sequencing data and observed as fragments of their precursor sncRNA. Using small RNA-seq read alignments, FlaiMapper is able to annotate fragments primarily by peak detection on the start and end position densities followed by filtering and a reconstruction process.\n\n\nRESULTS\nTo assess performance of FlaiMapper, we used independent publicly available small RNA-seq data. We were able to detect fragments representing putative sncdRNAs from nearly all types of sncRNA, including 97.8% of the annotated miRNAs in miRBase that have supporting reads. Comparison of FlaiMapper-predicted boundaries of miRNAs with miRBase entries demonstrated that 89% of the start and 54% of the end positions are identical. Additional benchmarking showed that FlaiMapper is superior in performance compared with existing software. Further analysis indicated a variety of characteristics in the fragments, including sequence motifs and relations with RNA interacting factors. These characteristics set a good basis for further research on sncdRNAs.\n\n\nAVAILABILITY AND IMPLEMENTATION\nThe platform independent GPL licensed Python 2.7 code is available at: https://github.com/yhoogstrate/flaimapper.", "year": 2015, "citationCount": 26, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "ca8a9ba8c4cda13b004c5b773e783b2b53bf9788", "title": "ARTWORK SEARCH ENGINE THROUGH WORD EMBEDDINGS", "abstract": "The Artigo Project collects annotations for artworks and has a search function that can currently search for artworks using these collected annotations. The current search engine works on an exact-match basis, which means that only artworks annotated with the same annotations as searched are returned in the search result. The goal of this thesis is to improve the search results by also showing semantically relevant artworks, which do not match the exact search keyword. To achieve this word embeddings are used. Word embedding is a very powerful mechanism that can be used for mostly all word comparison problems. It is an innovative way to enhance search results and has the potential to be improved further in the future. Its technique is to map words to word vectors which represent the meaning of the words. These vectors form a vector space, which can be used to define the similarity of the artworks. With this information, the search results can be sorted correspondingly. First, a big vector space has to be produced, which includes embeddings of words that a user will possibly type into the search field. Three existing methods for creating word embeddings are Glove, Word2Vec, and fastText. In the next step with each of these methods, for all artworks, a corresponding vector has to be calculated and put into the existing vector space. For this calculation, a custom algorithm is created that uses currently existing artwork annotations and a normalization method. Finally, the vector space needs to be searched for results. For searching, there are existing solutions available, that mostly work on the Approximate Nearest Neighbors principle. Annoy from Spotify, NGT from Yahoo and NMSLIB are examples of such solutions. These libraries are optimized for speed and search for points in word embeddings that are close to a given query point. After a comparison between the existing methods and algorithms, for the implementation, a custom Python search server is implemented which uses the vector space based search engine solution named Annoy and a corpus that has been trained with Word2Vec. Further modifications to the existing frontend and backend required to make it work are described. The result is a search engine in which users profit from improved search results and a greater chance to find what they are looking for.", "year": 2020, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "cb64726caf3f782dee7f9f814c4a6c8076b27577", "title": "Python Interpreter Performance Deconstructed", "abstract": "The Python programming language is known for performing poorly on many tasks. While to some extent this is to be expected from a dynamic language, it is not clear how much each dynamic feature contributes to the costs of interpreting Python. In this study we attempt to quantify the costs of language features such as dynamic typing, reference counting for memory management, boxing of numbers, and late binding of function calls.\n We use an experimental compilation framework for Python that can make use of type annotations provided by the user to specialize the program as well as elide unnecessary reference counting operations and function lookups. The compiled programs run within the Python interpreter and use its internal API to implement language semantics. By separately enabling and disabling compiler optimizations, we can thus measure how much each language feature contributes to total execution time in the interpreter.\n We find that a boxed representation of numbers as heap objects is the single most costly language feature on numeric codes, accounting for up to 43% of total execution time in our benchmark set. On symbolic object-oriented code, late binding of function and method calls costs up to 30%. Redundant reference counting, dynamic type checks, and Python's elaborate function calling convention have comparatively smaller costs.", "year": 2014, "citationCount": 18, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "cbb157aaf2c5e3d1f2f995a2a2cb1bf3710db4aa", "title": "ClusterScan: simple and generalistic identification of genomic clusters", "abstract": "Summary Studies on gene clusters proved to be an excellent source of information to understand genomes evolution and identifying specific metabolic pathways or gene families. Improvements in sequencing methods have resulted in a large increase of sequenced genomes for which cluster annotation could be performed and standardized. Currently available programs are developed to search for specific cluster types and none of them is suitable for a broad range of user\u2010based choices. We have developed ClusterScan which allows identifying clusters of any kind of feature simply based on their genomic coordinates and user\u2010defined categorical annotations. Availability and implementation The tool is written in Python, distributed under the GNU General Public License (GPL) and available on Github at http://bit.ly/ClusterScan or as Docker image at sangeslab/clusterscan: latest. It is supported through a mailing\u2010list on http://bit.ly/ClusterScanSupport. Supplementary information Supplementary data are available at Bioinformatics online.", "year": 2018, "citationCount": 10, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "cd0ebbfd8d0037514a55d64f793199a686bb2c7f", "title": "Perceptual annotation of local distortions in videos: tools and datasets", "abstract": "To assess the quality of multimedia content, create datasets, and train objective quality metrics, one needs to collect subjective opinions from annotators. Different subjective methodologies exist, from direct rating with single or double stimuli to indirect rating with pairwise comparisons. Triplet and quadruplet-based comparisons are a type of indirect rating. From these comparisons and preferences on stimuli, we can place the assessed stimuli on a perceptual scale (e.g., from low to high quality). Maximum Likelihood Difference Scaling (MLDS) solver is one of these algorithms working with triplets and quadruplets. A participant is asked to compare intervals inside pairs of stimuli: (a,b) and (c,d), where a,b,c,d are stimuli forming a quadruplet. However, one limitation is that the perceptual scales retrieved from stimuli of different contents are usually not comparable. We previously offered a solution to measure the inter-content scale of multiple contents. This paper presents an open-source python implementation of the method and demonstrates its use on three datasets collected in an in-lab environment. We compared the accuracy and effectiveness of the method using pairwise, triplet, and quadruplet for intra-content annotations. The code is available here: https://github.com/andreaspastor/MLDS_inter_content_scaling.", "year": 2023, "citationCount": 4, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "cede87fce1cc6075360e8b5149208c158c43df74", "title": "MethyMer: Design of combinations of specific primers for bisulfite sequencing of complete CpG islands", "abstract": "We present MethyMer, a Python-based tool aimed at selecting primers for amplification of complete CpG islands. These regions are difficult in terms of selecting appropriate primers because of their low-complexity, high GC content. Moreover, bisulfite treatment, in fact, leads to the reduction of the 4-letter alphabet (ATGC) to 3-letter one (ATG, except for methylated cytosines), and this also reduces region complexity and increases mispriming potential. MethyMer has a flexible scoring system, which optimizes the balance between various characteristics such as nucleotide composition, thermodynamic features (melting temperature, dimers [Formula: see text]G, etc.), the presence of CpG sites and polyN tracts, and primer specificity, which is assessed with aligning primers to the bisulfite-treated genome using bowtie (up to three mismatches are allowed). Users are able to customize desired or limit ranges of various parameters as well as penalties for non-desired values. Moreover, MethyMer allows picking up the optimal combination of PCR primer pairs to perform the amplification of a large genomic locus, e.g. CpG island or other hard-to-study region, with minimal overlap of the individual amplicons. MethyMer incorporates ENCODE genome annotation records (promoter/enhancer/insulator), The Cancer Genome Atlas (TCGA) CpG methylation data derived with Illumina Infinium 450K microarrays, and records on correlations between TCGA RNA-Seq and CpG methylation data for 20 cancer types. These databases are included in the MethyMer release. Our tool is available at https://sourceforge.net/projects/methymer/ .", "year": 2018, "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"]}
{"paperId": "cfa677f5d9afe49184342cccd899117470c0682a", "title": "SBtab: a flexible table format for data exchange in systems biology", "abstract": "Summary: SBtab is a table-based data format for Systems Biology, designed to support automated data integration and model building. It uses the structure of spreadsheets and defines conventions for table structure, controlled vocabularies and semantic annotations. The format comes with predefined table types for experimental data and SBML-compliant model structures and can easily be customized to cover new types of data. Availability and Implementation: SBtab documents can be created and edited with any text editor or spreadsheet tool. The website www.sbtab.net provides online tools for syntax validation and conversion to SBML and HTML, as well as software for using SBtab in MS Excel, MATLAB and R. The stand-alone Python code contains functions for file parsing, validation, conversion to SBML and HTML and an interface to SQLite databases, to be integrated into Systems Biology workflows. A detailed specification of SBtab, including examples and descriptions of table types and available tools, can be found at www.sbtab.net. Contact: wolfram.liebermeister@gmail.com", "year": 2016, "citationCount": 33, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "d24055a467bbf783d33a2a18e19a968f32cff34f", "title": "AnnotaPipeline: An integrated tool to annotate eukaryotic proteins using multi-omics data", "abstract": "Assignment of gene function has been a crucial, laborious, and time-consuming step in genomics. Due to a variety of sequencing platforms that generates increasing amounts of data, manual annotation is no longer feasible. Thus, the need for an integrated, automated pipeline allowing the use of experimental data towards validation of in silico prediction of gene function is of utmost relevance. Here, we present a computational workflow named AnnotaPipeline that integrates distinct software and data types on a proteogenomic approach to annotate and validate predicted features in genomic sequences. Based on FASTA (i) nucleotide or (ii) protein sequences or (iii) structural annotation files (GFF3), users can input FASTQ RNA-seq data, MS/MS data from mzXML or similar formats, as the pipeline uses both transcriptomic and proteomic information to corroborate annotations and validate gene prediction, providing transcription and expression evidence for functional annotation. Reannotation of the available Arabidopsis thaliana, Caenorhabditis elegans, Candida albicans, Trypanosoma cruzi, and Trypanosoma rangeli genomes was performed using the AnnotaPipeline, resulting in a higher proportion of annotated proteins and a reduced proportion of hypothetical proteins when compared to the annotations publicly available for these organisms. AnnotaPipeline is a Unix-based pipeline developed using Python and is available at: https://github.com/bioinformatics-ufsc/AnnotaPipeline.", "year": 2022, "citationCount": 1, "fieldsOfStudy": ["Medicine"]}
{"paperId": "d250dd92ff0737f0801cc483775257904b8827ff", "title": "MaxSMT-Based Type Inference for Python 3", "abstract": null, "year": 2018, "citationCount": 40, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "d454e3394621f811659a891f79dfa4b4a08f483b", "title": "scATAnno: Automated Cell Type Annotation for single-cell ATAC Sequencing Data", "abstract": "Recent advances in single-cell epigenomic techniques have created a growing demand for scATAC-seq analysis. One key analysis task is to determine cell type identity based on the epigenetic data. We introduce scATAnno, a python package designed to automatically annotate scATAC-seq data using large-scale scATAC-seq reference atlases. This workflow generates the reference atlases from publicly available datasets enabling accurate cell type annotation by integrating query data with reference atlases, without the use of scRNA-seq data. To enhance annotation accuracy, we have incorporated KNN-based and weighted distance-based uncertainty scores to effectively detect cell populations within the query data that are distinct from all cell types in the reference data. We compare and benchmark scATAnno against 7 other published approaches for cell annotation and show superior performance in multiple data sets and metrics. We showcase the utility of scATAnno across multiple datasets, including peripheral blood mononuclear cell (PBMC), Triple Negative Breast Cancer (TNBC), and basal cell carcinoma (BCC), and demonstrate that scATAnno accurately annotates cell types across conditions. Overall, scATAnno is a useful tool for scATAC-seq reference building and cell type annotation in scATAC-seq data and can aid in the interpretation of new scATAC-seq datasets in complex biological systems.", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "d628aedd8e273d24d4476a593328637c56c7313d", "title": "Haystack: systematic analysis of the variation of epigenetic states and cell-type specific regulatory elements", "abstract": "Motivation With the increasing amount of genomic and epigenomic data in the public domain, a pressing challenge is how to integrate these data to investigate the role of epigenetic mechanisms in regulating gene expression and maintenance of cell-identity. To this end, we have implemented a computational pipeline to systematically study epigenetic variability and uncover regulatory DNA sequences that play a role in gene regulation. Results Haystack is a bioinformatics pipeline to characterize hotspots of epigenetic variability across different cell-types as well as cell-type specific cis-regulatory elements along with their corresponding transcription factors. Our approach is generally applicable to any epigenetic mark and provides an important tool to investigate cell-type identity and the mechanisms underlying epigenetic switches during development. Additionally, we make available a set of precomputed tracks for a number of epigenetic marks across several cell types. These precomputed results may be used as an independent resource for functional annotation of the human genome. Availability The Haystack pipeline is implemented as an open-source, multiplatform, Python package called haystack_bio available at https://github.com/pinellolab/haystack_bio. Contact lpinello@mgh.harvard.edu, gcyuan@jimmy.harvard.edu", "year": 2017, "citationCount": 16, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"]}
{"paperId": "d6ab66e58815bd4a6af80c104b3393a694c87b13", "title": "NGSmethDB 2017: enhanced methylomes and differential methylation", "abstract": "The 2017 update of NGSmethDB stores whole genome methylomes generated from short-read data sets obtained by bisulfite sequencing (WGBS) technology. To generate high-quality methylomes, stringent quality controls were integrated with third-part software, adding also a two-step mapping process to exploit the advantages of the new genome assembly models. The samples were all profiled under constant parameter settings, thus enabling comparative downstream analyses. Besides a significant increase in the number of samples, NGSmethDB now includes two additional data-types, which are a valuable resource for the discovery of methylation epigenetic biomarkers: (i) differentially methylated single-cytosines; and (ii) methylation segments (i.e. genome regions of homogeneous methylation). The NGSmethDB back-end is now based on MongoDB, a NoSQL hierarchical database using JSON-formatted documents and dynamic schemas, thus accelerating sample comparative analyses. Besides conventional database dumps, track hubs were implemented, which improved database access, visualization in genome browsers and comparative analyses to third-part annotations. In addition, the database can be also accessed through a RESTful API. Lastly, a Python client and a multiplatform virtual machine allow for program-driven access from user desktop. This way, private methylation data can be compared to NGSmethDB without the need to upload them to public servers. Database website: http://bioinfo2.ugr.es/NGSmethDB.", "year": 2016, "citationCount": 15, "fieldsOfStudy": ["Computer Science", "Biology", "Medicine"]}
{"paperId": "d7893b35837519d7f3d7eebaf2d0105e2a8634d9", "title": "msCentipede: Modeling heterogeneity across genomic sites improves accuracy in the inference of transcription factor binding", "abstract": "Motivation: Understanding global gene regulation depends critically on accurate annotation of regulatory elements that are functional in a given cell type. CENTIPEDE, a powerful, probabilistic framework for identifying transcription factor binding sites from tissue-specific DNase I cleavage patterns and genomic sequence content, leverages the hypersensitivity of factor-bound chromatin and the information in the DNase I spatial cleavage profile characteristic of each DNA binding protein to accurately infer functional factor binding sites. However, the model for the spatial profile in this framework underestimates the substantial variation in the DNase I cleavage profiles across factor-bound genomic locations and across replicate measurements of chromatin accessibility. Results: In this work, we adapt a multi-scale modeling framework for inhomogeneous Poisson processes to better model the underlying variation in DNase I cleavage patterns across genomic locations bound by a transcription factor. In addition to modeling variation, we also model spatial structure in the heterogeneity in DNase I cleavage patterns for each factor. Using DNase-seq measurements assayed in a lymphoblastoid cell line, we demonstrate the improved performance of this model for several transcription factors by comparing against the Chip-Seq peaks for those factors. Finally, we propose an extension to this framework that allows for a more flexible background model and evaluate the additional gain in accuracy achieved when the background model parameters are estimated using DNase-seq data from naked DNA. The proposed model can also be applied to paired-end ATAC-seq and DNase-seq data in a straightforward manner. Availability: msCentipede, a Python implementation of an algorithm to infer transcription factor binding using this model, is made available at https://github.com/rajanil/msCentipede", "year": 2014, "citationCount": 2, "fieldsOfStudy": ["Biology"]}
{"paperId": "d7ad1a0a899d49144442189a35e048cafe9fae28", "title": "scEVOLVE: cell-type incremental annotation without forgetting for single-cell RNA-seq data", "abstract": "Abstract The evolution in single-cell RNA sequencing (scRNA-seq) technology has opened a new avenue for researchers to inspect cellular heterogeneity with single-cell precision. One crucial aspect of this technology is cell-type annotation, which is fundamental for any subsequent analysis in single-cell data mining. Recently, the scientific community has seen a surge in the development of automatic annotation methods aimed at this task. However, these methods generally operate at a steady-state total cell-type capacity, significantly restricting the cell annotation systems'capacity for continuous knowledge acquisition. Furthermore, creating a unified scRNA-seq annotation system remains challenged by the need to progressively expand its understanding of ever-increasing cell-type concepts derived from a continuous data stream. In response to these challenges, this paper presents a novel and challenging setting for annotation, namely cell-type incremental annotation. This concept is designed to perpetually enhance cell-type knowledge, gleaned from continuously incoming data. This task encounters difficulty with data stream samples that can only be observed once, leading to catastrophic forgetting. To address this problem, we introduce our breakthrough methodology termed scEVOLVE, an incremental annotation method. This innovative approach is built upon the methodology of contrastive sample replay combined with the fundamental principle of partition confidence maximization. Specifically, we initially retain and replay sections of the old data in each subsequent training phase, then establish a unique prototypical learning objective to mitigate the cell-type imbalance problem, as an alternative to using cross-entropy. To effectively emulate a model that trains concurrently with complete data, we introduce a cell-type decorrelation strategy that efficiently scatters feature representations of each cell type uniformly. We constructed the scEVOLVE framework with simplicity and ease of integration into most deep softmax-based single-cell annotation methods. Thorough experiments conducted on a range of meticulously constructed benchmarks consistently prove that our methodology can incrementally learn numerous cell types over an extended period, outperforming other strategies that fail quickly. As far as our knowledge extends, this is the first attempt to propose and formulate an end-to-end algorithm framework to address this new, practical task. Additionally, scEVOLVE, coded in Python using the Pytorch machine-learning library, is freely accessible at https://github.com/aimeeyaoyao/scEVOLVE.", "year": 2024, "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "d80c3fd556774a09305bd60bdfab154e312d85fd", "title": "How Dynamic Features Affect API Usages? An Empirical Study of API Misuses in Python Programs", "abstract": "Incorrect usages of Application Programming Interfaces (APIs) may lead to unexpected problems during the software development process. Although there have been many attempts to address API-misuse issues, most of them are mainly for static languages. In contrast, API misuses in dynamic languages are rarely covered, mostly due to challenges about dynamic features. In this paper, we develop the first-ever comprehensive study of API misuses for Python programs. To accomplish this, we manually analyze 79,096 commits of six popular open-source Python projects on GitHub to collect true-positive cases. Based on the validation, we develop a classification of Python API Misuses, called PAM, and a dataset, PAMBench, containing 670 validated real-world API-misuse cases in popular Python programs. For each API-misuse case, we explore its root cause, symptom, program issue and repair method. Specifically, we pay attention to the effect of dynamic features on API usages in Python. The systematic study on PAMBench shows that, most importantly, dynamic features, especially type dynamics, have a non-negligible impact on API usages in Python, mainly related to incorrect assumptions about the type, callable state, attribute and existence of caller object, method call itself, passed argument(s) and return value during an API invocation. Our root-cause analysis reveals the importance of correct design, implementation, annotation, checking and recording about the types and states of all parts of API method calls during Python program development. Finally, we present possible solutions for more secure, reliable and maintainable API usages in Python.", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "daa11dfed48bbfa7b128bd203296bef729fef519", "title": "Pythran: Enabling Static Optimization of Scientific Python Programs", "abstract": "Pythran is an open source static compiler that turns modules written in a subset of Python language into native ones. Assuming that scientific modules do not rely much on the dynamic features of the language, it trades them for powerful, possibly inter-procedural, optimizations. These optimizations include detection of pure functions, temporary allocation removal, constant folding, Numpy ufunc fusion and parallelization, explicit thread-level parallelism through OpenMP annotations, false variable polymorphism pruning, and automatic vector instruction generation such as AVX or SSE. In addition to these compilation steps, Pythran provides a C++ runtime library that leverages the C++ STL to provide generic containers, and the Numeric Template Toolbox for Numpy support. It takes advantage of modern C++11 features such as variadic templates, type inference, move semantics and perfect forwarding, as well as classical idioms such as expression templates. Unlike the Cython approach, Pythran input code remains compatible with the Python interpreter. Output code is generally as efficient as the annotated Cython equivalent, if not more, but without the backward compatibility loss.", "year": 2013, "citationCount": 45, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "daeae5271db4dd2084f481287e0beeee5247727e", "title": "Automated annotation of protein families", "abstract": "Introduction: The great challenge in bioinformatics is data integration. The amount of available data is always increasing and there are no common unified standards of where, or how, the data should be stored. The aim of this workis to build an automated tool to annotate the different member families within the protein superfamily of medium-chain dehydrogenases/reductases (MDR), by finding common properties among the member proteins. The goal is to increase the understanding of the MDR superfamily as well as the different member families.This will add to the amount of knowledge gained for free when a new, unannotated, protein is matched as a member to a specific MDR member family. Method: The different types of data available all needed different handling. Textual data was mainly compared as strings while numeric data needed some special handling such as statistical calculations. Ontological data was handled as tree nodes where ancestry between terms had to be considered. This was implemented as a plugin-based system to make the tool easy to extend with additional data sources of different types. Results: The biggest challenge was data incompleteness yielding little (or no) results for some families and thus decreasing the statistical significance of the results. Results show that all the human and mouse MDR members have a Pfam ADH domain (ADH_N and/or ADH_zinc_N) and takes part in an oxidation-reduction process, often with NAD or NADP as cofactor. Many of the proteins contain zinc and are expressed in liver tissue. Conclusions: A python based tool for automatic annotation has been created to annotate the different MDR member families. The tool is easily extendable to be used with new databases and much of the results agrees with information found in literature. The utility and necessity of this system, as well as the quality of its produced results, are expected to only increase over time, even if no additional extensions are produced, as the system itself is able to make further and more detailed inferences as more and more data become available.", "year": 2011, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "db7eb6a33d4b267346d362bcdceb6e715fa1365f", "title": "Development of a Python\u2010Based Algorithm for the Quantification and Analysis of Multivariate Next Generation Sequencing Data Following Genome\u2010Wide Mutagenesis and Cell Survival Screening", "abstract": "Hsp90 is a highly conserved eukaryotic chaperone protein responsible for mediating a myriad of intracellular signaling pathways, including those associated with the glucocorticoid receptor (GR) and the intracellular immunoinflammatory response. Both hsp90 inhibitors such as geldanamycin and glucocorticoids such as prednisone have been shown to possess potent anticancer activity, which can be overcome at the cellular level by hereto unknown genetic modifications. The purpose of this study was to develop a computational approach for interpreting and analyzing next generation sequencing data following genome\u2010wide mutagenesis and anticancer cell survival screening assays. In order to assess genome mutation sites, an open\u2010source computer program was developed. The program allows for the reading of raw sequencing and annotated data files and permits the user to select filtering parameters to quantify the incidence and prevalence of identified mutations and mutation insertion sites. Further analysis parameters include annotation and extrapolation of the surrounding gene landscape and anticipated mutation influence. Additional parameters, including filtration based on mutation type and significance, sequencing confidence and reads, mutation location and neighboring gene profiles are also included, thus allowing for prediction of potential causal candidate mutations and genetic perturbations affecting cell viability subsequent to cell chaperone inhibition and glucocorticoid\u2010mediated transactivation.", "year": 2016, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "ddf2e20427e24b422cc11f58a27458b75e1d3cca", "title": "Generative Type Inference for Python", "abstract": "Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze- style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high- quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic. What is more, their predictions are not interpretable, hindering developers' understanding and verification of the results. This paper introduces Typegen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. Typegen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypegEnconstructs example prompts from human annotations. Typeg Enonly requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, Typeg Enenhances the interpretability of results through the use of the input- explanation-output strategy, which generates both explanations and type predictions in COT prompts. Experiments show that Typegen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5 % in return value type prediction in terms of top-l Exact Match by using only five examples. Furthermore, Typeg Enachieves substantial improvements of 27 % to 84 % compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-I Exact Match.", "year": 2023, "citationCount": 6, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "de00b2ba6b221f6c739231d9e17b93974ed47f1b", "title": "Penerapan Metode CNN (Convolutional Neural Network) dalam Mengklasifikasi Uang Kertas dan Uang Logam", "abstract": "Banknotes and coins are valuable assets that are used as legal means of payment in everyday life. The value of these two types of money has been determined and is printed on each piece of banknote when used in transactions and trade. Even though currently banknotes can be recognized using technology such as ATM machines, these machines are only able to recognize the value of the largest currency owned by a country. Computers require digital images as input to display the information contained therein because computers do not have the ability of the human eye to directly recognize or calculate the objects they see. Therefore, techniques or methods are needed that aim to obtain information from digital images to facilitate human interpretation. This research aims to design a system for detecting banknotes in images using the Convolutional Neural Network (CNN) architecture, which is a form of deep learning. . The system also integrates image pre-processing using user-based manual annotation techniques in Python program code. Using the CNN method, a test was carried out to detect the nominal amount of money in the input image. Test results using 29 banknote dataset samples and 31 coin money dataset samples show that the two types of money are divided into two classes, namely paper and coins. From the training carried out on banknotes and coins, an average accuracy of 98% was obtained, showing good results. Repetition of the detection process also shows consistent output probabilities. However, there are several denominations of money that show high accuracy values, so it can be concluded that the labeling annotation method is thought to be less effective.", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "dea1b39697764c67e7ed70575f4452189747f020", "title": "Computational Analysis of the Relationships Between Antibiotic Resistance and Plasmid Backbone Genes", "abstract": "Antibiotic-resistant bacteria are becoming one of the leading public health threats globally. Although, the presence of plasmid-encoded antibiotic resistance has been both extensively researched and well-documented, the relationships between antibiotic resistance genes (ARGs) and plasmid backbone structure have not. Understanding these relationships would provide important insight into the history of how bacterial plasmids have developed and the potential of plasmid backbones to acquire ARGs. This research aims to provide a comprehensive analysis of the relationships between ARGs and plasmid backbone structure. By implementing a computational approach to characterize a large sample of plasmids representative of each incompatibility group, we determined what ARGs are most frequently associated with given plasmid backbones as well as common insertion patterns. Introduction The dissemination of antibiotic resistance genes (ARGs) in bacteria is a growing threat to public health worldwide. One contributing factor to the rapid spread of bacteria that are resistant to clinically relevant antibiotics is plasmid mediated horizontal gene transfer [3]. Plasmids are \u201ccircular DNA molecules that replicate independently of the chromosome and are able to transfer horizontally between bacteria by conjugation\u201d [3]. These mobile genetic elements have been extensively researched, and studies have shown that \u201conce resistance genes have become established on successful plasmids, they may rapidly spread across different strains, species, or even genera\u201d [4]. This is because plasmids containing ARGs provide their host cell with a survival advantage over its competitors, such as the ability to thrive in the presence of otherwise lethal antibiotics. As a result, \u201cplasmids carry a considerable variety of genes, including those that confer antibiotic resistance\u201d [1]. This development in resistance in response to environmental pressures is a considerable threat to human health and presents serious challenges in regards to the efficacy of modern medical practice [1]. Although the existence of ARGs in plasmids has been studied extensively, lack of consistency and standardization in plasmid classification and the annotation of genes belonging to plasmids has inhibited scientists\u2019 ability to further understand plasmid evolution. Traditionally, plasmids have been classified and categorized into incompatibility groups, which were originally defined as the failure of two plasmids to reside in a host cell and be inherited together. More recently, incompatibility groups are being defined in terms of genetic similarity of the plasmid replicon, which is the region of the plasmid that encodes functions for the activation and control of replication [2]. This method of plasmid typing, however, is becoming antiquated because it makes the assumption that variation in the modular structure of plasmids, meaning that related functions are clustered in specific regions of the DNA, is due to different phylogenetic origins and that plasmids are built via the random juxtaposition of these different functional modules [6]. Instead, plasmid classification is starting to rely more on evolutionary strategy, in other words, the genes in the plasmid genome that are responsible for the functions related to its own survival and propagation. These genes make up the \u201cplasmid backbone\u201d. It is the backbone genes (BGs) that determine characteristics such as copy number, transfer frequency, host range, stability, and other qualities. Therefore, developing a standardized system of naming backbone genes is imperative to the successful classification of plasmids based on modular function. There currently exist many discrepancies in the annotation of backbone genes. For example, multiple names exist for identical proteins, and often distantly related proteins can have the same name. These errors are the result of biases in sequence analysis programs and propagated via automated annotation programs [5]. Recently, Thomas et al. proposed a standardized plasmid backbone gene naming convention to help reconcile some of the issues in the annotation of plasmid backbone genes. Building on this, [17] used some of the reference plasmids and their backbone genes, and applied this nomenclature to identify other plasmid backbone genes showing high sequence similarity to these known references. The export feature of this tool allows one to export plasmid backbone genes that can be reasonably identified and called based on this new nomenclature. With some modification, these annotated genes can then be turned into a curated database following a standardized naming convention and applied to the automated annotation of plasmids. This standardized naming convention allows for the systematic comparison of plasmid organization and structure. Understanding the structure and organization of plasmids allows us to better understand the relationships and interactions between mobile genetic elements and the plasmid genes themselves, likely leading to differences in plasmid function. It is for this reason that understanding the relationship between backbone genes and the insertion patterns of ARGs is so important and yet factors determining how and where ARGs insert into the plasmid backbone have not been determined. It is known that ARGs are inserted into plasmid sequences via transposition (transposons) and site specific recombination mechanisms (integron gene cassettes) [1]. Most transposons are thought to not have preference for specific insertion sites on plasmids but instead insert into new sites more or less at random [1]. Although integrons and their gene cassettes utilize site-specific recombination, the placement of the integron itself (the int gene for example) is also thought to be random, thus there is no discovered determinant for the overall insertion placement of the gene cassettes. However, disruptions to the plasmid backbone organization could be deleterious to the plasmid and thus may play a role in limiting the locations of insertions and accessory load. This research aims to deduce the relationships between the plasmid backbone genes of various incompatibility groups and ARGs by using bioinformatics tools and computational methods to annotate, identify, and analyze a large representative population of naturally occurring bacterial plasmids. First, a database was created by selecting backbone genes from the standardized backbone database whose names we can confidently call because they have been taken from a reference plasmid with a known determined incompatibility group. ARGs and nucleotide sequences representative of incompatibility groups were also pulled from the distinguished databases, resFinder and plasmidFinder, respectively [18,2]. All sequences underwent extensive formatting to create a database compatible with the annotation tool, Prokka [16]. Prokka was then used to annotate a large representative group of 8,895 plasmids from Genbank using the database created in this study as the priority reference. Output from Prokka was then run through python scripts to pull genes annotated only with our database and create tables of ARGs and BGs with their corresponding plasmid name as well as its assigned incompatibility group. Gene tables created by these python scripts were then analyzed for patterns and figures were created using RStudio. In response to the movement from typing plasmids by incompatibility group toward a new method of categorizing plasmid backbones based on modular function, this research uses standardized backbone naming in aims to quantify distribution preferences for particular incompatibility groups and locations within them. In this way, results from this study can provide direction for research within the constructs of both categorizing methods in terms of how these patterns and relationships affect antibiotic resistance.", "year": 2019, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "e04bc1a2f740fb5779d394f2d0692b37c3a2b825", "title": "A scalable sparse neural network framework for rare cell type annotation of single-cell transcriptome data", "abstract": "Automatic cell type annotation methods are increasingly used in single-cell RNA sequencing (scRNA-seq) analysis due to their fast and precise advantages. However, current methods often fail to account for the imbalance of scRNA-seq datasets and ignore information from smaller populations, leading to significant biological analysis errors. Here, we introduce scBalance, an integrated sparse neural network framework that incorporates adaptive weight sampling and dropout techniques for auto-annotation tasks. Using 20 scRNA-seq datasets with varying scales and degrees of imbalance, we demonstrate that scBalance outperforms current methods in both intra- and inter-dataset annotation tasks. Additionally, scBalance displays impressive scalability in identifying rare cell types in million-level datasets, as shown in the bronchoalveolar cell landscape. scBalance is also significantly faster than commonly used tools and comes in a user-friendly format, making it a superior tool for scRNA-seq analysis on the Python-based platform. An integrated sparse neural network framework scBalance incorporates adaptive weight sampling and dropout techniques to handle noise and imbalanced datasets and achieves high classification accuracy for both rare and major cell types.", "year": 2022, "citationCount": 6, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "e0dec78865f733eb7ac572c08cf65466c6d7d3d8", "title": "Scaling up SoccerNet with multi-view spatial localization and re-identification", "abstract": null, "year": 2022, "citationCount": 32, "fieldsOfStudy": ["Medicine"]}
{"paperId": "e0fdc657375ae2531a762c9756520625d1f538df", "title": "Abstract 5039: Visualization and analysis of cancer genomics data using UCSC Xena", "abstract": "\n UCSC Xena (http://xena.ucsc.edu/) is a web-based visual integration and exploration tool for multi-omic data and associated clinical and phenotypic annotations. Researchers can easily view and explore public data, their own private data, or both using the Xena Browser. Private data are kept on the researcher's computer and are never uploaded to our public servers. The toll supports Mac, Windows, and Linux.\n Questions Xena can help you answer:\n 1) Is overexpression of this gene associated with lower/higher survival?\n 2) What genes are differentially expressed between these two groups of samples?\n 3) What is the relationship between mutation, copy number, expression, etc for this gene?\n Xena showcases seminal cancer genomics datasets from TCGA, the Pan-Cancer Atlas, GDC, PCAWG, ICGC, and more; a total of more than 1500 datasets across 50 cancer types. We support virtually any type of functional genomics data: SNPs, INDELs, copy number variation, gene expression, ATAC-seq, DNA methylation, exon-, transcript-, miRNA-, lncRNA-expression, and structural variants. We also support clinical data such as phenotype information, subtype classifications and biomarkers. All of our data is available for download via python or R APIs, or using our URL links.\n Our signature Visual Spreadsheet view shows multiple data types side-by-side enabling discovery of correlations across and within genes and genomic regions. We also have dynamic Kaplan-Meier survival analysis, powerful filtering and subgrouping, differential gene expression analysis, charts, statistical analyses, genomic signatures, and the ability to generate URLs to live views. We link out to the UCSC Genome Browser as well as MuPIT/CRAVAT and TumorMap.\n New features include:\n - Genome-wide differential gene expression analysis\n - New interface for filtering samples and creating subgroups\n - New interface to create charts and graphs\n - Violin plots on any numerical data\n - Loading of Microsoft Excel files\n - A new Publication Page showcasing publications and authors that use UCSC Xena\n Our beta prototype for visualizing single-cell data delivers million-cell-scale multi-omics data for interactive visualization in a web browser.\n Citation Format: Mary Goldman, Brian Craft, Jingchun Zhu, David Haussler. Visualization and analysis of cancer genomics data using UCSC Xena [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2022; 2022 Apr 8-13. Philadelphia (PA): AACR; Cancer Res 2022;82(12_Suppl):Abstract nr 5039.", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "e1bf21ab4f577d660a11727ebc3ee76b17b41693", "title": "Static Inference Meets Deep learning: A Hybrid Type Inference Approach for Python", "abstract": "Type inference for dynamic programming languages such as Python is an important yet challenging task. Static type inference techniques can precisely infer variables with enough static constraints but are unable to handle variables with dynamic features. Deep learning (DL) based approaches are feature-agnostic, but they can-not guarantee the correctness of the predicted types. Their per-formance significantly depends on the quality of the training data (i.e., DL models perform poorly on some common types that rarely appear in the training dataset). It is interesting to note that the static and DL-based approaches offer complementary benefits. Un-fortunately, to our knowledge, precise type inference based on both static inference and neural predictions has not been exploited and remains an open challenge. In particular, it is hard to integrate DL models into the framework of rule-based static approaches. This paper fills the gap and proposes a hybrid type inference approach named Hityper based on both static inference and deep learning. Specifically, our key insight is to record type dependen-cies among variables in each function and encode the dependency information in type dependency graphs (TDGs). Based on TDGs, we can easily integrate type inference rules in the nodes to conduct static inference and type rejection rules to inspect the correctness of neural predictions. Hityper iteratively conducts static inference and DL-based prediction until the TDG is fully inferred. Experi-ments on two benchmark datasets show that Hityper outperforms state-of-the-art DL models by exactly matching 10% more human annotations. Hityper also achieves an increase of more than 30% on inferring rare types. Considering only the static part of Hityper, it infers 2\u00d7 ~3\u00d7 more types than existing static type inference tools. Moreover, Hityper successfully corrected seven wrong human an-notations in six GitHub projects, and two of them have already been approved by the repository owners.", "year": 2021, "citationCount": 39, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e2ffb1e2418476d0a6281a542cc3db5d266744d9", "title": "Fast and lightweight cell atlas approximations across organs and organisms", "abstract": "Omic technologies at single-cell resolution are reshaping our understanding of cellular diversity. The generation of cell atlases that capture the cellular composition of an entire individual is progressing rapidly. However, the science of organising and extracting information from these atlases is still in its infancy and for many biomedical researchers atlas exploration remains challenging. Here, we leveraged extensive experience in single-cell data analytics to pinpoint three major accessibility barriers to cell atlases, related to (i) programming skill or language, (ii) scalability, and (iii) dissemination standards. To help researchers overcome these barriers, we developed cell atlas approximations, a computational approach enabling the analysis of cell atlases across organs and organisms without programming skills, rapidly, and at scale. The web interface at https://atlasapprox.org facilitates the exploration of cell atlases in 19 species across the tree of life through a chatbot driven by frontend natural language processing. In parallel, application programming interfaces streamline data access for computational researchers and include specialised packages for Python, R, JavaScript, and Bash. Supported queries include marker gene identification, cross-organ comparisons, cell embeddings, gene sequences, searches for similar features, and bidirectional zoom between cell types and cell states. Most queries are answered in less than 1.5 seconds thanks to lossy data compression algorithms based on cell annotations and similarity graphs. Compared to traditional cell atlas analysis, this approach can reduce data size by more than 100 times and accelerate workflows by up to 100,000 times. Atlas approximations aim to make the exploration of cell atlases accessible to anyone in the world.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "e36bd1f5d0feb0c2e42c7c4ec22103f28da89784", "title": "scBOL: a universal cell type identification framework for single-cell and spatial transcriptomics data", "abstract": "Abstract Motivation Over the past decade, single-cell transcriptomic technologies have experienced remarkable advancements, enabling the simultaneous profiling of gene expressions across thousands of individual cells. Cell type identification plays an essential role in exploring tissue heterogeneity and characterizing cell state differences. With more and more well-annotated reference data becoming available, massive automatic identification methods have sprung up to simplify the annotation process on unlabeled target data by transferring the cell type knowledge. However, in practice, the target data often include some novel cell types that are not in the reference data. Most existing works usually classify these private cells as one generic \u2018unassigned\u2019 group and learn the features of known and novel cell types in a coupled way. They are susceptible to the potential batch effects and fail to explore the fine-grained semantic knowledge of novel cell types, thus hurting the model\u2019s discrimination ability. Additionally, emerging spatial transcriptomic technologies, such as in situ hybridization, sequencing and multiplexed imaging, present a novel challenge to current cell type identification strategies that predominantly neglect spatial organization. Consequently, it is imperative to develop a versatile method that can proficiently annotate single-cell transcriptomics data, encompassing both spatial and non-spatial dimensions. Results To address these issues, we propose a new, challenging yet realistic task called universal cell type identification for single-cell and spatial transcriptomics data. In this task, we aim to give semantic labels to target cells from known cell types and cluster labels to those from novel ones. To tackle this problem, instead of designing a suboptimal two-stage approach, we propose an end-to-end algorithm called scBOL from the perspective of Bipartite prototype alignment. Firstly, we identify the mutual nearest clusters in reference and target data as their potential common cell types. On this basis, we mine the cycle-consistent semantic anchor cells to build the intrinsic structure association between two data. Secondly, we design a neighbor-aware prototypical learning paradigm to strengthen the inter-cluster separability and intra-cluster compactness within each data, thereby inspiring the discriminative feature representations. Thirdly, driven by the semantic-aware prototypical learning framework, we can align the known cell types and separate the private cell types from them among reference and target data. Such an algorithm can be seamlessly applied to various data types modeled by different foundation models that can generate the embedding features for cells. Specifically, for non-spatial single-cell transcriptomics data, we use the autoencoder neural network to learn latent low-dimensional cell representations, and for spatial single-cell transcriptomics data, we apply the graph convolution network to capture molecular and spatial similarities of cells jointly. Extensive results on our carefully designed evaluation benchmarks demonstrate the superiority of scBOL over various state-of-the-art cell type identification methods. To our knowledge, we are the pioneers in presenting this pragmatic annotation task, as well as in devising a comprehensive algorithmic framework aimed at resolving this challenge across varied types of single-cell data. Finally, scBOL is implemented in Python using the Pytorch machine-learning library, and it is freely available at https://github.com/aimeeyaoyao/scBOL.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "e3b581cd563f7243531f9274c32ba2db18f5fdce", "title": "Design and evaluation of gradual typing for python", "abstract": "Combining static and dynamic typing within the same language offers clear benefits to programmers. It provides dynamic typing in situations that require rapid prototyping, heterogeneous data structures, and reflection, while supporting static typing when safety, modularity, and efficiency are primary concerns. Siek and Taha (2006) introduced an approach to combining static and dynamic typing in a fine-grained manner through the notion of type consistency in the static semantics and run-time casts in the dynamic semantics. However, many open questions remain regarding the semantics of gradually typed languages. In this paper we present Reticulated Python, a system for experimenting with gradual-typed dialects of Python. The dialects are syntactically identical to Python 3 but give static and dynamic semantics to the type annotations already present in Python 3. Reticulated Python consists of a typechecker and a source-to-source translator from Reticulated Python to Python 3. Using Reticulated Python, we evaluate a gradual type system and three approaches to the dynamic semantics of mutable objects: the traditional semantics based on Siek and Taha (2007) and Herman et al. (2007) and two new designs. We evaluate these designs in the context of several third-party Python programs.", "year": 2014, "citationCount": 134, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e3befd3aea28395ed9b16e15248b60d03e320849", "title": "Gradual Typing in an Open World", "abstract": "Gradual typing combines static and dynamic typing in the same language, offering the benefits of both to programmers. Static typing provides error detection and strong guarantees while dynamic typing enables rapid prototyping and flexible programming idioms. For programmers to fully take advantage of a gradual type system, however, they must be able to trust their type annotations, and so runtime checks must be performed at the boundaries of static and dynamic code to ensure that static types are respected. Higher order and mutable values cannot be completely checked at these boundaries, and so additional checks must be performed at their use sites. Traditionally, this has been achieved by installing wrappers or proxies on such values that moderate the flow of data between static and dynamic, but these can cause problems if the language supports comparison of object identity or has a foreign function interface. \nReticulated Python is a gradually typed variant of Python implemented via a source-to-source translator for Python 3. It implements a proxy-free alternative design named transient casts. This paper presents a formal semantics for transient casts and shows that not only are they sound, but they work in an open-world setting in which the Reticulated translator has only been applied to some of the program; the rest is untranslated Python. We formalize this open world soundness property and use Coq to prove that it holds for Anthill Python, a calculus that models Reticulated Python.", "year": 2016, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e40df9e7ff4b68c09434ccf9abd735fc409713b2", "title": "ESBMC-Python: A Bounded Model Checker for Python Programs", "abstract": "This paper introduces a tool for verifying Python programs, which, using type annotation and front-end processing, can harness the capabilities of a bounded model-checking (BMC) pipeline. It transforms an input program into an abstract syntax tree to infer and add type information. Then, it translates Python expressions and statements into an intermediate representation. Finally, it converts this description into formulae evaluated with satisfiability modulo theories (SMT) solvers. The proposed approach was realized with the efficient SMT-based bounded model checker (ESBMC), which resulted in a tool called ESBMC-Python, the first BMC-based Python-code verifier. Experimental results, with a test suite specifically developed for this purpose, showed its effectiveness, where successful and failed tests were correctly evaluated. Moreover, it found a real problem in the Ethereum Consensus Specification.", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e7dae5d4157de54b7cf29684fe387a74e500c67d", "title": "Programming Languages and Systems - 10th Asian Symposium, APLAS 2012, Kyoto, Japan, December 11-13, 2012. Proceedings", "abstract": null, "year": 2012, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e9a8dfe4e80c87f1940d41f5b895d4d20dee58b4", "title": "Planet Dynamic or: How I Learned to Stop Worrying and Love Reflection", "abstract": null, "year": 2012, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "ea1b62a03ab16c4acca6f7041c6f096bedec244b", "title": "Deep learning type inference", "abstract": "Dynamically typed languages such as JavaScript and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like TypeScript offer a middle-ground for JavaScript: a strict superset of JavaScript, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like JavaScript as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose DeepTyper, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. DeepTyper, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95% precision that could not be inferred without the aid of DeepTyper.", "year": 2018, "citationCount": 183, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "ec1eb2022483e7748cc36e3921d089b4a25b29a2", "title": "Variable-Arity Polymorphism T", "abstract": "Just as some functions have uniform behavior over distinct types, other functions have uniform behavior over distinct arities. These variable-arity functions are widely used in many languages, such as Scheme, Python, and other scripting languages. Statically typed languages, such as C and Java, accommodate modest forms of variable-arity \u201crest\u201d arguments via homogeneous data structures (plus casts). Even languages with highly expressive type systems such as ML or Haskell, however, cannot type check the typical variety of variable-arity functions found in untyped functional languages; instead their standard libraries contain numerous copies of the same function definition, with slightly different names. As part of the Typed Scheme project\u2014an effort to design an explicitly typed sister language for PLT Scheme\u2014we have designed, implemented, and evaluated a highly expressive type system for variable-arity functions. In order to make the type system convenient to use for Scheme programmers who wish to convert untyped modules, the implementation of the type system comes with a local type inference algorithm that restores types for polymorphic applications whenever possible. Our practical validation of the type system has confirmed for a small sample that the type system should now support most use cases in the extensive PLT Scheme code base. 1. Types for Variable-Arity Functions For the past two years, Tobin-Hochstadt and Felleisen (2006, 2008) have been developing Typed Scheme, an explicitly and statically typed sister language of PLT Scheme (Flatt 2008). In many cases, Typed Scheme accommodates existing Scheme programming idiom as much as possible. One remaining obstacle concerns functions of variable arity. Such functions have a long history in programming languages, especially in LISP and Scheme systems where they are widely used for a variety of purposes, ranging from arithmetic operations to list processing. In response we have revised Typed Scheme\u2019s type system so that it can cope variable-arity functions of many kinds. Some variadic functions in Scheme are quite simple. For example, the function + takes any number of numeric values and produces their sum. This function, and others like it, could be typed in a system that maps a variable number of arguments to a homogeneous data structure. A fair number of other variable-arity functions, however, demand a far more sophisticated approach than allocating rest arguments in homogeneous data structures. Consider the map function, which takes a function as input as well as an arbitrary number of lists. It then applies the function to the elements of the lists in a stepwise fashion. The function must therefore take precisely as many arguments as the number of lists provided. For example, if the make-student function takes two arguments, a name as a string and a number for a grade, then the expression 1 Such systems are found in C, C++, Java, C# and many other languages. (map make-student (list \"Al\" \"Bob\" \"Carol\") (list 87 98 64)) produces a list of students. We refer to variable-arity functions such as + and map as having uniform and non-uniform rest parameter types, respectively. For Typed Scheme to be useful to working programmers, its type system must handle all of these cases. Further, although map and + are part of the standard library, language implementors cannot arrogate the ability to abstract over the arities of functions. Scheme programmers routinely define such functions, and if any of them wish to refactor their Scheme programs into Typed Scheme, our language must allow such function definitions. Of course, our concerns are relevant beyond the confines of Scheme. Variable-arity functions are present in numerous other typed languages, such as C, C++, and Java. Even highly expressive typed functional languages offer such functions in their libraries, but do so via copying of code. For example, the SML Basis Library (Gansner and Reppy 2002) includes the ARRAY MAP and ARRAY MAP2 signatures, which could be abstracted into a single code component using our system. The GHC (The GHC Team 2008) standard library also features close to a dozen families of functions defined at a variety of arities, such as zipWith, zipWith3, zipWith4 and so on. In fact, the zipWith function, in full generality, simply is the standard map function from Scheme, as described above. This paper is organized as follows. In the next two sections, we describe Typed Scheme, first in general terms and then the type system for variable-arity functions; the examples involve the definition and use of such functions with both uniform and nonuniform rest parameter types. In section 4, we introduce a formal model of our variable-arity type system. In section 5, we describe how our innovations are integrated with the rest of Typed Scheme and sketch a local type inference algorithm. In section 6 we present some preliminary results concerning a comprehensive evaluation effort with respect to our code base and the limitations of our system. In section 7 we discuss related work, and in section 8 we conclude with a unifying perspective. 2. Typed Scheme . . . The goal of our Typed Scheme project is to design a typed sister language for an untyped scripting language so that programmers can transfer programs into the typed world on a module-by-module basis. Like PLT Scheme, Typed Scheme is a modular programming language; unlike plain Scheme programs, Typed Scheme programs come with explicit type annotations for functions and structures that are statically checked. Typed Scheme also provides integration with untyped PLT Scheme code, allowing a typed program to link in untyped modules and vice versa. The mechanism exploits functional contracts (Findler and Felleisen 2002) to guarantee a gen2 The full list of such functions is: zipWith, zip, unzip, liftA, liftM, typeOf, typeOfDefault, gcast and dataCast. eralized type soundness theorem (Tobin-Hochstadt and Felleisen 2006). Interlanguage refactoring can therefore proceed gradually. Typed Scheme supports this gradual refactoring with a type system that accommodates the standard Scheme programming idioms without (much) code modification. In principle, Scheme programmers need only annotate structure and function headers with types to move a module into the Typed Scheme world; on occasion, they may also have to define a type alias to keep type expressions concise. For this purpose, the type system combines true union types, recursive types, first-class polymorphic functions, and the novel discipline of occurrence typing. Additionally, Typed Scheme infers types for instantiations of polymorphic functions, based on locallyavailable type information. 2.1 Basic Typed Scheme Good Scheme programmers typically describe the structure of their data in comments, rather than in executable code. For example, a shape data type might be represented as: ;; A shape is either a rectangle or a circle (define-struct rectangle (l w)) (define-struct circle (r)) To accommodate this style in Typed Scheme, programmers can specify true, untagged unions of types: (define-type-alias shape ( S rectangle circle)) (define-struct: rectangle ([l : Integer] [w : Integer])) (define-struct: circle ([r : Integer])) Typed Scheme also supports explicit recursive types, which are necessary for typing uses of cons pairs in Scheme programs. This allows the specification of both fixed-length heterogeneous lists and arbitrary-length homogeneous lists, or even combinations of the two. Finally, Typed Scheme introduces occurrence typing, which allows the types of variable occurrences to depend on their position in the control flow. For example, the program fragment . . . (display \"Enter a number to double: \") (let ([val (read)]) ;; an arbitrary S-expression (if (number? val) (display (\u2217 2 val)) (display \"That wasn\u2019t a number!\"))) . . . type-checks correctly because the use of \u2217 is guarded by the number? check. 2.2 Polymorphic Functions and Local Type Inference Typed Scheme supports first-class polymorphic functions. For example, list-ref has the type (\u2200 (\u03b1) ((Listof \u03b1) Integer\u2192 \u03b1)). It can be defined in Typed Scheme as follows: (: list-ref (\u2200 (\u03b1) ((Listof \u03b1) Integer\u2192 \u03b1))) (define (list-ref l i) (cond [(not (pair? l)) (error \"empty list\")] [(= 0 i) (car l)] [else (list-ref (cdr l) (\u2212 i 1))])) The example illustrates two important aspects of polymorphism in Typed Scheme. First, the abstraction over types is explicit in the polymorphic type of list-ref but implicit in the function definition. Second, typical uses of polymorphic functions, e.g., car and listref , do not require explicitly type instantiation. Instead, the required type instantiations are synthesized from the types of the arguments. 3 Such functions are not always parametric, because occurrence typing can be used to examine the arguments. Argument type synthesis uses the local type inference algorithm of Pierce and Turner (2000). It greatly facilitates the use of polymorphic functions and makes conversions from Scheme to Typed Scheme convenient, while dealing with the subtyping present in the rest of the type system in an elegant manner. Furthermore, it ensures that type inference errors are always locally confined, rendering them reasonably comprehensible to programmers. 3. . . . with Variable-Arity Functions A Scheme programmer defines functions with lambda or define. Both syntactic forms permit fixed and variable-arity parameter specifications: 1. (lambda (x y z) (+ x (\u2217 y z))) creates a function of three arguments (and one result) and (define (f x y z) (+ x (\u2217 y z))) creates the same function and names it f ; 2. the function (lambda (x y . z) (+ x (apply max y z))) consumes at least two arguments and otherwise as many as needed; 3. (define (g x y . z) (+ x (apply max y z))) is another way of defining such a function; 4. (lambda x (apply + x)) creates a function of an arbitrary number of arguments; and 5. (define (h . x) (apply + x)) is the equi", "year": 2008, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "eca2188e812fde202104d929685270b21c7d13e7", "title": "Alignment of spatial transcriptomics data using diffeomorphic metric mapping", "abstract": "Spatial transcriptomics (ST) technologies enable high throughput gene expression characterization within thin tissue sections. However, comparing spatial observations across sections, samples, and technologies remains challenging. To address this challenge, we developed STalign to align ST datasets in a manner that accounts for partially matched tissue sections and other local non-linear distortions using diffeomorphic metric mapping. We apply STalign to align ST datasets within and across technologies as well as to align ST datasets to a 3D common coordinate framework. We show that STalign achieves high gene expression and cell-type correspondence across matched spatial locations that is significantly improved over landmark-based affine alignments. Applying STalign to align ST datasets of the mouse brain to the 3D common coordinate framework from the Allen Brain Atlas, we highlight how STalign can be used to lift over brain region annotations and enable the interrogation of compositional heterogeneity across anatomical structures. STalign is available as an open-source Python toolkit at https://github.com/JEFworks-Lab/STalign and as supplementary software with additional documentation and tutorials available at https://jef.works/STalign.", "year": 2023, "citationCount": 11, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "ecab4338c58ab7dea4e403854660191b96f6e238", "title": "Pygenomics: manipulating genomic intervals and data files in Python", "abstract": "Abstract Summary We present pygenomics, a Python package for working with genomic intervals and bioinformatic data files. The package implements interval operations, provides both API and CLI, and supports reading and writing data in widely used bioinformatic formats, including BAM, BED, GFF3, and VCF. The source code of pygenomics is provided with in-source documentation and type annotations and adheres to the functional programming paradigm. These features facilitate seamless integration of pygenomics routines into scripts and pipelines. The package is implemented in pure Python using its standard library only and contains the property-based testing framework. Comparison of pygenomics with other Python bioinformatic packages with relation to features and performance is presented. The performance comparison covers operations with genomic intervals, read alignments, and genomic variants and demonstrates that pygenomics is suitable for computationally effective analysis. Availability and implementation The source code is available at https://gitlab.com/gtamazian/pygenomics.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "f24462deda0002f0cfb6eaa035837d84be6ccf2c", "title": "Extreme Structure from Motion for Indoor Panoramas without Visual Overlaps [Supplementary Material]", "abstract": "We use two annotation tools to create annotations at the level of panoramas and at the level of houses/apartments. Both tools are implemented with PyQT5 and Python. Panorama-level Annotator: We used a modified version of PanoAnnotator [1] to annotate room type, room layout, and door/window bounding-boxes/segmentations. We fix the layout height to 3.2 and allow the specification of different object types. Figure 1 shows its screenshot.", "year": 2021, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "f400b6dd7f44cdf85fc79808b4fe3dd428c10106", "title": "Static Type Inference for Foreign Functions of Python", "abstract": "Static type inference is an effective way to maintain the safety of programs written in a dynamically typed language. However, foreign functions implemented in another programming language are often outside the inference range. Python, a popular dynamically typed language, has a lot of widely used packages which follow the multilingual structure with C/C++ extension modules. Existing deterministic Python static type inference tools which are not based on type annotations can do nothing about these foreign functions. In this paper, we propose a novel method to infer the type signature of foreign functions by analyzing implicit information in the layer of foreign function interface. We design a static type inference system, its evaluation on CPython, NumPy and Pillow shows that our method soundly infers the number and type of arguments for most foreign functions. Our results can further work as a complement to the state-of-the-art Python static type inference tool and enable it to analyze programs with foreign function calls. We catch 48 bugs of mismatch between foreign function declaration and its implementation, which make a parameter-free foreign function take argument of any type. 8 of the bugs we reported have been confirmed and fixed by communities.", "year": 2021, "citationCount": 10, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f4532ca8573955c239bdd6cf03af395cfb481aae", "title": "Probabilistic metabolite annotation using retention time prediction and meta-learned projections", "abstract": null, "year": 2022, "citationCount": 10, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "f4552b4c0778ae15a531fe7af24d6acf66ec2e9b", "title": "DDUO: General-Purpose Dynamic Analysis for Differential Privacy", "abstract": "Differential privacy enables general statistical analysis of data with formal guarantees of privacy protection at the individual level. Tools that assist data analysts with utilizing differential privacy have frequently taken the form of programming languages and libraries. However, many existing programming languages designed for compositional verification of differential privacy impose significant burden on the programmer (in the form of complex type annotations). Supplementary library support for privacy analysis built on top of existing general-purpose languages has been more usable, but incapable of pervasive end-to-end enforcement of sensitivity analysis and privacy composition. We introduce DDuo, a dynamic analysis for enforcing differential privacy. DDuo is usable by non-experts: its analysis is automatic and it requires no additional type annotations. DDuo can be implemented as a library for existing programming languages; we present a reference implementation in Python which features moderate runtime overheads on realistic workloads. We include support for several data types, distance metrics and operations which are commonly used in modern machine learning programs. We also provide initial support for tracking the sensitivity of data transformations in popular Python libraries for data analysis. We formalize the novel core of the DDuo system and prove it sound for sensitivity analysis via a logical relation for metric preservation. We also illustrate DDuo's usability and flexibility through various case studies which implement state-of-the-art machine learning algorithms.", "year": 2021, "citationCount": 9, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f57e41ac932caedd3c21861b8cd65d5f87eb9221", "title": "Genome-based characterization of two Colombian clinical Providencia rettgeri isolates co-harboring NDM-1, VIM-2, and other \u03b2-lactamases", "abstract": null, "year": 2020, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "f6867393c8648f3d40abda59812b8a8ff673b1d9", "title": "Automatic generation of library bindings using static analysis", "abstract": "High-level languages are growing in popularity. However, decades of C software development have produced large libraries of fast, time-tested, meritorious code that are impractical to recreate from scratch. Cross-language bindings can expose low-level C code to high-level languages. Unfortunately, writing bindings by hand is tedious and error-prone, while mainstream binding generators require extensive manual annotation or fail to offer the language features that users of modern languages have come to expect.\n We present an improved binding-generation strategy based on static analysis of unannotated library source code. We characterize three high-level idioms that are not uniquely expressible in C's low-level type system: array parameters, resource managers, and multiple return values. We describe a suite of interprocedural analyses that recover this high-level information, and we show how the results can be used in a binding generator for the Python programming language. In experiments with four large C libraries, we find that our approach avoids the mistakes characteristic of hand-written bindings while offering a level of Python integration unmatched by prior automated approaches. Among the thousands of functions in the public interfaces of these libraries, roughly 40% exhibit the behaviors detected by our static analyses.", "year": 2009, "citationCount": 23, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f78c654b8db7f5e18c53db42e9d683e9ce81749e", "title": "Proceedings for the 1st workshop on Script to Program Evolution", "abstract": "Recent years have seen increased interest in scripting languages, notably script to program migration/evolution and the interplay between typed and untyped code. \n \nScripting languages are lightweight, dynamic programming languages designed to maximize productivity by offering high-level abstractions and reducing the syntactic overhead found in most system's languages. The rising popularity of scripting languages such as Perl, Python, PHP, Tcl, JavaScript, Ruby, and Groovy have many underlying causes: they allow partial execution of programs, permitting easy unit testing, interactive experimentation, and even demoing of software at all times; their support for powerful and flexible high-level datatypes and dynamic typing admits quick interim solutions that can later be revised; etc. In short, scripting languages optimize development time rather than machine time, a good approach early in the software development life cycle. \n \nHowever, once the understanding of the system has reached a critical point and requirements have stabilized, scripting languages become less appealing. The compromises made to optimize development time make it harder to reason about program correctness, harder to do semantic-preserving refactorings, and harder to optimize execution speed. The lack of type information makes the code harder to navigate. \n \nThis situation often leads to a rewrite of a program in a less dynamic language, which may be costly and may introduce many bugs due to human error or semantic differences between the scripting language and the new target language (e.g., lack of garbage collection in C++, stricter type rules in Java, differences in available libraries, etc.). Sometimes only parts of the program are reimplemented, e.g., in C or assembler, as an optimization technique for a particularly computation-intensive method. Bridging a high-level scripting language to lower-level C introduces new opportunities for errors, possibly introduces platform-specific ties, and increases the number of languages a programmer must know to maintain the system. Both these approaches, especially the first one, have the downside of slowing down future development as they effectively preclude further use of the scripting language for prototyping new features. \n \nRecently, the concept of pluggable types has been proposed, which encourages the use of different optional type systems which do not have any effect on the run-time semantics of the programming language. It is believed that untyped scripts can be retrofitted to work with pluggable type systems (2), but with very few exceptions (i.e., Strongtalk), practical reports are yet to be found. Various ways of integrating or interfacing typed with untyped code have been proposed, like gradual typing, which allows for evolving script code to a more \"safe state\" more suitable for program analysis and compile-time optimisations. \n \nThe STOP workshop is interested in evolution of scripts in the sense of largely untyped pieces of code into safer programs, with more rigid structure and constrained behaviour through the use of gradual/hybrid/pluggable typing, optional contract checking, extensible languages, refactoring tools, and the like. The goal is to further the understanding of such systems in practise, and connect practise and theory. \n \nSTOP's subject areas have recently seen increased interest by several research groups, but there is no common understanding of how gradual/hybrid/pluggable typing, script to program evolution, typed to untyped evolution, etc. interplay, and there are few experience reports from extant languages that support pluggable types or optional type annotations (Cecil, CLOS, Dylan, PLT Scheme, Strongtalk, and others). This workshop aims to bring researchers together for passionate discussion about these topics, and to promote not only the theory, but practical evalution of these ideas, and experience reports.", "year": 2009, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f81a1b4510631d14b5b565c4701ee056f8d5c72f", "title": "CodePlan: Repository-level Coding using LLMs and Planning", "abstract": "\n Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as\n repository-level coding\n tasks. Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it. CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs. We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2\u201397 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines. CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them. We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan.\n", "year": 2023, "citationCount": 43, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f81af9b284efad8f6719f46dea034885ce3322c3", "title": "Information Extraction from Social Media: A Hands-on Tutorial on Tasks, Data, and Open Source Tools", "abstract": "Information extraction (IE) is a common sub-area of natural language processing that focuses on identifying structured data from unstructured data. One application domain of IE is Information Retrieval (IR), which relies on accurate and high-performance IE to retrieve high quality results from massive datasets. Another example of IE is to identify named entities in a text. For example, in the the sentence \"Katy Perry lives in the USA\", Katy Perry and USA are named entities of types of PERSON and LOCATION, respectively. Also, identify the sentiment expressed in a text is another instance of IE: in the sentence, \"This movie was awesome\", the expressed sentiment is positive. Finally, IE is concerned with identifying various linguistic aspects of text data, e.g., part of speech of words, noun phrases, dependency parses, etc., which can serve as features for additional IE tasks. This tutorial introduces participants to a) the usage of Python based, open-source tools that support IE from social media data (mainly Twitter), and b) best practices for ensuring the responsible use of IE and research data. Participants will learn and practice various lexical, semantic, and syntactic IE techniques that are commonly used for analyzing tweets. Participants will also be familiarized with the landscape of publicly available social media data (including popular NLP and IE benchmarks) and methods for collecting and preparing them for analysis. Furthermore, participants will be trained to use a suite of open source tools (SAIL for active learning, TwitterNER for named entity recognition, TweetNLP for transformer based NLP, and SocialMediaIE for multi task learning), which utilize advanced machine learning techniques (e.g., deep learning, active learning with human-in-the-loop, multi-lingual, and multi-task learning) to perform IE on their own or existing datasets. Participants will also learn how social contexts of text production and usage of results can be integrated into IE systems to improve these systems and to consider the role of time in improving social media IE quality. Finally, participants will learn about the governance of social media data for research purposes. The tools introduced in the tutorial will focus on the three main stages of IE, namely, collection of data (including annotation), data processing and analytics, and visualization of the extracted information. More details can be found at: https://socialmediaie.github.io/tutorials/", "year": 2022, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f8742581920a778e62bd7196dcc9be02c9914b76", "title": "OMEinfo: global geographic metadata for -omics experiments", "abstract": "Microbiome studies increasingly associate geographical features like rurality and climate types with microbiomes. However, microbiologists/bioinformaticians often struggle to access and integrate rich geographical metadata from sources such as GeoTIFFs; and inconsistent definitions of rurality, for example, can hinder cross-study comparisons. To address this, we present OMEinfo, a Python-based tool for automated retrieval of consistent geographical metadata from user-provided location data. OMEinfo leverages open data sources such as the Global Human Settlement Layer, K\u00f6ppen-Geiger climate classification models, and Open-Data Inventory for Anthropogenic Carbon dioxide, to ensure metadata accuracy and provenance. OMEinfo\u2019s Dash application enables users to visualise their sample metadata on an interactive map and to investigate the spatial distribution of metadata features, which is complemented by data visualisation to analyse patterns and trends in the geographical data before further analysis. The tool is available as a Docker container, providing a portable, lightweight solution for researchers. Through its standardised metadata retrieval approach and incorporation of FAIR and Open data principles, OMEinfo promotes reproducibility and consistency in microbiome metadata. To demonstrate its utility, OMEinfo is utilised to replicate the results of a previous study linking population density to soil sample alpha diversity. As the field continues to explore the relationship between microbiomes and geographical features, tools like OMEinfo will prove vital in developing a robust, accurate, and interconnected understanding of these interactions, whilst having applicability beyond this field to any studies utilising location-based metadata. Finally, we release the OMEinfo annotation dataset, a collection of 5.3 million OMEinfo annotated samples from the ENA, for use in a retrospective analysis of sequencing samples, and highlight a number of ways researchers and sequencing read repositories can improve the quality of underlying metadata submitted to these public stores. Availability OMEinfo is freely available and released under an MIT licence. OMEinfo source code is available at https://github.com/m-crown/OMEinfo/ Contact matthew.crown@northumbria.ac.uk, matthew.bashton@northumbria.ac.uk", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "f89def503972e5e3918184b293f7d8636133178d", "title": "Combining Static and Dynamic Typing in Ruby", "abstract": "Title of dissertation: Combining Static and Dynamic Typing in Ruby Michael Furr Doctor of Philosophy, 2009 Dissertation directed by: Professor Jeffrey S. Foster Department of Computer Science Many popular scripting languages such as Ruby, Python, and Perl are dynamically typed. Dynamic typing provides many advantages such as terse, flexible code and the ability to use highly dynamic language constructs, such as an eval method that evaluates a string as program text. However these dynamic features have traditionally obstructed static analyses leaving the programmer without the benefits of static typing, including early error detection and the documentation provided by type annotations. In this dissertation, we present Diamondback Ruby (DRuby), a tool that blends static and dynamic typing for Ruby. DRuby provides a type language that is rich enough to precisely type Ruby code, without unneeded complexity. DRuby uses static type inference to automatically discover type errors in Ruby programs and provides a type annotation language that serves as verified documentation of a method\u2019s behavior. When necessary, these annotations can be checked dynamically using runtime contracts. This allows statically and dynamically checked code to safely coexist, and any runtime errors are properly blamed on dynamic code. To handle dynamic features such as eval, DRuby includes a novel dynamic analysis and transformation that gathers per-application profiles of dynamic feature usage via a program\u2019s test suite. Based on these profiles, DRuby transforms the program before applying its type inference algorithm, enforcing type safety for dynamic constructs. By leveraging a program\u2019s test suite, our technique gives the programmer an easy to understand trade-off: the more dynamic features covered by their tests, the more static checking is achieved. We evaluated DRuby on a benchmark suite of sample Ruby programs. We found that our profile-guided analysis and type inference algorithms worked well, discovering several previously unknown type errors. Furthermore, our results give us insight into what kind of Ruby code programmers \u201cwant\u201d to write but is not easily amenable to traditional static typing. This dissertation shows that it is possible to effectively integrate static typing into Ruby without losing the feel of a dynamic language. Combining Static and Dynamic Typing in Ruby", "year": 2009, "citationCount": 11, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f8c1ab7be430a94ff24918db7d0f7a9250720b75", "title": "Straintables: An application that extracts sequences from genome assemblies and generates dissimilarity matrices", "abstract": "Background and Objectives The dissimilarity matrix (DM) is an important component of phylogenetic analysis, and many software packages exist to build and show DMs. However, as the common input for this type of software are sequences in FASTA file format, the process of extracting and aligning each set of sequences to produce a big number of matrices can be laborious. Additionally, existing software do not facilitate the comparison of clusters of similarity across several DMs built for the same group of individuals, using different genomic regions. To address our requirements of such a tool, we designed Straintables to extract specific genomic region sequences from a group of intraspecies genomic assemblies, using extracted sequences to build dissimilarity matrices. Methods A Python module with executable scripts was developed for a study on genetic diversity across strains of Toxoplasma gondii, being a general purpose system for DM calculation and visualization for preliminary phylogenetic studies. For automatic region sequence extraction from genomic assemblies we assembled a system that designs virtual primers using reference sequences located at genomic annotations, then matches those primers on genome files by using regex patterns. Extracted sequences are then aligned using Clustal Omega and compared to generate matrices. Results Using this software saves the user from manual preparation and alignment of the sequences, a process that can be laborious when a large number of assemblies or regions are involved. The automatic sequence extraction process can be checked against BLAST results using extracted sequence as queries, where correct results were observed for same-species pools for various organisms. The package also contains a matrix visualization tool focused on cluster visualization, capable of drawing matrices into image files with custom settings, and features methods of reordering matrices to facilitate the comparison of clustering patterns across two or more matrices. Conclusion Straintables may replace and extend the functionality of existing matrix-oriented phylogenetic software, featuring automatic region extraction from genomic assemblies and enhanced matrix visualization capabilities emphasizing cluster identification. This module is open source, available at GitHub (https://github.com/Gab0/straintables) under a MIT license and also as a PIPY package. Highlights Simple in-silico protocol for generation, visualization and comparison of dissimilarity matrices. Accurate automatic sequence extraction from multiple genomic assemblies by using virtual primers built from reference sequences in an annotation file. Draws matrices as images, with enhanced cluster visualization and customized options. Supports reordering of matrix indices to better visualize clustering pattern conservation across multiple regions.", "year": 2021, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "f977e13ee695a4944bbdedcb35d43e0483d20ff3", "title": "Epigenomic Signatures in Myelodysplastic Syndrome Patients As Predictors of Donor Compatibility and Transplant Outcome", "abstract": "Myelodysplastic syndromes (MDS) are a heterogeneous group of hematopoietic stem cell disorders for which allogeneic hematopoietic stem cell transplantation (HCT) is currently the only curative treatment. Epigenetic lesions are considered a major pathogenetic determinant in many cancers, including MDS, and combination epigenetic therapies have emerged. In this study, we hypothesize that interplay between key epigenomic signatures in the MDS patient undergoing HCT and their donor epigenomic profile serve as a prognostic factor of post-HCT MDS relapse risk.\n Reduced representation bisulfite sequencing (RRBS) was chosen to identify genome-wide epigenetic alterations as a cost-efficient method for building large data resources that reduces sequence redundancy and selects only CpG-rich regions of the genome for sequencing. A unique cohort of 188 samples from the Center for International Blood and Marrow Transplant Research (CIBMTR) biorepository was sequenced through RRBS. This cohort was composed of 94 pre-transplant samples from MDS patients that received peripheral blood stem cell grafts and were selected as case/controls for post-HCT relapse/non-relapse matched on patient, disease and transplant characteristics. The remaining 94 samples were from the patients' healthy allogeneic donors. Only patient samples that were wild-type for previously-identified MDS-prognostic TP53, RAS pathway and JAK2 mutations were included in this cohort to promote discovery of novel factors. We developed methylPrep, a Python application, to filter the low methylation calls and group shared sites by donors, relapsed patients and non-relapsed patients.\n We comprehensively identified differentially methylated regions (DMRs) by comparing the methylation patterns in healthy donors and MDS patients that relapsed or did not relapse. The healthy donor group displayed higher global methylation levels (GML) than the patient group as a whole, and the relapsed patient showed higher GML than the non-relapsed patient, though these differences were not statistically significant, and we continue to investigate whether hypo-methylating agents play a role. We selected high DMRs (50-bp interval), with at least 25% difference in methylation calls, using Fisher's exact test, where the threshold q-value equals 0.05, and uncovered 367 significant hyper-DMRs and 38 significant hypo-DMRs in donors compared to patients genome-wide. For disease relapsed versus non-relapsed MDS patients, we identified 121 hyper-DMRs and 64 hypo-DMRs, and the distribution of DMRs was highly varied. Furthermore, we compared epigenome compatibility between donors and patients who did or did not relapse after transplantation and discovered a distinct difference in DMR patterns from chromosome to chromosome and through region annotation. Interestingly, a higher number of DMRs were located in promoter regions between donors and non-relapsed patients versus donors and disease-relapsed patients.\n Identified DMRs, especially those located in promoter regions, may be involved in regulation of gene expression. These promoter DMRs may serve as candidate indicators or sites for potential diagnosis and therapy selection for MDS patients and may aid in the prediction of transplant outcomes and matching of the best donor for the MDS patient. Continued investigation will enable validation and assessment of the impact and mode of action for these distinct methylation signatures and global methylation patterns in MDS associated with HCT outcomes.\n Figure\n \n \n Nazha: Incyte: Speakers Bureau; Daiichi Sankyo: Consultancy; Jazz Pharmacutical: Research Funding; Novartis: Speakers Bureau; Tolero, Karyopharma: Honoraria; MEI: Other: Data monitoring Committee; Abbvie: Consultancy.\n", "year": 2019, "citationCount": 1, "fieldsOfStudy": ["Medicine"]}
{"paperId": "fab636ef521c60636b57e7c1c28f9eb0a6c4d756", "title": "ManyTypes4TypeScript: A Comprehensive TypeScript Dataset for Sequence-Based Type Inference", "abstract": "In this paper, we present ManyTypes4TypeScript, a very large corpus for training and evaluating machine-learning models for sequence-based type inference in TypeScript. The dataset includes over 9 million type annotations, across 13,953 projects and 539,571 files. The dataset is approximately 10x larger than analogous type inference datasets for Python, and is the largest available for Type-Script. We also provide API access to the dataset, which can be integrated into any tokenizer and used with any state-of-the-art sequence-based model. Finally, we provide analysis and performance results for state-of-the-art code-specific models, for baselining. ManyTypes4TypeScript is available on Huggingface, Zenodo, and CodeXGLUE.", "year": 2022, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "fb1b07e774abefcfa718db5d3298a9b1d7a1589c", "title": "CIA: a Cluster Independent Annotation method to investigate cell identities in scRNA-seq data", "abstract": "Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of the transcriptional landscape of complex tissues, enabling the discovery of novel cell types and biological functions. However, the identification and classification of cells from scRNA-seq datasets remain significant challenges. To address this, we developed a new computational tool called CIA (Cluster Independent Annotation), which accurately identifies cell types across different datasets without requiring a fully annotated reference dataset or complex machine learning processes. Based on predefined cell type signatures, CIA provides a highly user-friendly and practical solution to functional annotation of single cells. Our results demonstrate that CIA outperforms other state-of-the-art approaches, while also having significantly lower computational running time. Overall, CIA simplifies the process of obtaining reproducible signature-based cell assignments that can be easily interpreted through graphical summaries providing researchers with a powerful tool to explore the complex transcriptional landscape of single cells. The CIA framework is implemented in both the Python and R programming languages, making it applicable to all main single-cell analysis frameworks, and it is available under the MIT license with its documentation at the following links: Python package: https://pypi.org/project/cia-python/ Python tutorial: https://cia-python.readthedocs.io/en/latest/tutorial/Cluster_Independent_Annotation.html R package and tutorial: https://github.com/ingmbioinfo/CIA_R", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "fb5402a401a991379a866ac55df9ed2179a32cad", "title": "Crowdsourcing Subjective Annotations Using Pairwise Comparisons Reduces Bias and Error Compared to the Majority-vote Method", "abstract": "How to better reduce measurement variability and bias introduced by subjectivity in crowdsourced labelling remains an open question. We introduce a theoretical framework for understanding how random error and measurement bias enter into crowdsourced annotations of subjective constructs. We then propose a pipeline that combines pairwise comparison labelling with Elo scoring, and demonstrate that it outperforms the ubiquitous majority-voting method in reducing both types of measurement error. To assess the performance of the labelling approaches, we constructed an agent-based model of crowdsourced labelling that lets us introduce different types of subjectivity into the tasks. We find that under most conditions with task subjectivity, the comparison approach produced higher f1 scores. Further, the comparison approach is less susceptible to inflating bias, which majority voting tends to do. To facilitate applications, we show with simulated and real-world data that the number of required random comparisons for the same classification accuracy scales log-linearly O(N log N) with the number of labelled items. We also implemented the Elo system as an open-source Python package.", "year": 2023, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "fc0ae9ef42e1fcb994947808f1dd40bd26fa0368", "title": "Automated assignment of cell identity from single-cell multiplexed imaging and proteomic data", "abstract": "The creation of scalable single-cell and highly-multiplexed imaging technologies that profile the protein expression and phosphorylation status of heterogeneous cellular populations has led to multiple insights into disease processes including cancer initiation and progression. A major analytical challenge in interpreting the resulting data is the assignment of cells to a priori known cell types in a robust and interpretable manner. Existing approaches typically solve this by clustering cells followed by manual annotation of individual clusters or by strategies that gate protein expression at predefined thresholds. However, these often require several subjective analysis choices such as selecting the number of clusters and do not automatically assign cell types in line with prior biological knowledge. They further lack the ability to explicitly assign cells to an unknown or uncharacterized type, which exist in most highly multiplexed imaging experiments due to the limited number of markers quantified. To address these issues we present Astir, a probabilistic model to assign cells to cell types by integrating prior knowledge of marker proteins. Astir uses deep recognition neural networks for fast Bayesian inference, allowing for cell type annotations at the million-cell scale and in the absence of previously annotated reference data across multiple experimental modalities and antibody panels. We demonstrate that Astir outperforms existing approaches in terms of accuracy and robustness by applying it to over 2.1 million single cells from several suspension and imaging mass cytometry and microscopy datasets in multiple tissue contexts. We further showcase that Astir can be used for the fast analysis of the spatial architecture of the tumour microenvironment, automatically quantifying the immune influx and spatial heterogeneity of patient samples. Astir is freely available as an open source Python package at https://www.github.com/camlab-bioml/astir.", "year": 2021, "citationCount": 33, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "fc9bda6658f6c2ebb83e81f83ab90f5363905e63", "title": "Safe-DS: A Domain Specific Language to Make Data Science Safe", "abstract": "Due to the long runtime of Data Science (DS) pipelines, even small programming mistakes can be very costly, if they are not detected statically. However, even basic static type checking of DS pipelines is difficult because most are written in Python. Static typing is available in Python only via external linters. These require static type annotations for parameters or results of functions, which many DS libraries do not provide.In this paper, we show how the wealth of Python DS libraries can be used in a statically safe way via Safe-DS, a domain specific language (DSL) for DS. Safe-DS catches conventional type errors plus errors related to range restrictions, data manipulation, and call order of functions, going well beyond the abilities of current Python linters. Python libraries are integrated into Safe-DS via a stub language for specifying the interface of its declarations, and an API-Editor that is able to extract type information from the code and documentation of Python libraries, and automatically generate suitable stubs.Moreover, Safe-DS complements textual DS pipelines with a graphical representation that eases safe development by preventing syntax errors. The seamless synchronization of textual and graphic view lets developers always choose the one best suited for their skills and current task.We think that Safe-DS can make DS development easier, faster, and more reliable, significantly reducing development costs.", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "fdb5db4a3d331fe8bb186eb34c25f9991a305c90", "title": "Pytometry: Flow and mass cytometry analytics in Python", "abstract": "Flow and mass cytometry data are commonly analyzed via manual gating strategies which requires prior knowledge, expertise and time. With increasingly complex experiments with many parameters and samples, traditional manual flow and mass cytometry data analysis becomes cumbersome if not inefficient. At the same time, computational tools developed for the analysis of single-cell RNA-sequencing data have made single cell genomics analysis highly efficient, yet they are mostly inaccessible for the analysis of flow and mass cytometry data due to different data formats, noise assumptions and scales. To bring the advantages of both fields together, we developed Pytometry as an extension to the popular scanpy framework for the analysis of flow and mass cytometry data. We showcase a standard analysis workflow on healthy human bone marrow data, illustrating the applicability of tools developed for the larger feature space of single cell genomics data. Pytometry combines joint analysis of multiple samples and advanced computational applications, ranging from automated pre-processing, cell type annotation and disease classification.", "year": 2022, "citationCount": 6, "fieldsOfStudy": ["Biology"]}
{"paperId": "fe80e35fa61b2d4b674a40338dc1fc655e074050", "title": "Interact: Automated analysis of protein-ligand interactions by 1D and 2D NMR", "abstract": "NMR titration experiments contain rich information on the thermodynamic, kinetic and structural aspects of protein-ligand interactions. Automated tools are required to process the large number of signals typically acquired in these experiments and facilitate quantitative interpretations. We present Interact, a Python script accessible within the Bruker BioSpin TopSpin\u2122 software, which allows automated analysis of both 1D and 2D NMR titration experiments. Interact performs peak picking and annotation of the successive spectra and supports quantitative interpretation of changes in chemical shifts and linewidths induced by the ligand (e.g. to estimate dissociation constants) through different fitting procedures. Interact can be applied to all types of 1D and 2D NMR experiments and all nuclei, hence facilitating routine analysis of existing and forthcoming NMR titration data. Interact was implemented in Python and can be used on Windows, Unix and MacOS platforms. The source code is distributed under OpenSource license at http://github.com/MetaSys-LISBP/Interact.", "year": 2018, "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Biology"]}
{"paperId": "fea50e8367e508588a0c222f63991bbb966c9707", "title": "Collaborative Metadata Definition using Controlled Vocabularies, and Ontologies", "abstract": "Data's role in a variety of technical and research areas is undeniably growing. This can be seen, for example, in the increased investments in the development of data-intensive analytical methods such as artificial intelligence (Zhang 2022), as well as in the rising rate of data generation which is expected to continue into the near future (Rydning and Shirer 2021). Academic research is one of the areas, where data is the lifeblood of generating hypotheses, creating new knowledge, and reporting results. Unlike proprietary industry data, academic research data is often subjected to stricter requirements regarding transparency, and accessibility. This is in part due to the public funding which many research institutions receive. One way to fulfil these requirements is by observing the FAIR (Findability, Accessibility, Interoperability, Reusability) principles for scientific data (Wilkinson et al. 2016). These introduce a variety of benefits, such as increased research reproducibility, a more transparent use of public funding, and environmental sustainability. A way of implementing the FAIR principles in practice is with the help of FAIR Digital Objects (FDOs) (European Commission: Directorate-General for Research and Innovation 2018). A FDO consists of data, an accompanying Persistent Identifier (PID), and rich metadata which describes the context of the data. Additionally, the data format contained in an FDO should be widely used, and ideally open. Our presentation is focused on the third of FDO's components mentioned previously \u2013 metadata. It outlines the concept for a framework which enables the collaborative definition of metadata fields which can be used to annotate FDO-encapsulated data for a given domain of research.\n The first component of the presented framework is a controlled vocabulary of the domain related to the data which needs to be annotated. A controlled vocabulary is a collective that denotes a controlled list of terms, their definitions, and the relations between them. In the framework presented in this contribution, the terms correspond to the metadata fields used in the data annotation process. Formally, the type of controlled vocabularies used in the framework is a thesaurus (National Information Standards Organization 2010). Thesauri consist not only of the elements mentioned previously, but also allow for the inclusion of synonyms for every defined term. This eliminates the ambiguity which can occur when using terms with similar definitions. Additionally, thesauri specify simple hierarchical relations between the terms in the vocabulary, which can provide an explicit structure to the set of defined metadata fields. The most important feature of our framework, however, is that the controlled vocabularies can be developed in a collaborative fashion by the domain experts of a given research field. Specifically, people are able to propose term definitions and edits, as well as cast votes on the appropriateness of terms which have already been proposed.\n Despite their advantages, one limit of thesauri is their lacking capability of relating metadata fields to each other in a more semantically rich fashion. This motivated the use of the second component of the framework, namely ontologies. An ontology can be defined as \u201ca specification of a conceptualization\u201d (Gruber 1995). More precisely, it is a data structure which represents entities in a given domain, as well as various relations between them. After a set of metadata fields has been defined within a controlled vocabulary, that vocabulary can be transformed into an ontology which contains additional relations between the fields. These can extend beyond the hierarchical structure of a thesaurus and can contain domain-specific information about the metadata fields. For example, one such relation can denote the data type of the value which a given field must take. Furthermore, ontologies can be used to link not only metadata, but also data, as well as individual FDOs themselves. This can contribute to the Reusability aspect of FAIR Data Objects. For example, an FDO generated by a research group in a given domain can be linked to an existing domain ontology. Afterwards, the FDO can be reused more easily by researchers from the same scientific field, because the ontology will have already specified the FDO's relation to the subject area. Additionally, cross-domain ontologies can be combined with each other which can increase the reusability of FDOs beyond their domain boundaries.\n The components described above are being implemented in the form of multiple software tools related to the framework. The first one, a controlled vocabulary editor written as a Python-based web application called VocPopuli, is the entry point for domain experts who want to develop a metadata vocabulary for their field of research or lab. The software, whose first version is already being tested internally, enables the collaborative definition, and editing of metadata terms. Additionally, it annotates each term, as well as the entire vocabulary, with the help of the PROV Data Model (PROV-DM) (Moreau and Missier 2013) - a schema used to describe the provenance of a given object. Finally, it assigns a PID to each term in the vocabulary, as well as the vocabulary itself. It is worth noting that the generated vocabularies themselves can be seen through the prism of FDOs: they contain data (the defined terms) which is annotated with metadata (e.g., the terms' authors) and provided with a PID.\n The second software solution will facilitate the transformation of the vocabularies developed with the help of VocPopuli into ontologies. It will handle two distinct use cases \u2013 the from-scratch conversion of vocabularies into ontologies, and the augmentation of existing ontologies with the terms from a given thesaurus. As is the case with VocPopuli, the second tool is being developed in the Python programming language. The software solutions will be finally tested by two semi-overlapping groups of users from materials science. On the one hand, domain experts will input, edit, and discuss vocabulary terms in their area of interest, and thus create vocabularies. On the other hand, vocabulary and ontology administrators will oversee the vocabulary creation, and ontology transformation processes in a semi-automatic fashion.\n After development is complete, the tools will be used in the creation of controlled vocabularies for various experimental procedures, as well as their transformation and/or integration into semantically richer ontologies. This will augment our already published work in the area (Garabedian et al. 2022) and will thereby test the integration of the new framework with already existing resources. The new vocabularies will describe processes in multiple domains, such as materials science, tribology, and metalworking. Afterwards, the developed thesauri will be used in the creation of metadata templates which can be used to annotate experimental data generated in the procedures mentioned above.", "year": 2022, "citationCount": 2, "fieldsOfStudy": null}
{"paperId": "008e160ba46f49bb0513875525103d0a0bb902b5", "title": "Developing a Comprehensive Framework for Multimodal Feature Extraction", "year": 2017, "citationCount": 45, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0140f2155376a20a06d6c7cd698b0ccd8bec136a", "title": "Database Creator for Mass Analysis of Peptides and Proteins, DC-MAPP: A Standalone Tool for Simplifying Manual Analysis of Mass Spectral Data to Identify Peptide/Protein Sequences.", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "025c576a8afa49a72bcfd2137e2ac5e4a1271c3a", "title": "SNPAAMapper-Python: A highly efficient genome-wide SNP variant analysis pipeline for Next-Generation Sequencing data", "year": 2022, "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "031c94ddfab775a727474f5cd55daaddde799e0d", "title": "ChocoPy: a programming language for compilers courses", "year": 2019, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "050820fe1062722723f4b759a154021f8ef91502", "title": "Physical Type Tracking through Minimal Source-Code Annotation", "year": 2014, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "062b7d793fe46ca48af5327715d63db9a17898c5", "title": "OpenAnnotateApi: Python and R packages to efficiently annotate and analyze chromatin accessibility of genomic regions", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "072fa0d5f856a6931b530e954fb57b362438f916", "title": "GOGrapher: A Python library for GO graph representation and analysis", "year": 2009, "citationCount": 13, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "07d7305f09d843b8fba7552fa8104b8f188f3f38", "title": "DLInfer: Deep Learning with Static Slicing for Python Type Inference", "year": 2023, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "08286aeb9c9c99b4b4767167ac9d35dc31151068", "title": "multiplierz: an extensible API based desktop environment for proteomics data analysis", "year": 2009, "citationCount": 76, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "08e97a1a373bb9abccff94d02e3d2cd568559b22", "title": "Multifaceted quality assessment of gene repertoire annotation with OMArk", "year": 2022, "citationCount": 10, "fieldsOfStudy": ["Biology"]}
{"paperId": "09ae0f50b62330fb5d5fe13f76b855eb8bcc67e5", "title": "The evolution of type annotations in python: an empirical study", "year": 2022, "citationCount": 10, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0adaf1037dd7ae4749c4a67198e687fc79443cca", "title": "Mining of extended signal temporal logic specifications with ParetoLib 2.0", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0b11c38a6d938916c840e8e5576f206770fd0588", "title": "Enhancing RDM in Galaxy by integrating RO-Crate", "year": 2022, "citationCount": 3, "fieldsOfStudy": null}
{"paperId": "0b7509abcf9af50ad4e551be35f11b62ce7d2695", "title": "TypeT5: Seq2seq Type Inference using Static Analysis", "year": 2023, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0bc844245ec79ea9d036add3474504d45ef9944d", "title": "Towards a Large-Scale Empirical Study of Python Static Type Annotations", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0d5b2c2fa1b905e66440648a8f571495b508f576", "title": "MSIpred: a python package for tumor microsatellite instability classification from tumor mutation annotation data using a support vector machine", "year": 2018, "citationCount": 45, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "0e33e862d7ba29de60425fef3038824c108ad01f", "title": "Deep audio embeddings for vocalisation clustering", "year": 2023, "citationCount": 10, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "0eb1e2554d58d8cfb5f2cc53e9d32fe446d7dd15", "title": "scATAcat: Cell-type annotation for scATAC-seq data", "year": 2024, "citationCount": 1, "fieldsOfStudy": ["Biology"]}
{"paperId": "0efe1aa1ac10f916ee7f90ad8da0114c3181ffb8", "title": "PanGraphViewer: A Versatile Tool to Visualize Pangenome Graphs", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Biology"]}
{"paperId": "0f2ec436de5a3a6a9a817378f918f029972733fb", "title": "Test-Case Generation for Finding Neural Network Bugs", "year": 2021, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "0fc9c8681543bead57099426f72eaeb8fa77aed3", "title": "EagerPy: Writing Code That Works Natively with PyTorch, TensorFlow, JAX, and NumPy", "year": 2020, "citationCount": 6, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "10baebd2264a34b9d14428bcb6baa73fece71eb0", "title": "Where to Start: Studying Type Annotation Practices in Python", "year": 2021, "citationCount": 8, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "115b9c3ed8310f33a1e9f338dce935a620cd4c80", "title": "Type-Based Gradual Typing Performance Optimization", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1164269ea18402f44da2b33ed8571dbef9f59b65", "title": "Casts and costs: harmonizing safety and performance in gradual typing", "year": 2018, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "13c6ad940b0f0298ee7f437d8239ccbca6b240ed", "title": "ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference", "year": 2021, "citationCount": 21, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "14e58291f06db7769a51d20cd336afab2b2e30d8", "title": "The Tanl Pipeline", "year": 2010, "citationCount": 30, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1531e36a20ef3788cf07c318cebd34851c4e17da", "title": "The L3Pilot Common Data Format - Enabling Efficient Automated Driving Data Analysis", "year": 2019, "citationCount": 11, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "15a850e657d16e65a742d219dcff21e4e059ea6d", "title": "Analysis of the mRNA export protein ZC3H11A in HCMV infection and pan-cancer", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "16a47e17fa6e4842332423779c4a77be85972242", "title": "RNAvigate: efficient exploration of RNA chemical probing datasets", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "174adcc7d1df3112c3234d80ecd6d49c14fe6056", "title": "impunity: Enforcing Physical Unit Consistency at Definition Time in Python", "year": 2023, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "180bdbcfa27f50b5b2d959b46b0e95b2324c602d", "title": "Reconstruction of neuronal activity and connectivity patterns in the zebrafish olfactory bulb", "year": 2016, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "182c333597ddcdfd2d9c6384af41c9495eeda6a4", "title": "Genome-based characterization of two Colombian clinical Providencia rettgeri isolates co-harboring NDM-1, VIM-2, and other \u03b2-lactamases", "year": 2020, "citationCount": 10, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "18cf8201acdecfd67259070bd79ffe892306ba7c", "title": "Variant Library Annotation Tool (VaLiAnT): an oligonucleotide library design and annotation tool for saturation genome editing and other deep mutational scanning experiments", "year": 2021, "citationCount": 5, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "1b50bcdaf6ddbe87fda34300a9331f5cabb188d1", "title": "An automated model annotation system (AMAS) for SBML models", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "1c1f4d0b49049fd138cd51f3c0f880e691551fa0", "title": "TypeEvalPy: A Micro-Benchmarking Framework for Python Type Inference Tools", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1c5147e3eb4fc9f27303f5e036fa8b2763585cc7", "title": "Static Type Analysis for Python", "year": 2014, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1c580446b38e3765a5b120109ba60af8fc9b0548", "title": "Position Paper : Dynamically Inferred Types for Dynamic Languages", "year": 2010, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "1de53160d48ef6ec31559ee79e21afabc4b12469", "title": "Systematic tissue annotations of genomics samples by modeling unstructured metadata", "year": 2022, "citationCount": 7, "fieldsOfStudy": ["Medicine"]}
{"paperId": "1e0f707a9ad066664264682078bb1aa25d6f7c74", "title": "QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "1e655fa69c62b430b051224153f701f1b607fd9c", "title": "Typilus: neural type hints", "year": 2020, "citationCount": 111, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "20083c79a078b64d4dfe07b4514309ff5cd77581", "title": "Annotation and quality assessment of left ventricular filling and relaxation pattern using one-dimensional convolutional neural network", "year": 2022, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "220c91ee7634ec51a9b1d14a7816c18cf113c6bc", "title": "Refinement type contracts for verification of scientific investigative software", "year": 2019, "citationCount": 4, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "2270410d1be75b205715cca5b1743d8250223176", "title": "MACA: Marker-based automatic cell-type annotation for single cell expression data", "year": 2021, "citationCount": 8, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"]}
{"paperId": "22a928de63ee3dc069c6b08d0bd34f89328f68c9", "title": "Evaluating importance of edge types when using graph neural network for predicting return types of Python functions", "year": 2020, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "22dc74368c04b983a0494e5fc07da78916145ccf", "title": "Contrastive Learning for Robust Cell Annotation and Representation from Single-Cell Transcriptomics", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "22f44225b88b72c5786d2c8148e9aec1a71d58ef", "title": "An Automated Model Annotation System (AMAS) for SBML Models", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "234de17e61c8d5ee4636fd9407eb9777ad1d506c", "title": "Abstract 4956: A fast and efficient bioinformatics analysis workflow for processing reads from single-cell multiomics assays captured on a microwell-based platform", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "2352b3a6e4adc7b010ee5de424079480e3afbfbf", "title": "Goal-driven Answers in the Cards Dialogue Corpus", "year": 2012, "citationCount": 39, "fieldsOfStudy": null}
{"paperId": "23ff29e7a88af7c58fdf4ca18d56d8d57b3a3345", "title": "PyTy: Repairing Static Type Errors in Python", "year": 2024, "citationCount": 6, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "259ace68b5b776e9e8a3c756eb34b95cb723fdf8", "title": "Abstract 2066: Visualization and analysis of cancer genomics data using UCSC Xena", "year": 2023, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "26383131a0feb343c2b121a2fe00905f5363d5ae", "title": "Advanced Graph-Based Deep Learning for Probabilistic Type Inference", "year": 2020, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "27a118ef5bce2fc62b97f1c537fa1e31e1f80587", "title": "Julia: dynamism and performance reconciled by design", "year": 2018, "citationCount": 72, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "29fc7ff02e68537a86d32978bfd2e8e45258e77d", "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks", "year": 2020, "citationCount": 102, "fieldsOfStudy": ["Computer Science", "Mathematics"]}
{"paperId": "2bbe7c182af82c09005c32e6692a7de808b48cc3", "title": "Snek: Overloading Python Semantics via Virtualization", "year": 2019, "citationCount": 2, "fieldsOfStudy": null}
{"paperId": "2bd9b519cb5e6bfe448ba079077f7c14822d5260", "title": "Understanding How CodeLLMs (Mis)Predict Types with Activation Steering", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "2c515e8fd566f84670a94620eee16649a8920b6d", "title": "Getting the Roles Right: Using FrameNet in NLP", "year": 2015, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "2f70ef1fa023b087ab266b368a3430b51e6621bd", "title": "Data Fusion on the CANDELA Cloud Platform", "year": 2020, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "30b7b1adb6b67ad84183400e1375dc073cdcf1ac", "title": "preon: Fast and accurate entity normalization for drug names and cancer types in precision oncology", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Medicine", "Biology", "Computer Science"]}
{"paperId": "34a1239b736d03d9661089caf4c332335822302a", "title": "PYInfer: Deep Learning Semantic Type Inference for Python Variables", "year": 2021, "citationCount": 14, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "3698f2b8e555e8027f09a1140a0fe5942cc5c94b", "title": "A multi-organ transcriptome resource for the Burmese Python (Python molurus bivittatus)", "year": 2011, "citationCount": 21, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "36d36cbfd2342fd35c57da91be4a2f7c39f6b7ce", "title": "Static Type Recommendation for Python", "year": 2022, "citationCount": 4, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "3a601de3f056e7d82006fe77943ab259b181ce75", "title": "Platform for Analysis and Labeling of Medical Time Series", "year": 2020, "citationCount": 10, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "3ab2ba66e642217f59a0f77984116d0022e69e40", "title": "RNAvigate: Efficient exploration of RNA chemical probing datasets", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "3b97850da6b4bd4985a3e7e89edbdc9a8b33b0cf", "title": "Root System Markup Language: Toward a Unified Root Architecture Description Language1[OPEN]", "year": 2015, "citationCount": 100, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "3c623857dae37f08e16564f2abf57a60a478cc8e", "title": "Linking big biomedical datasets to modular analysis with Portable Encapsulated Projects", "year": 2020, "citationCount": 14, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "3e01427c091bcdc6ded313be87e35e512776c31e", "title": "\u00c6THEL: Automatically Extracted Typelogical Derivations for Dutch", "year": 2019, "citationCount": 7, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "3e1560e86606490ba294b4d21eb32e955b2a8b24", "title": "scDIOR: single cell RNA-seq data IO software", "year": 2022, "citationCount": 11, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "3e42f47ed61d4baba727c19ec0338b5a4f2761a2", "title": "An Empirical Study of Type-Related Defects in Python Projects", "year": 2022, "citationCount": 11, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "40d8d6243149844fb8987fe7faa5300c2e51b083", "title": "The DBCLS BioHackathon : standardization and interoperability for bioinformatics web services and workflows", "year": 2010, "citationCount": 30, "fieldsOfStudy": null}
{"paperId": "40f3de86086a4e938ad977f83f6c1a2eab9d0a2e", "title": "CIViCutils: Matching and downstream processing of clinical annotations from CIViC", "year": 2023, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "42db68957509e83e97f5628c6a99f7a54b324902", "title": "Sifter-T: Um framework escal\u00e1vel para anota\u00e7\u00e3o filogen\u00f4mica probabil\u00edstica funcional de dom\u00ednios prot\u00e9icos", "year": 2013, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "43039ecb64821b6c4d0bb1c80b04cdcd882eb4ab", "title": "Posters with Abstracts J : Methods and technologies for computational biology", "year": 2014, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "44286fe08a2d14560f8035d0a32cc4eb762d624b", "title": "Application of high performance compute technology in bioinformatics", "year": 2019, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "4497f6f93bf9e0ec901a8ce04c09fa8162e17c6b", "title": "An Integrated Framework for Analysis and Prediction of Impact of Single Nucleotide Polymorphism Associated with Human Diseases", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "44cc31e59a87b438c64241a25e7a4e7bcd4fba09", "title": "Abstract 1932: Pollock: Fishing for cell states", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "4574654e5c54f174a651ff5565eea64868c73084", "title": "A Guided Tour of a Static Analyzer for Data Science Software", "year": 2020, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "4613229aff389bf0330cd63efab3733ec12b73e5", "title": "Pgltools: a genomic arithmetic tool suite for manipulation of Hi-C peak and other chromatin interaction data", "year": 2017, "citationCount": 35, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "463dc5bb6412504eab29192ec77477147e9d31f7", "title": "\u0412\u044b\u0432\u043e\u0434 \u0442\u0438\u043f\u043e\u0432 \u0434\u043b\u044f \u044f\u0437\u044b\u043a\u0430 Python", "year": 2013, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "4749aed8228b0bfc7c2f81adfbc3b3e238b2b914", "title": "NGScloud2: optimized bioinformatic analysis using Amazon Web Services", "year": 2020, "citationCount": 4, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"]}
{"paperId": "47adf2557630323c762dffbd375169662a374e37", "title": "SLACC: Simion-based Language Agnostic Code Clones", "year": 2020, "citationCount": 23, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "47bb33db2ede899c232aace0d7a6b5d7bf843a7e", "title": "Orchid: a novel management, annotation and machine learning framework for analyzing cancer mutations", "year": 2018, "citationCount": 15, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "47ee815259beefcf87ad73bc6bab107a50c1a8a4", "title": "Automated Return Type Annotation for Python", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "481eaa183680deddb020b9fc7543072d6fb4e124", "title": "Variable Domain-specific Software Languages with DjDSL: Design and Implementation", "year": 2020, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "48395a87e86b824fcf285030424599d9fba123ed", "title": "Development of a Protein Conservation Analysis Pipeline and Application to Carbonic Anhydrase IV", "year": 2013, "citationCount": 2, "fieldsOfStudy": ["Chemistry"]}
{"paperId": "48d14a3c23851730a1b0e9470713d75cefab0ffe", "title": "Comprehensive annotations of the mutational spectra of SARS\u2010CoV\u20102 spike protein: a fast and accurate pipeline", "year": 2020, "citationCount": 35, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "4a6f68abc7dcc81691887e8f12c22c6474698423", "title": "Abstract LB548: Genomic analysis of antineoplastic drug-related genetic variations based on large-scale population sequencing", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "4dd54f3a0276774e3a7b792cdb59a1fc61937190", "title": "Efficient searching and annotation of metabolic networks using chemical similarity", "year": 2015, "citationCount": 50, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "4e244b4d58cd223fb99cfb9363741f4bad772e54", "title": "Automated annotation of rare-cell types from single-cell RNA-sequencing data through synthetic oversampling", "year": 2021, "citationCount": 9, "fieldsOfStudy": ["Medicine", "Computer Science", "Biology"]}
{"paperId": "50021ec4c1fd676f9f56a74a6caf529d89dd02bb", "title": "A Framework for Collaborative Curation of Neuroscientific Literature", "year": 2017, "citationCount": 9, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "500d69558cad3ca286ce91b9ef8d50a96991e96e", "title": "Cellenium\u2014a scalable and interactive visual analytics app for exploring multimodal single-cell data", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "50b51e5064edef8347f573cc50dede952994bdd4", "title": "Scalable taint specification inference with big code", "year": 2019, "citationCount": 28, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "52481cc394f5aed25410f970a58ad1dd8683f9f2", "title": "STalign: Alignment of spatial transcriptomics data using diffeomorphic metric mapping", "year": 2023, "citationCount": 15, "fieldsOfStudy": ["Medicine"]}
{"paperId": "5357b6b45ed0c60c12389dc6e59fa4f31256e2db", "title": "Gradvis typat Python med Cython.", "year": 2011, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "54cd1a33a978ec279f46a671a13c7883f8a706c4", "title": "Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs", "year": 2018, "citationCount": 82, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "56175ff48db1e4f21063749f39264097ef784d5c", "title": "Python Programmers have GPUs too Automatic Python Loop Parallelization with Staged Dependence Analysis", "year": 2019, "citationCount": 7, "fieldsOfStudy": null}
{"paperId": "574041308529471f8fca5fef16522e337196d813", "title": "Incorporating Practical Single Cell and Spatial Transcriptomics Analysis in a Bioinformatics Course", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "58ded57e020a3543580034ab9f2384772fa65c92", "title": "TGStools: A Bioinformatics Suit to Facilitate Transcriptome Analysis of Long Reads from Third Generation Sequencing Platform", "year": 2019, "citationCount": 1, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "59589b39f2d054e52cd74bd25327b28d4266dd13", "title": "easyGWAS: An Integrated Computational Framework for Advanced Genome-Wide Association Studies", "year": 2015, "citationCount": 0, "fieldsOfStudy": ["Biology", "Computer Science"]}
{"paperId": "59fac250d79e2527e4295d42a4c09b341f7bb923", "title": "Micromeda: a genome property prediction pipeline and web visualization tool", "year": 2020, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "5ad83378d77bd19fd297a35ffbcbc0bfa0db13ac", "title": "Programmer en Java Ed. 9", "year": 2017, "citationCount": 1, "fieldsOfStudy": ["Art"]}
{"paperId": "5b50125d5b8334cd234a12a763697c9df0cdf44e", "title": "visPIG - A Web Tool for Producing Multi-Region, Multi-Track, Multi-Scale Plots of Genetic Data", "year": 2014, "citationCount": 39, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "5c8bd21386bf1dbc172dad40c0bde3724d86f858", "title": "devCellPy is a machine learning-enabled pipeline for automated annotation of complex multilayered single-cell transcriptomic data", "year": 2022, "citationCount": 19, "fieldsOfStudy": ["Medicine"]}
{"paperId": "5f96dc9fb68cac34c0af276f2a27e9ac2e9ad45c", "title": "Single Cell Explorer, collaboration-driven tools to leverage large-scale single cell RNA-seq data", "year": 2019, "citationCount": 26, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "5fca4787cfccadd3a519d2e6fdbcb17b830e8df2", "title": "funkea: Functional Enrichment Analysis in Python", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "60ab4984108c51c2c2aa451c7865f3765d1c7b4a", "title": "Predicting Type Annotations for Python using Embeddings from Graph Neural Networks", "year": 2021, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "61106307c956e704e5a609f71b344a68bd51a9f2", "title": "Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python", "year": 2021, "citationCount": 48, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "617b84ff8daca43dc78f055c90ffeff932635e3f", "title": "Photography Style Analysis using Convolutional Neural Networks", "year": 2022, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "61f46081817f058bf2501fc60a162df5b7c73932", "title": "Flora Prepper: Preparing floras for morphological parsing and integration", "year": 2019, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "62c00320bcb7d8ffdac31f04a087f78f71e07e1f", "title": "Large Scale Generation of Labeled Type Data for Python", "year": 2022, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "64ce9de85f354669b8ca8d35b1db09ceb53e2b51", "title": "NU-AIR - A Neuromorphic Urban Aerial Dataset for Detection and Localization of Pedestrians and Vehicles", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "64f5c1e3da5f7dc21e11759de666d51052928c26", "title": "CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course", "year": 2022, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "658c00695a2b6aecda63e973ab93920800a92e2e", "title": "Pfeature: A Tool for Computing Wide Range of Protein Features and Building Prediction Models", "year": 2022, "citationCount": 27, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "66f7d39319289e9ad6f263fc977a0247ba129547", "title": "Transmorph: a unifying computational framework for modular single-cell RNA-seq data integration", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "69e0c45e06c612ec92e969767af13feaf5c6b321", "title": "Mixed-species RNA-seq for elucidation of non-cell-autonomous control of gene transcription", "year": 2018, "citationCount": 23, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "6b52a95acb74bf170917328a8a2705ef10a406e2", "title": "SUMA: a lightweight machine learning model-powered shared nearest neighbour-based clustering application interface for scRNA-Seq data", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "6cb40055bd871ee2178cea3d535c5c52d63ac3af", "title": "Learning Python Code Suggestion with a Sparse Pointer Network", "year": 2016, "citationCount": 85, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6d31353e041d806d4fd90a052374da61bee56659", "title": "Abstract 250: UCSC Xena for the visualization and analysis of cancer genomics data", "year": 2021, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6d839e244536ac79da64b4bfee68c3299cc616a3", "title": "Python 3 types in the wild: a tale of two type systems", "year": 2020, "citationCount": 21, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6dc7d659b828d42c1a3d9d968824c43123b19ac2", "title": "Rcount: simple and flexible RNA-Seq read counting", "year": 2015, "citationCount": 34, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "6e3355b142d426b6d8e1adbf9fffe0cb25bcc044", "title": "Semantic typing of linked geoprocessing workflows", "year": 2018, "citationCount": 22, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "6f5228009482d8d6aaed3372d49c35796911abde", "title": "R and Python Annotation Bindings for OMS", "year": 2018, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7174977e7b36b886f74258a7bca1032370fedbc3", "title": "CIViCpy: A Python Software Development and Analysis Toolkit for the CIViC Knowledgebase", "year": 2019, "citationCount": 10, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"]}
{"paperId": "72b2806b8dd1ee7523a579bdbc5dac30f40010c4", "title": "ScType enables fast and accurate cell type identification from spatial transcriptomics data", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "72b65e3b36d9b727bf5a6873ff6c3ded872ef3cb", "title": "GenomeQC: a quality assessment tool for genome assemblies and gene structure annotations", "year": 2019, "citationCount": 49, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "72cc0b36560030917971c2e9b77969d0377b031f", "title": "DeepGSR: an optimized deep-learning structure for the recognition of genomic signals and regions", "year": 2018, "citationCount": 53, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "73c29f57e92cc04a50308c7045457a918aa2484b", "title": "Building Family Trees With NooJ", "year": 2015, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "73db8bf26af340ff7907957186fa52b8c5d070e9", "title": "INSTINCT: The infrastructure for noise and soundscape tolerant investigation of nonspecific call types", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "74134d77b76c6577a8ea939a7c8f04db50fcc3f0", "title": "An Industry Classification Model of Small and Medium-sized Enterprises based on TF-IDF Characteristics", "year": 2019, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7638d6dfbf284347fb47691696cc492185b65d17", "title": "Abstract 7406: Visualization and analysis of cancer genomics data using UCSC Xena", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "7672d26ac9eeb63a5841d871468ee02dbdd3b7d7", "title": "Self Containment, a Property of Modular RNA Structures, Distinguishes microRNAs", "year": 2008, "citationCount": 26, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "76e9b27932f93d0c639c4257734031ee54ea8254", "title": "Building and inferring knowledge bases using biomedical text mining", "year": 2018, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "77b1916f4ae90f9324c66f5fdcb8bd9875a8d334", "title": "GOAT: Genetic Output Analysis Tool: An open source GWAS and genomic region visualization tool", "year": 2016, "citationCount": 4, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7a6eab9a15a5c8e74d7aba2fe6b54f22245ba0c5", "title": "Automated Mechanical Ventilator Waveform Analysis of Patient-Ventilator Asynchrony", "year": 2015, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "7b46e9c7bffa3ab6945a3a1c6131ba077e63e3c6", "title": "Exploring Geospatial Data Visualization Based on Python", "year": 2022, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "7c5639f770fd072ceb09772ea3bdc4f197a08b0c", "title": "Multi-template matching: a versatile tool for object-localization in microscopy images", "year": 2019, "citationCount": 28, "fieldsOfStudy": ["Computer Science", "Biology", "Medicine"]}
{"paperId": "7d947fd7b995c787baf4de21ddb2055e2838264d", "title": "Vaeda computationally annotates doublets in single-cell RNA sequencing data", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Medicine", "Biology", "Computer Science"]}
{"paperId": "809238dddf5be36a7b34c6152fa10393543cd6b8", "title": "PrismEXP: gene annotation prediction from stratified gene-gene co-expression matrices", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "80fd152033705a15873a5941ec5eaa43673487d2", "title": "msCentipede: Modeling Heterogeneity across Genomic Sites and Replicates Improves Accuracy in the Inference of Transcription Factor Binding", "year": 2015, "citationCount": 35, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "8126210233eb8f877f5c994e44b00a05a041d492", "title": "SBOannotator: a Python tool for the automated assignment of systems biology ontology terms", "year": 2023, "citationCount": 8, "fieldsOfStudy": ["Medicine"]}
{"paperId": "81dbff5b9164e6b5245324c8c464e6d5d688e1db", "title": "Programmer en Java : Couvre Java 9 Ed. 10", "year": 2017, "citationCount": 0, "fieldsOfStudy": ["Art"]}
{"paperId": "82d9e1e496b4849faef89380704a2b17a706cf99", "title": "BioAnalyzer: Bioinformatic Software of Routinely Used Tools for Analysis of Genomic Data", "year": 2019, "citationCount": 7, "fieldsOfStudy": ["Biology"]}
{"paperId": "851bc40b66451ffd44281a0d140563bf285bb989", "title": "Characterization of Institutional Texts for an Automated Golden Standard: Enhancing Machine Translation Quality Assessment between English and Spanish", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "85baa64c0c7b690dfbdc6d0643f627dbc6713a66", "title": "Accelerating Dynamically-Typed Languages on Heterogeneous Platforms Using Guards Optimization", "year": 2018, "citationCount": 11, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "8645af06a0d4a44303becd27e3808e9d5e792fb8", "title": "Poio APIAn annotation framework to bridge Language Documentation and Natural Language Processing", "year": 2012, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "881e0a20da9b6a597a608a7602d4d51330aa072d", "title": "Automated fragment formula annotation for electron ionisation, high resolution mass spectrometry: application to atmospheric measurements of halocarbons", "year": 2021, "citationCount": 4, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "88689b365b7cb5eca6b83b5ad3b5d30923142124", "title": "Describing models on the web using OGC standards", "year": 2013, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "891c62e3e73f67d0c52901b7c75201d81fb4bf27", "title": "Insane in the vembrane: filtering and transforming VCF/BCF files", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "8927f4e3da20bd47f27bec9e505e4b6aeeb5d5d6", "title": "How Well Static Type Checkers Work with Gradual Typing? A Case Study on Python", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "894b4768b116913e19506b3f5f3542def3c5b001", "title": "SEGUID v2: Extending SEGUID checksums for circular, linear, single- and double-stranded biological sequences", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "896d5b782e4e09dd0d415b9d0eb59eb006ba2c89", "title": "Machine-Learning Classification Suggests That Many Alphaproteobacterial Prophages May Instead Be Gene Transfer Agents", "year": 2019, "citationCount": 21, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "8a9846283e556406b0e78b203c087c4821071137", "title": "Mass2SMILES: deep learning based fast prediction of structures and functional groups directly from high-resolution MS/MS spectra", "year": 2023, "citationCount": 2, "fieldsOfStudy": ["Biology"]}
{"paperId": "8c30b4ef53fcd8e381201097346c7867fe6a736f", "title": "Static analysis driven enhancements for comprehension in machine learning notebooks", "year": 2023, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "8e7f3c4b73a9cf2fe933b528e012ce5081533884", "title": "DeepLontar dataset for handwritten Balinese character detection and syllable recognition on Lontar manuscript", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "8ea7972b8148b532890fcb4b85d4d69c9228c11c", "title": "3-dimensional electron microscopic imaging of the zebrafish olfactory bulb and dense reconstruction of neurons", "year": 2016, "citationCount": 34, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "8fdc16c0ab39e0a655b2488eced120e619d783af", "title": "Reproducible Humanities Research: Developing Extensible Databases for Recording \"Messy\" Categorisation, Annotation and Provenance Data", "year": 2020, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "924bf2a33fd04040b13b4913be73c220a59e98b5", "title": "Taming type annotations in gradual typing", "year": 2020, "citationCount": 6, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "92b4ea53b4ed3b832628fa5df4d5e49f0434d679", "title": "Fast clustering and cell-type annotation of scATAC data using pre-trained embeddings", "year": 2024, "citationCount": 3, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "92cbdcfb77adbe5a5f1ef18dfdb6e785344b2cc0", "title": "From Text-based Genome, Population Variations, and Transcriptome Datafiles to SQLite Database and Web Application: A Bioinformatical Study on Alfalfa", "year": 2021, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "93107809381d75143ec3bac1ed1339ed78740c92", "title": "Type Prediction With Program Decomposition and Fill-in-the-Type Training", "year": 2023, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "941e9f91f9dfb29404284db2d51d7a26fbceb5e8", "title": "Statically typed string sanitation inside a python", "year": 2014, "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Psychology"]}
{"paperId": "95d2d9cb0750a03833f1ed8bfb8cec07c8e5469e", "title": "SWAAT Bioinformatics Workflow for Protein Structure-Based Annotation of ADME Gene Variants", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Medicine"]}
{"paperId": "95eaff33497595509baa7a637989178b5575758b", "title": "SeismiQB - a novel framework for deep learning with seismic data", "year": 2020, "citationCount": 0, "fieldsOfStudy": ["Physics", "Computer Science"]}
{"paperId": "96a50949fb3846fabde4d99d1f97eeed61301722", "title": "scGAD: a new task and end-to-end framework for generalized cell type annotation and discovery", "year": 2023, "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "979d5677b001b5d253241ce437cf31a1d734ff39", "title": "MEMOTE for standardized genome-scale metabolic model testing", "year": 2020, "citationCount": 309, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "99a76876b78588eca0895cd36435202f2a3c5f2c", "title": "Charge cluster occurrence in land plants' mitochondrial proteomes with functional and structural insights.", "year": 2024, "citationCount": 1, "fieldsOfStudy": ["Medicine"]}
{"paperId": "9b14d84a6cd68b7fd829c884d0897fa9efa04e15", "title": "PyAnalyzer: An Effective and Practical Approach for Dependency Extraction from Python Code", "year": 2024, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "9be12c00628ff471cc2e18bedcf48a67e77a054f", "title": "On the cost of type-tag soundness", "year": 2017, "citationCount": 25, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "9cdaac878f9f051d06d1cf78f946e979c9a55781", "title": "A Complete Descritpion of the UnPython and Jit4GPU Framework", "year": 2011, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "9e2078e7ada3d1e1f66bcd632fb3c58a82d8e30c", "title": "Static Type Analysis by Abstract Interpretation of Python Programs (Artifact)", "year": 2020, "citationCount": 27, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "9e30b21ec05756775e0b7ce354f70968d76eb130", "title": "BioTEA: Containerized Methods of Analysis for Microarray-Based Transcriptomics Data", "year": 2022, "citationCount": 3, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "9f5425fd6d95de795bf6280afdb9cdc54220f025", "title": "Evaluating embedded semantics for accessibility description of web crawl data", "year": 2023, "citationCount": 1, "fieldsOfStudy": null}
{"paperId": "a429c6c8407b10a89c9610ab5e9682ec89772356", "title": "TypeWriter: neural type prediction with search-based validation", "year": 2019, "citationCount": 102, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "aa2f4c6bc7805ec43fb7b307043d360262eacea1", "title": "Multi-Modal Dataset Creation for Federated~Learning with DICOM Structured Reports", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "ae0139736bd11d3a36a49b726fa6e40b2930b211", "title": "A Comprehensive WebScraping of IMDb\u2019s Top 50 Movies using Beautiful Soup", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "b2052463bf09968804f41547cecd866ca9b2e3d8", "title": "alona: a web server for single-cell RNA-seq analysis", "year": 2020, "citationCount": 32, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "b41462ba6251c4be3956143c33eea5d5e6f926fb", "title": "Exosomal circRNA in Digestive System Tumors: The Main Player or Coadjuvants?", "year": 2021, "citationCount": 10, "fieldsOfStudy": ["Medicine"]}
{"paperId": "b6011964c28c9962ab64e79fe15d4f1a94ad09d7", "title": "ScanExitronLR: characterization and quantification of exitron splicing events in long-read RNA-seq data", "year": 2022, "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"]}
{"paperId": "b6c960e92d24fdc1f2e19151278c94957388aaeb", "title": "Type4Py: Deep Similarity Learning-Based Type Inference for Python", "year": 2021, "citationCount": 14, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "b8e711b9b12612fa922163c6ec2935b966edb454", "title": "Fast Lexical-Semantic Analysis of Syntactic n-grams in Myria CSE 544 Final Project", "year": 2015, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "b95c50af765557b6428d596ce285b5ef1e4da109", "title": "Technical report: CSVM dictionaries", "year": 2012, "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Biology"]}
{"paperId": "ba77a003796d2f89317736d62a8812da4273cd79", "title": "AS-Quant: Detection and Visualization of Alternative Splicing Events with RNA-seq Data", "year": 2020, "citationCount": 8, "fieldsOfStudy": ["Biology", "Medicine", "Computer Science"]}
{"paperId": "bd43d44eec45e922303f94fa19d79b859c72b17c", "title": "Bio.Phylo: A unified toolkit for processing, analyzing and visualizing phylogenetic trees in Biopython", "year": 2012, "citationCount": 124, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "bd4823dca61900a6a47220644d207fdc5f90f664", "title": "Scikit-talk: A toolkit for processing real-world conversational speech data", "year": 2021, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "be0ccf6a34f46caa4e28f72b3c15bcf39452fc47", "title": "Annot: a Django-based sample, reagent, and experiment metadata tracking system", "year": 2019, "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "be7f9f690bf283d354624c9ced9e4adc913844c0", "title": "Database Creator for Protein/Peptide Mass Analysis, DC-PPMA: A novel standalone computational tool for simplifying the analysis of MS/MS data to identify protein/polypeptide sequences by di\ufb00erent proteomic approaches", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "becc2a1a45a01f81c5cbf2353d364e1a43c95896", "title": "Polyglot and Speech Corpus Tools: A System for Representing, Integrating, and Querying Speech Corpora", "year": 2017, "citationCount": 13, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "bf53ef5d186a7e6d5540667ef188050b841595d2", "title": "BioThings SDK: a toolkit for building high-performance data APIs in biomedical research", "year": 2021, "citationCount": 15, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"]}
{"paperId": "bf7b426c0fe087f2d31749f286cbd771060bbca3", "title": "Staphylococcus aureus viewed from the perspective of 40,000+ genomes", "year": 2018, "citationCount": 78, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "bfcedc002a70132e69be732347cf89ccc17de93b", "title": "scMomentum: Inference of Cell-Type-Specific Regulatory Networks and Energy Landscapes", "year": 2020, "citationCount": 2, "fieldsOfStudy": ["Biology"]}
{"paperId": "c1225d221909509b8ce0bdbaa881cc7e2d74ed2f", "title": "UROPA: a tool for Universal RObust Peak Annotation", "year": 2017, "citationCount": 44, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "c2e5c0e08f454757c4c0a745fc693b1b1bbd332e", "title": "miEAA 2.0: integrating multi-species microRNA enrichment analysis and workflow management systems", "year": 2020, "citationCount": 138, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"]}
{"paperId": "c50656e86e7e6a1041c01e6f9a680ab6e5f1fe35", "title": "Transcriptomic analysis of benznidazole-resistant and susceptible Trypanosoma cruzi populations", "year": 2023, "citationCount": 6, "fieldsOfStudy": ["Medicine"]}
{"paperId": "c57017f5df871714a02a8e765f285dfee4c0edb4", "title": "Generating Python Type Annotations from Type Inference: How Far Are We?", "year": 2024, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "c639fc12e0add4fc2da38ff70b58071143b90ccd", "title": "QuakeLabeler: A Fast Seismic Data Set Creation and Annotation Toolbox for AI Applications", "year": 2022, "citationCount": 3, "fieldsOfStudy": null}
{"paperId": "c6cb292b4ba78e7b52aa5472c92be7078774035a", "title": "Systematic analysis of deep semantic segmentation architecture PSPNet on land cover ISPRS Vinhingen dataset", "year": 2020, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "c7c3895ffdb07a09616df5b8ec4c18dffa9457c4", "title": "as Data in the Context of Anonymising Clinical Study Reports", "year": 2018, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "c8dce2bea9bb73058b8310e367fbb1a0a5af8a1b", "title": "FlaiMapper: computational annotation of small ncRNA-derived fragments using RNA-seq high-throughput data", "year": 2015, "citationCount": 26, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "ca8a9ba8c4cda13b004c5b773e783b2b53bf9788", "title": "ARTWORK SEARCH ENGINE THROUGH WORD EMBEDDINGS", "year": 2020, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "cb64726caf3f782dee7f9f814c4a6c8076b27577", "title": "Python Interpreter Performance Deconstructed", "year": 2014, "citationCount": 18, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "cbb157aaf2c5e3d1f2f995a2a2cb1bf3710db4aa", "title": "ClusterScan: simple and generalistic identification of genomic clusters", "year": 2018, "citationCount": 10, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "cd0ebbfd8d0037514a55d64f793199a686bb2c7f", "title": "Perceptual annotation of local distortions in videos: tools and datasets", "year": 2023, "citationCount": 4, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "cede87fce1cc6075360e8b5149208c158c43df74", "title": "MethyMer: Design of combinations of specific primers for bisulfite sequencing of complete CpG islands", "year": 2018, "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"]}
{"paperId": "cfa677f5d9afe49184342cccd899117470c0682a", "title": "SBtab: a flexible table format for data exchange in systems biology", "year": 2016, "citationCount": 33, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "d24055a467bbf783d33a2a18e19a968f32cff34f", "title": "AnnotaPipeline: An integrated tool to annotate eukaryotic proteins using multi-omics data", "year": 2022, "citationCount": 1, "fieldsOfStudy": ["Medicine"]}
{"paperId": "d250dd92ff0737f0801cc483775257904b8827ff", "title": "MaxSMT-Based Type Inference for Python 3", "year": 2018, "citationCount": 40, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "d454e3394621f811659a891f79dfa4b4a08f483b", "title": "scATAnno: Automated Cell Type Annotation for single-cell ATAC Sequencing Data", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "d628aedd8e273d24d4476a593328637c56c7313d", "title": "Haystack: systematic analysis of the variation of epigenetic states and cell-type specific regulatory elements", "year": 2017, "citationCount": 16, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"]}
{"paperId": "d6ab66e58815bd4a6af80c104b3393a694c87b13", "title": "NGSmethDB 2017: enhanced methylomes and differential methylation", "year": 2016, "citationCount": 15, "fieldsOfStudy": ["Computer Science", "Biology", "Medicine"]}
{"paperId": "d7893b35837519d7f3d7eebaf2d0105e2a8634d9", "title": "msCentipede: Modeling heterogeneity across genomic sites improves accuracy in the inference of transcription factor binding", "year": 2014, "citationCount": 2, "fieldsOfStudy": ["Biology"]}
{"paperId": "d7ad1a0a899d49144442189a35e048cafe9fae28", "title": "scEVOLVE: cell-type incremental annotation without forgetting for single-cell RNA-seq data", "year": 2024, "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"]}
{"paperId": "d80c3fd556774a09305bd60bdfab154e312d85fd", "title": "How Dynamic Features Affect API Usages? An Empirical Study of API Misuses in Python Programs", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "daa11dfed48bbfa7b128bd203296bef729fef519", "title": "Pythran: Enabling Static Optimization of Scientific Python Programs", "year": 2013, "citationCount": 45, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "daeae5271db4dd2084f481287e0beeee5247727e", "title": "Automated annotation of protein families", "year": 2011, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "db7eb6a33d4b267346d362bcdceb6e715fa1365f", "title": "Development of a Python\u2010Based Algorithm for the Quantification and Analysis of Multivariate Next Generation Sequencing Data Following Genome\u2010Wide Mutagenesis and Cell Survival Screening", "year": 2016, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "ddf2e20427e24b422cc11f58a27458b75e1d3cca", "title": "Generative Type Inference for Python", "year": 2023, "citationCount": 6, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "de00b2ba6b221f6c739231d9e17b93974ed47f1b", "title": "Penerapan Metode CNN (Convolutional Neural Network) dalam Mengklasifikasi Uang Kertas dan Uang Logam", "year": 2024, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "dea1b39697764c67e7ed70575f4452189747f020", "title": "Computational Analysis of the Relationships Between Antibiotic Resistance and Plasmid Backbone Genes", "year": 2019, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "e04bc1a2f740fb5779d394f2d0692b37c3a2b825", "title": "A scalable sparse neural network framework for rare cell type annotation of single-cell transcriptome data", "year": 2022, "citationCount": 6, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "e0dec78865f733eb7ac572c08cf65466c6d7d3d8", "title": "Scaling up SoccerNet with multi-view spatial localization and re-identification", "year": 2022, "citationCount": 32, "fieldsOfStudy": ["Medicine"]}
{"paperId": "e0fdc657375ae2531a762c9756520625d1f538df", "title": "Abstract 5039: Visualization and analysis of cancer genomics data using UCSC Xena", "year": 2022, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "e1bf21ab4f577d660a11727ebc3ee76b17b41693", "title": "Static Inference Meets Deep learning: A Hybrid Type Inference Approach for Python", "year": 2021, "citationCount": 39, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e2ffb1e2418476d0a6281a542cc3db5d266744d9", "title": "Fast and lightweight cell atlas approximations across organs and organisms", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "e36bd1f5d0feb0c2e42c7c4ec22103f28da89784", "title": "scBOL: a universal cell type identification framework for single-cell and spatial transcriptomics data", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "e3b581cd563f7243531f9274c32ba2db18f5fdce", "title": "Design and evaluation of gradual typing for python", "year": 2014, "citationCount": 134, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e3befd3aea28395ed9b16e15248b60d03e320849", "title": "Gradual Typing in an Open World", "year": 2016, "citationCount": 3, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e40df9e7ff4b68c09434ccf9abd735fc409713b2", "title": "ESBMC-Python: A Bounded Model Checker for Python Programs", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e7dae5d4157de54b7cf29684fe387a74e500c67d", "title": "Programming Languages and Systems - 10th Asian Symposium, APLAS 2012, Kyoto, Japan, December 11-13, 2012. Proceedings", "year": 2012, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "e9a8dfe4e80c87f1940d41f5b895d4d20dee58b4", "title": "Planet Dynamic or: How I Learned to Stop Worrying and Love Reflection", "year": 2012, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "ea1b62a03ab16c4acca6f7041c6f096bedec244b", "title": "Deep learning type inference", "year": 2018, "citationCount": 183, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "ec1eb2022483e7748cc36e3921d089b4a25b29a2", "title": "Variable-Arity Polymorphism T", "year": 2008, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "eca2188e812fde202104d929685270b21c7d13e7", "title": "Alignment of spatial transcriptomics data using diffeomorphic metric mapping", "year": 2023, "citationCount": 11, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "ecab4338c58ab7dea4e403854660191b96f6e238", "title": "Pygenomics: manipulating genomic intervals and data files in Python", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Medicine"]}
{"paperId": "f24462deda0002f0cfb6eaa035837d84be6ccf2c", "title": "Extreme Structure from Motion for Indoor Panoramas without Visual Overlaps [Supplementary Material]", "year": 2021, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "f400b6dd7f44cdf85fc79808b4fe3dd428c10106", "title": "Static Type Inference for Foreign Functions of Python", "year": 2021, "citationCount": 10, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f4532ca8573955c239bdd6cf03af395cfb481aae", "title": "Probabilistic metabolite annotation using retention time prediction and meta-learned projections", "year": 2022, "citationCount": 10, "fieldsOfStudy": ["Medicine", "Computer Science"]}
{"paperId": "f4552b4c0778ae15a531fe7af24d6acf66ec2e9b", "title": "DDUO: General-Purpose Dynamic Analysis for Differential Privacy", "year": 2021, "citationCount": 9, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f57e41ac932caedd3c21861b8cd65d5f87eb9221", "title": "Genome-based characterization of two Colombian clinical Providencia rettgeri isolates co-harboring NDM-1, VIM-2, and other \u03b2-lactamases", "year": 2020, "citationCount": 0, "fieldsOfStudy": null}
{"paperId": "f6867393c8648f3d40abda59812b8a8ff673b1d9", "title": "Automatic generation of library bindings using static analysis", "year": 2009, "citationCount": 23, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f78c654b8db7f5e18c53db42e9d683e9ce81749e", "title": "Proceedings for the 1st workshop on Script to Program Evolution", "year": 2009, "citationCount": 2, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f81a1b4510631d14b5b565c4701ee056f8d5c72f", "title": "CodePlan: Repository-level Coding using LLMs and Planning", "year": 2023, "citationCount": 43, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f81af9b284efad8f6719f46dea034885ce3322c3", "title": "Information Extraction from Social Media: A Hands-on Tutorial on Tasks, Data, and Open Source Tools", "year": 2022, "citationCount": 0, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f8742581920a778e62bd7196dcc9be02c9914b76", "title": "OMEinfo: global geographic metadata for -omics experiments", "year": 2023, "citationCount": 0, "fieldsOfStudy": ["Biology", "Medicine"]}
{"paperId": "f89def503972e5e3918184b293f7d8636133178d", "title": "Combining Static and Dynamic Typing in Ruby", "year": 2009, "citationCount": 11, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "f8c1ab7be430a94ff24918db7d0f7a9250720b75", "title": "Straintables: An application that extracts sequences from genome assemblies and generates dissimilarity matrices", "year": 2021, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "f977e13ee695a4944bbdedcb35d43e0483d20ff3", "title": "Epigenomic Signatures in Myelodysplastic Syndrome Patients As Predictors of Donor Compatibility and Transplant Outcome", "year": 2019, "citationCount": 1, "fieldsOfStudy": ["Medicine"]}
{"paperId": "fab636ef521c60636b57e7c1c28f9eb0a6c4d756", "title": "ManyTypes4TypeScript: A Comprehensive TypeScript Dataset for Sequence-Based Type Inference", "year": 2022, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "fb1b07e774abefcfa718db5d3298a9b1d7a1589c", "title": "CIA: a Cluster Independent Annotation method to investigate cell identities in scRNA-seq data", "year": 2024, "citationCount": 0, "fieldsOfStudy": ["Biology"]}
{"paperId": "fb5402a401a991379a866ac55df9ed2179a32cad", "title": "Crowdsourcing Subjective Annotations Using Pairwise Comparisons Reduces Bias and Error Compared to the Majority-vote Method", "year": 2023, "citationCount": 5, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "fc0ae9ef42e1fcb994947808f1dd40bd26fa0368", "title": "Automated assignment of cell identity from single-cell multiplexed imaging and proteomic data", "year": 2021, "citationCount": 33, "fieldsOfStudy": ["Medicine", "Biology"]}
{"paperId": "fc9bda6658f6c2ebb83e81f83ab90f5363905e63", "title": "Safe-DS: A Domain Specific Language to Make Data Science Safe", "year": 2023, "citationCount": 1, "fieldsOfStudy": ["Computer Science"]}
{"paperId": "fdb5db4a3d331fe8bb186eb34c25f9991a305c90", "title": "Pytometry: Flow and mass cytometry analytics in Python", "year": 2022, "citationCount": 6, "fieldsOfStudy": ["Biology"]}
{"paperId": "fe80e35fa61b2d4b674a40338dc1fc655e074050", "title": "Interact: Automated analysis of protein-ligand interactions by 1D and 2D NMR", "year": 2018, "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Biology"]}
{"paperId": "fea50e8367e508588a0c222f63991bbb966c9707", "title": "Collaborative Metadata Definition using Controlled Vocabularies, and Ontologies", "year": 2022, "citationCount": 2, "fieldsOfStudy": null}
